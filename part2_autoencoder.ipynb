{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.,  0.,  0., 13., 15., 10.,\n",
       "       15.,  5.,  0.,  0.,  3., 15.,  2.,  0., 11.,  8.,  0.,  0.,  4.,\n",
       "       12.,  0.,  0.,  8.,  8.,  0.,  0.,  5.,  8.,  0.,  0.,  9.,  8.,\n",
       "        0.,  0.,  4., 11.,  0.,  1., 12.,  7.,  0.,  0.,  2., 14.,  5.,\n",
       "       10., 12.,  0.,  0.,  0.,  0.,  6., 13., 10.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(set(X.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data by the max value. \n",
    "X = X / 16.0\n",
    "\n",
    "# autoencoder's labels are the input\n",
    "y = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  \tTraining Loss: 0.0028699046985849134\tValidation Loss: 0.002724279731345607\n",
      "Epoch 2  \tTraining Loss: 0.0027391765991805145\tValidation Loss: 0.0026067917704214985\n",
      "Epoch 3  \tTraining Loss: 0.0026204369684596114\tValidation Loss: 0.002481072955551589\n",
      "Epoch 4  \tTraining Loss: 0.0024937146615297222\tValidation Loss: 0.0023353631408482463\n",
      "Epoch 5  \tTraining Loss: 0.0023470215614059546\tValidation Loss: 0.002164021509183964\n",
      "Epoch 6  \tTraining Loss: 0.002174588742433443\tValidation Loss: 0.001970427237276765\n",
      "Epoch 7  \tTraining Loss: 0.00197975252902755\tValidation Loss: 0.0017711587176862688\n",
      "Epoch 8  \tTraining Loss: 0.0017793051062177058\tValidation Loss: 0.0015916001695347516\n",
      "Epoch 9  \tTraining Loss: 0.0015988564354710078\tValidation Loss: 0.0014512327187743306\n",
      "Epoch 10  \tTraining Loss: 0.0014579317316908034\tValidation Loss: 0.0013529166936641318\n",
      "Epoch 11  \tTraining Loss: 0.0013593344160138866\tValidation Loss: 0.0012876965241440876\n",
      "Epoch 12  \tTraining Loss: 0.0012939766105598811\tValidation Loss: 0.001244683382069029\n",
      "Epoch 13  \tTraining Loss: 0.0012508772341084397\tValidation Loss: 0.0012156607522051668\n",
      "Epoch 14  \tTraining Loss: 0.0012217795555640298\tValidation Loss: 0.001195374755782593\n",
      "Epoch 15  \tTraining Loss: 0.001201416057725782\tValidation Loss: 0.0011806355427529443\n",
      "Epoch 16  \tTraining Loss: 0.0011865965846909177\tValidation Loss: 0.0011695073333231483\n",
      "Epoch 17  \tTraining Loss: 0.0011753886657452721\tValidation Loss: 0.001160792902863267\n",
      "Epoch 18  \tTraining Loss: 0.0011665976577218066\tValidation Loss: 0.001153732178922047\n",
      "Epoch 19  \tTraining Loss: 0.0011594671444950463\tValidation Loss: 0.001147835695175389\n",
      "Epoch 20  \tTraining Loss: 0.0011535062779454487\tValidation Loss: 0.0011427733449924618\n",
      "Epoch 21  \tTraining Loss: 0.0011483858004590242\tValidation Loss: 0.0011383193100189064\n",
      "Epoch 22  \tTraining Loss: 0.001143881721739342\tValidation Loss: 0.0011343224285557574\n",
      "Epoch 23  \tTraining Loss: 0.001139838248554093\tValidation Loss: 0.0011306677650931875\n",
      "Epoch 24  \tTraining Loss: 0.0011361413824420288\tValidation Loss: 0.0011272776741672668\n",
      "Epoch 25  \tTraining Loss: 0.0011327108147794012\tValidation Loss: 0.0011240913466340832\n",
      "Epoch 26  \tTraining Loss: 0.0011294856717803752\tValidation Loss: 0.0011210614128630348\n",
      "Epoch 27  \tTraining Loss: 0.0011264190593120334\tValidation Loss: 0.0011181550379126216\n",
      "Epoch 28  \tTraining Loss: 0.001123476021424446\tValidation Loss: 0.0011153450704102191\n",
      "Epoch 29  \tTraining Loss: 0.0011206292178251626\tValidation Loss: 0.0011126068160470287\n",
      "Epoch 30  \tTraining Loss: 0.0011178533745567225\tValidation Loss: 0.0011099228138899402\n",
      "Epoch 31  \tTraining Loss: 0.0011151310613638217\tValidation Loss: 0.001107279754205559\n",
      "Epoch 32  \tTraining Loss: 0.0011124475980283726\tValidation Loss: 0.0011046663121894176\n",
      "Epoch 33  \tTraining Loss: 0.0011097921187922753\tValidation Loss: 0.0011020718854978318\n",
      "Epoch 34  \tTraining Loss: 0.0011071537497221917\tValidation Loss: 0.001099485188168656\n",
      "Epoch 35  \tTraining Loss: 0.0011045220921911205\tValidation Loss: 0.0010969026123336988\n",
      "Epoch 36  \tTraining Loss: 0.0011018901436332456\tValidation Loss: 0.0010943166983638185\n",
      "Epoch 37  \tTraining Loss: 0.0010992527227688027\tValidation Loss: 0.0010917218600587203\n",
      "Epoch 38  \tTraining Loss: 0.001096603747518059\tValidation Loss: 0.0010891123909766646\n",
      "Epoch 39  \tTraining Loss: 0.0010939370642492911\tValidation Loss: 0.0010864814899669388\n",
      "Epoch 40  \tTraining Loss: 0.00109124766919197\tValidation Loss: 0.0010838266611645404\n",
      "Epoch 41  \tTraining Loss: 0.001088531142968493\tValidation Loss: 0.0010811442037267792\n",
      "Epoch 42  \tTraining Loss: 0.001085783978716186\tValidation Loss: 0.0010784283840329972\n",
      "Epoch 43  \tTraining Loss: 0.001083002234187983\tValidation Loss: 0.0010756823154525889\n",
      "Epoch 44  \tTraining Loss: 0.00108018463480292\tValidation Loss: 0.001072899917937604\n",
      "Epoch 45  \tTraining Loss: 0.0010773268821054087\tValidation Loss: 0.0010700785394909703\n",
      "Epoch 46  \tTraining Loss: 0.0010744269804241558\tValidation Loss: 0.0010672170991113824\n",
      "Epoch 47  \tTraining Loss: 0.0010714847070629682\tValidation Loss: 0.0010643143462191135\n",
      "Epoch 48  \tTraining Loss: 0.0010684986802828647\tValidation Loss: 0.0010613664100821325\n",
      "Epoch 49  \tTraining Loss: 0.001065467387092468\tValidation Loss: 0.0010583719675113154\n",
      "Epoch 50  \tTraining Loss: 0.0010623885520386598\tValidation Loss: 0.001055326583590251\n",
      "Epoch 51  \tTraining Loss: 0.0010592571227378683\tValidation Loss: 0.0010522303204652044\n",
      "Epoch 52  \tTraining Loss: 0.0010560717843139285\tValidation Loss: 0.0010490818239355785\n",
      "Epoch 53  \tTraining Loss: 0.001052833298267225\tValidation Loss: 0.0010458843841864994\n",
      "Epoch 54  \tTraining Loss: 0.0010495398422088287\tValidation Loss: 0.001042634531273067\n",
      "Epoch 55  \tTraining Loss: 0.0010461900325698119\tValidation Loss: 0.001039329039900152\n",
      "Epoch 56  \tTraining Loss: 0.001042781651028651\tValidation Loss: 0.001035966751010142\n",
      "Epoch 57  \tTraining Loss: 0.0010393147032182256\tValidation Loss: 0.0010325469116237785\n",
      "Epoch 58  \tTraining Loss: 0.0010357892017162536\tValidation Loss: 0.0010290687814366356\n",
      "Epoch 59  \tTraining Loss: 0.0010322026614713094\tValidation Loss: 0.0010255302885727926\n",
      "Epoch 60  \tTraining Loss: 0.00102855472591961\tValidation Loss: 0.0010219335632882924\n",
      "Epoch 61  \tTraining Loss: 0.001024846823688718\tValidation Loss: 0.0010182783272249631\n",
      "Epoch 62  \tTraining Loss: 0.0010210777557266308\tValidation Loss: 0.0010145648747284677\n",
      "Epoch 63  \tTraining Loss: 0.0010172485087248202\tValidation Loss: 0.001010793461540118\n",
      "Epoch 64  \tTraining Loss: 0.0010133600166378637\tValidation Loss: 0.0010069641033049399\n",
      "Epoch 65  \tTraining Loss: 0.0010094122687816002\tValidation Loss: 0.0010030778082509564\n",
      "Epoch 66  \tTraining Loss: 0.001005405673806161\tValidation Loss: 0.000999132556690401\n",
      "Epoch 67  \tTraining Loss: 0.0010013402928634862\tValidation Loss: 0.0009951290376161626\n",
      "Epoch 68  \tTraining Loss: 0.000997216548244797\tValidation Loss: 0.0009910671455278905\n",
      "Epoch 69  \tTraining Loss: 0.0009930354446413524\tValidation Loss: 0.0009869494629528546\n",
      "Epoch 70  \tTraining Loss: 0.0009887978806062652\tValidation Loss: 0.0009827799898361861\n",
      "Epoch 71  \tTraining Loss: 0.0009845055918015172\tValidation Loss: 0.0009785583682782417\n",
      "Epoch 72  \tTraining Loss: 0.0009801600077153118\tValidation Loss: 0.0009742849390085088\n",
      "Epoch 73  \tTraining Loss: 0.000975763236180883\tValidation Loss: 0.0009699626728518174\n",
      "Epoch 74  \tTraining Loss: 0.0009713174543281023\tValidation Loss: 0.0009655936875723842\n",
      "Epoch 75  \tTraining Loss: 0.0009668225553273767\tValidation Loss: 0.000961177334651297\n",
      "Epoch 76  \tTraining Loss: 0.0009622803244257071\tValidation Loss: 0.0009567164767344665\n",
      "Epoch 77  \tTraining Loss: 0.0009576934236991707\tValidation Loss: 0.000952212995402489\n",
      "Epoch 78  \tTraining Loss: 0.000953065162126273\tValidation Loss: 0.00094767023065424\n",
      "Epoch 79  \tTraining Loss: 0.0009483983290101868\tValidation Loss: 0.0009430910709347222\n",
      "Epoch 80  \tTraining Loss: 0.0009436942974035538\tValidation Loss: 0.0009384808396370228\n",
      "Epoch 81  \tTraining Loss: 0.0009389566926092625\tValidation Loss: 0.0009338398646657994\n",
      "Epoch 82  \tTraining Loss: 0.000934187245680924\tValidation Loss: 0.0009291692233769444\n",
      "Epoch 83  \tTraining Loss: 0.0009293886109553931\tValidation Loss: 0.0009244707525279417\n",
      "Epoch 84  \tTraining Loss: 0.0009245647198041257\tValidation Loss: 0.0009197507455300765\n",
      "Epoch 85  \tTraining Loss: 0.0009197194817398473\tValidation Loss: 0.0009150113513531099\n",
      "Epoch 86  \tTraining Loss: 0.0009148538477209247\tValidation Loss: 0.0009102541462312003\n",
      "Epoch 87  \tTraining Loss: 0.0009099708728752194\tValidation Loss: 0.0009054831718138829\n",
      "Epoch 88  \tTraining Loss: 0.0009050743103762785\tValidation Loss: 0.0009007003488194886\n",
      "Epoch 89  \tTraining Loss: 0.000900167407770027\tValidation Loss: 0.0008959107980543781\n",
      "Epoch 90  \tTraining Loss: 0.0008952539458081734\tValidation Loss: 0.0008911175960849532\n",
      "Epoch 91  \tTraining Loss: 0.0008903375756086743\tValidation Loss: 0.000886323045232579\n",
      "Epoch 92  \tTraining Loss: 0.000885421707741342\tValidation Loss: 0.0008815298085357927\n",
      "Epoch 93  \tTraining Loss: 0.000880508508699699\tValidation Loss: 0.0008767416023425844\n",
      "Epoch 94  \tTraining Loss: 0.0008756018883874211\tValidation Loss: 0.0008719605484495532\n",
      "Epoch 95  \tTraining Loss: 0.0008707050768487643\tValidation Loss: 0.0008671935599849109\n",
      "Epoch 96  \tTraining Loss: 0.0008658230172450871\tValidation Loss: 0.0008624397461218147\n",
      "Epoch 97  \tTraining Loss: 0.0008609561179501942\tValidation Loss: 0.0008577049306658006\n",
      "Epoch 98  \tTraining Loss: 0.0008561097537281195\tValidation Loss: 0.0008529901222926871\n",
      "Epoch 99  \tTraining Loss: 0.0008512861090919477\tValidation Loss: 0.000848300045697757\n",
      "Epoch 100  \tTraining Loss: 0.0008464892340885689\tValidation Loss: 0.0008436353218321392\n",
      "Epoch 101  \tTraining Loss: 0.0008417194925798543\tValidation Loss: 0.0008390012397664852\n",
      "Epoch 102  \tTraining Loss: 0.0008369781695568311\tValidation Loss: 0.0008343977857975885\n",
      "Epoch 103  \tTraining Loss: 0.0008322674530148213\tValidation Loss: 0.0008298276292940586\n",
      "Epoch 104  \tTraining Loss: 0.000827590234606186\tValidation Loss: 0.0008252920001343832\n",
      "Epoch 105  \tTraining Loss: 0.0008229489048401558\tValidation Loss: 0.0008207931757199465\n",
      "Epoch 106  \tTraining Loss: 0.0008183458038772064\tValidation Loss: 0.0008163330441256722\n",
      "Epoch 107  \tTraining Loss: 0.0008137826747267636\tValidation Loss: 0.0008119137739396434\n",
      "Epoch 108  \tTraining Loss: 0.0008092623414800704\tValidation Loss: 0.0008075357144194526\n",
      "Epoch 109  \tTraining Loss: 0.0008047856855081987\tValidation Loss: 0.0008032020386580883\n",
      "Epoch 110  \tTraining Loss: 0.0008003548035680088\tValidation Loss: 0.000798914187277736\n",
      "Epoch 111  \tTraining Loss: 0.0007959715106646767\tValidation Loss: 0.0007946727822670936\n",
      "Epoch 112  \tTraining Loss: 0.0007916368598332241\tValidation Loss: 0.0007904813286494563\n",
      "Epoch 113  \tTraining Loss: 0.0007873515739968216\tValidation Loss: 0.0007863406644329661\n",
      "Epoch 114  \tTraining Loss: 0.0007831165481969447\tValidation Loss: 0.0007822499437383358\n",
      "Epoch 115  \tTraining Loss: 0.0007789322657814892\tValidation Loss: 0.0007782096968884052\n",
      "Epoch 116  \tTraining Loss: 0.0007748002971030081\tValidation Loss: 0.0007742213232395372\n",
      "Epoch 117  \tTraining Loss: 0.0007707212070826346\tValidation Loss: 0.0007702861130038186\n",
      "Epoch 118  \tTraining Loss: 0.0007666949549478574\tValidation Loss: 0.0007664038316133983\n",
      "Epoch 119  \tTraining Loss: 0.0007627226760825328\tValidation Loss: 0.0007625740563310453\n",
      "Epoch 120  \tTraining Loss: 0.000758804404027673\tValidation Loss: 0.0007587974146488572\n",
      "Epoch 121  \tTraining Loss: 0.0007549405974073169\tValidation Loss: 0.0007550743408847097\n",
      "Epoch 122  \tTraining Loss: 0.0007511316791671952\tValidation Loss: 0.0007514047370424912\n",
      "Epoch 123  \tTraining Loss: 0.000747377549642267\tValidation Loss: 0.0007477887695440009\n",
      "Epoch 124  \tTraining Loss: 0.0007436785835097039\tValidation Loss: 0.0007442267934604854\n",
      "Epoch 125  \tTraining Loss: 0.0007400340358462635\tValidation Loss: 0.0007407180951645489\n",
      "Epoch 126  \tTraining Loss: 0.0007364447311061393\tValidation Loss: 0.0007372623661588473\n",
      "Epoch 127  \tTraining Loss: 0.0007329095942209391\tValidation Loss: 0.000733859332429013\n",
      "Epoch 128  \tTraining Loss: 0.0007294284448887916\tValidation Loss: 0.0007305085100428667\n",
      "Epoch 129  \tTraining Loss: 0.0007260003694858815\tValidation Loss: 0.0007272098348264615\n",
      "Epoch 130  \tTraining Loss: 0.0007226250748209359\tValidation Loss: 0.0007239629246784894\n",
      "Epoch 131  \tTraining Loss: 0.0007193026428075171\tValidation Loss: 0.0007207669196713821\n",
      "Epoch 132  \tTraining Loss: 0.0007160323967043265\tValidation Loss: 0.0007176212170923058\n",
      "Epoch 133  \tTraining Loss: 0.0007128135861479385\tValidation Loss: 0.0007145252070553463\n",
      "Epoch 134  \tTraining Loss: 0.0007096459856775355\tValidation Loss: 0.0007114781712807562\n",
      "Epoch 135  \tTraining Loss: 0.0007065301766273391\tValidation Loss: 0.0007084798417139111\n",
      "Epoch 136  \tTraining Loss: 0.0007034640862350893\tValidation Loss: 0.0007055299922462409\n",
      "Epoch 137  \tTraining Loss: 0.000700446359512181\tValidation Loss: 0.0007026273651971618\n",
      "Epoch 138  \tTraining Loss: 0.0006974765085207103\tValidation Loss: 0.0006997709835625149\n",
      "Epoch 139  \tTraining Loss: 0.0006945540985805426\tValidation Loss: 0.0006969600339909453\n",
      "Epoch 140  \tTraining Loss: 0.0006916789347037816\tValidation Loss: 0.0006941954152892431\n",
      "Epoch 141  \tTraining Loss: 0.0006888492872893033\tValidation Loss: 0.0006914751665833556\n",
      "Epoch 142  \tTraining Loss: 0.0006860642918500197\tValidation Loss: 0.0006887973190622996\n",
      "Epoch 143  \tTraining Loss: 0.0006833228272163167\tValidation Loss: 0.000686161895919392\n",
      "Epoch 144  \tTraining Loss: 0.0006806245763704519\tValidation Loss: 0.0006835677082696757\n",
      "Epoch 145  \tTraining Loss: 0.0006779684401641815\tValidation Loss: 0.0006810147389684511\n",
      "Epoch 146  \tTraining Loss: 0.0006753535210017521\tValidation Loss: 0.0006785018011721665\n",
      "Epoch 147  \tTraining Loss: 0.0006727795292218546\tValidation Loss: 0.0006760285478884568\n",
      "Epoch 148  \tTraining Loss: 0.0006702460445221454\tValidation Loss: 0.0006735942045261667\n",
      "Epoch 149  \tTraining Loss: 0.0006677517260583043\tValidation Loss: 0.0006711969101663103\n",
      "Epoch 150  \tTraining Loss: 0.0006652956286226641\tValidation Loss: 0.0006688368326361389\n",
      "Epoch 151  \tTraining Loss: 0.0006628769477149605\tValidation Loss: 0.0006665125367420752\n",
      "Epoch 152  \tTraining Loss: 0.0006604948125844459\tValidation Loss: 0.0006642237479314923\n",
      "Epoch 153  \tTraining Loss: 0.000658148158853786\tValidation Loss: 0.0006619687873577816\n",
      "Epoch 154  \tTraining Loss: 0.0006558368161556934\tValidation Loss: 0.0006597472626324555\n",
      "Epoch 155  \tTraining Loss: 0.0006535600924560099\tValidation Loss: 0.0006575586688213516\n",
      "Epoch 156  \tTraining Loss: 0.0006513173556196594\tValidation Loss: 0.0006554019744755073\n",
      "Epoch 157  \tTraining Loss: 0.0006491072476340319\tValidation Loss: 0.0006532770598136039\n",
      "Epoch 158  \tTraining Loss: 0.0006469288697307345\tValidation Loss: 0.0006511822655080287\n",
      "Epoch 159  \tTraining Loss: 0.0006447812018029249\tValidation Loss: 0.0006491162415813894\n",
      "Epoch 160  \tTraining Loss: 0.0006426643545692062\tValidation Loss: 0.000647079441273474\n",
      "Epoch 161  \tTraining Loss: 0.0006405773505776789\tValidation Loss: 0.0006450707338595194\n",
      "Epoch 162  \tTraining Loss: 0.0006385189900710395\tValidation Loss: 0.0006430898874901771\n",
      "Epoch 163  \tTraining Loss: 0.0006364893462774511\tValidation Loss: 0.0006411362355131732\n",
      "Epoch 164  \tTraining Loss: 0.000634487889988015\tValidation Loss: 0.000639209185934794\n",
      "Epoch 165  \tTraining Loss: 0.000632514106210196\tValidation Loss: 0.0006373084242954523\n",
      "Epoch 166  \tTraining Loss: 0.0006305669354823378\tValidation Loss: 0.000635432395514537\n",
      "Epoch 167  \tTraining Loss: 0.0006286448871902898\tValidation Loss: 0.0006335807492431827\n",
      "Epoch 168  \tTraining Loss: 0.0006267483340775342\tValidation Loss: 0.0006317532011529529\n",
      "Epoch 169  \tTraining Loss: 0.0006248766132690978\tValidation Loss: 0.0006299479747687078\n",
      "Epoch 170  \tTraining Loss: 0.0006230291782128884\tValidation Loss: 0.0006281666242567619\n",
      "Epoch 171  \tTraining Loss: 0.0006212065702512329\tValidation Loss: 0.0006264071277779169\n",
      "Epoch 172  \tTraining Loss: 0.0006194067378737114\tValidation Loss: 0.0006246696676902114\n",
      "Epoch 173  \tTraining Loss: 0.000617629287796483\tValidation Loss: 0.0006229528034141677\n",
      "Epoch 174  \tTraining Loss: 0.0006158742929161695\tValidation Loss: 0.00062125799252667\n",
      "Epoch 175  \tTraining Loss: 0.0006141410552655769\tValidation Loss: 0.0006195841808739678\n",
      "Epoch 176  \tTraining Loss: 0.0006124290393212971\tValidation Loss: 0.0006179305394357913\n",
      "Epoch 177  \tTraining Loss: 0.000610738491689973\tValidation Loss: 0.0006162974429621004\n",
      "Epoch 178  \tTraining Loss: 0.0006090689128740718\tValidation Loss: 0.0006146843112101474\n",
      "Epoch 179  \tTraining Loss: 0.0006074191480302917\tValidation Loss: 0.0006130890562755103\n",
      "Epoch 180  \tTraining Loss: 0.0006057884376729185\tValidation Loss: 0.0006115121913115247\n",
      "Epoch 181  \tTraining Loss: 0.0006041779423342993\tValidation Loss: 0.0006099551677996671\n",
      "Epoch 182  \tTraining Loss: 0.0006025881012154872\tValidation Loss: 0.0006084178990314125\n",
      "Epoch 183  \tTraining Loss: 0.0006010179927198455\tValidation Loss: 0.0006068995022516906\n",
      "Epoch 184  \tTraining Loss: 0.0005994673170225624\tValidation Loss: 0.0006053990232155764\n",
      "Epoch 185  \tTraining Loss: 0.0005979355797158117\tValidation Loss: 0.0006039159108708892\n",
      "Epoch 186  \tTraining Loss: 0.0005964223070945272\tValidation Loss: 0.0006024504786430911\n",
      "Epoch 187  \tTraining Loss: 0.0005949267939316675\tValidation Loss: 0.0006010018108902334\n",
      "Epoch 188  \tTraining Loss: 0.0005934486522185229\tValidation Loss: 0.000599569706975494\n",
      "Epoch 189  \tTraining Loss: 0.0005919875268042597\tValidation Loss: 0.000598154663158876\n",
      "Epoch 190  \tTraining Loss: 0.0005905433972851202\tValidation Loss: 0.0005967554671246904\n",
      "Epoch 191  \tTraining Loss: 0.0005891159109026291\tValidation Loss: 0.0005953719114373825\n",
      "Epoch 192  \tTraining Loss: 0.0005877044850797846\tValidation Loss: 0.0005940033131295006\n",
      "Epoch 193  \tTraining Loss: 0.0005863091286874069\tValidation Loss: 0.0005926495066103635\n",
      "Epoch 194  \tTraining Loss: 0.0005849291304558884\tValidation Loss: 0.0005913104187973664\n",
      "Epoch 195  \tTraining Loss: 0.0005835641091561135\tValidation Loss: 0.0005899853174105286\n",
      "Epoch 196  \tTraining Loss: 0.0005822137679039727\tValidation Loss: 0.000588673741227257\n",
      "Epoch 197  \tTraining Loss: 0.0005808779255434804\tValidation Loss: 0.0005873754874153909\n",
      "Epoch 198  \tTraining Loss: 0.0005795562212544048\tValidation Loss: 0.0005860906590247095\n",
      "Epoch 199  \tTraining Loss: 0.00057824848478599\tValidation Loss: 0.0005848188683358188\n",
      "Epoch 200  \tTraining Loss: 0.0005769545893178511\tValidation Loss: 0.0005835596751512963\n",
      "Epoch 201  \tTraining Loss: 0.00057567407196364\tValidation Loss: 0.000582312986326546\n",
      "Epoch 202  \tTraining Loss: 0.0005744065241664928\tValidation Loss: 0.0005810785731233556\n",
      "Epoch 203  \tTraining Loss: 0.0005731517934477482\tValidation Loss: 0.0005798564649473303\n",
      "Epoch 204  \tTraining Loss: 0.0005719095479119065\tValidation Loss: 0.0005786460668787105\n",
      "Epoch 205  \tTraining Loss: 0.0005706794768640563\tValidation Loss: 0.0005774474158130578\n",
      "Epoch 206  \tTraining Loss: 0.0005694619068165758\tValidation Loss: 0.0005762600177870171\n",
      "Epoch 207  \tTraining Loss: 0.0005682561879968404\tValidation Loss: 0.0005750836299424436\n",
      "Epoch 208  \tTraining Loss: 0.0005670621623881765\tValidation Loss: 0.000573918401165788\n",
      "Epoch 209  \tTraining Loss: 0.0005658795570426627\tValidation Loss: 0.0005727640597265539\n",
      "Epoch 210  \tTraining Loss: 0.0005647080899905518\tValidation Loss: 0.0005716204767808736\n",
      "Epoch 211  \tTraining Loss: 0.0005635475024060978\tValidation Loss: 0.0005704871347090849\n",
      "Epoch 212  \tTraining Loss: 0.0005623974629179074\tValidation Loss: 0.0005693637323215347\n",
      "Epoch 213  \tTraining Loss: 0.0005612580484316293\tValidation Loss: 0.0005682499884084108\n",
      "Epoch 214  \tTraining Loss: 0.0005601289152438851\tValidation Loss: 0.0005671459077486587\n",
      "Epoch 215  \tTraining Loss: 0.0005590098438505453\tValidation Loss: 0.0005660510662330149\n",
      "Epoch 216  \tTraining Loss: 0.0005579004898758635\tValidation Loss: 0.0005649654449119444\n",
      "Epoch 217  \tTraining Loss: 0.0005568010550348546\tValidation Loss: 0.0005638895210328467\n",
      "Epoch 218  \tTraining Loss: 0.0005557113778878755\tValidation Loss: 0.0005628221708426559\n",
      "Epoch 219  \tTraining Loss: 0.000554630920826213\tValidation Loss: 0.0005617635390966259\n",
      "Epoch 220  \tTraining Loss: 0.0005535595359032028\tValidation Loss: 0.0005607131449824332\n",
      "Epoch 221  \tTraining Loss: 0.0005524970301232553\tValidation Loss: 0.0005596709338581776\n",
      "Epoch 222  \tTraining Loss: 0.0005514433680953581\tValidation Loss: 0.0005586368963645966\n",
      "Epoch 223  \tTraining Loss: 0.0005503982389858375\tValidation Loss: 0.0005576106436411261\n",
      "Epoch 224  \tTraining Loss: 0.0005493615100645778\tValidation Loss: 0.0005565920251369048\n",
      "Epoch 225  \tTraining Loss: 0.0005483329907708066\tValidation Loss: 0.0005555808174562013\n",
      "Epoch 226  \tTraining Loss: 0.0005473123918773636\tValidation Loss: 0.0005545769071455156\n",
      "Epoch 227  \tTraining Loss: 0.0005462996347047822\tValidation Loss: 0.000553581052801795\n",
      "Epoch 228  \tTraining Loss: 0.0005452946927704627\tValidation Loss: 0.0005525921814681135\n",
      "Epoch 229  \tTraining Loss: 0.0005442973078340428\tValidation Loss: 0.0005516103688530546\n",
      "Epoch 230  \tTraining Loss: 0.0005433074272165053\tValidation Loss: 0.0005506352539172471\n",
      "Epoch 231  \tTraining Loss: 0.0005423247605242009\tValidation Loss: 0.0005496670770714908\n",
      "Epoch 232  \tTraining Loss: 0.0005413492390737358\tValidation Loss: 0.0005487056531155355\n",
      "Epoch 233  \tTraining Loss: 0.0005403808476828792\tValidation Loss: 0.0005477504243929668\n",
      "Epoch 234  \tTraining Loss: 0.000539419370192551\tValidation Loss: 0.0005468012229125994\n",
      "Epoch 235  \tTraining Loss: 0.0005384646605699488\tValidation Loss: 0.0005458583036147719\n",
      "Epoch 236  \tTraining Loss: 0.0005375166210443696\tValidation Loss: 0.000544921324353126\n",
      "Epoch 237  \tTraining Loss: 0.000536575135640454\tValidation Loss: 0.0005439902931297034\n",
      "Epoch 238  \tTraining Loss: 0.0005356400736889588\tValidation Loss: 0.0005430650804518562\n",
      "Epoch 239  \tTraining Loss: 0.0005347111988034638\tValidation Loss: 0.000542145464956633\n",
      "Epoch 240  \tTraining Loss: 0.0005337884667929861\tValidation Loss: 0.0005412313119605821\n",
      "Epoch 241  \tTraining Loss: 0.0005328720989005301\tValidation Loss: 0.000540322891265884\n",
      "Epoch 242  \tTraining Loss: 0.0005319615314950325\tValidation Loss: 0.0005394200064539011\n",
      "Epoch 243  \tTraining Loss: 0.0005310568211684965\tValidation Loss: 0.0005385220594383559\n",
      "Epoch 244  \tTraining Loss: 0.0005301577717707132\tValidation Loss: 0.0005376298907415119\n",
      "Epoch 245  \tTraining Loss: 0.0005292642543751622\tValidation Loss: 0.0005367429427909591\n",
      "Epoch 246  \tTraining Loss: 0.00052837615452078\tValidation Loss: 0.0005358611650520194\n",
      "Epoch 247  \tTraining Loss: 0.0005274934227800339\tValidation Loss: 0.0005349841745750224\n",
      "Epoch 248  \tTraining Loss: 0.0005266158393386157\tValidation Loss: 0.0005341117592961062\n",
      "Epoch 249  \tTraining Loss: 0.0005257433111471437\tValidation Loss: 0.0005332439344963448\n",
      "Epoch 250  \tTraining Loss: 0.0005248758165677377\tValidation Loss: 0.0005323803568538473\n",
      "Epoch 251  \tTraining Loss: 0.000524013313404843\tValidation Loss: 0.0005315212155836806\n",
      "Epoch 252  \tTraining Loss: 0.0005231556533877198\tValidation Loss: 0.000530666208055029\n",
      "Epoch 253  \tTraining Loss: 0.0005223028422258857\tValidation Loss: 0.0005298157094095859\n",
      "Epoch 254  \tTraining Loss: 0.0005214551050864528\tValidation Loss: 0.0005289697576942997\n",
      "Epoch 255  \tTraining Loss: 0.0005206122070185878\tValidation Loss: 0.0005281276177692639\n",
      "Epoch 256  \tTraining Loss: 0.0005197738133194624\tValidation Loss: 0.0005272893983197476\n",
      "Epoch 257  \tTraining Loss: 0.0005189398406990826\tValidation Loss: 0.0005264552968542595\n",
      "Epoch 258  \tTraining Loss: 0.000518110252554322\tValidation Loss: 0.0005256248789313704\n",
      "Epoch 259  \tTraining Loss: 0.0005172849392605261\tValidation Loss: 0.000524798453802386\n",
      "Epoch 260  \tTraining Loss: 0.0005164637830076158\tValidation Loss: 0.000523976311509549\n",
      "Epoch 261  \tTraining Loss: 0.0005156466341324648\tValidation Loss: 0.0005231578667263896\n",
      "Epoch 262  \tTraining Loss: 0.0005148334940221035\tValidation Loss: 0.0005223431692088281\n",
      "Epoch 263  \tTraining Loss: 0.0005140244797364786\tValidation Loss: 0.0005215318646556857\n",
      "Epoch 264  \tTraining Loss: 0.0005132192936717674\tValidation Loss: 0.0005207239732712451\n",
      "Epoch 265  \tTraining Loss: 0.0005124179717852727\tValidation Loss: 0.0005199196113467946\n",
      "Epoch 266  \tTraining Loss: 0.0005116204240680175\tValidation Loss: 0.0005191186301614862\n",
      "Epoch 267  \tTraining Loss: 0.0005108264962979366\tValidation Loss: 0.0005183208440643247\n",
      "Epoch 268  \tTraining Loss: 0.0005100361274876529\tValidation Loss: 0.0005175266163874743\n",
      "Epoch 269  \tTraining Loss: 0.0005092496988430862\tValidation Loss: 0.0005167354310558242\n",
      "Epoch 270  \tTraining Loss: 0.0005084667030837031\tValidation Loss: 0.0005159472908322142\n",
      "Epoch 271  \tTraining Loss: 0.000507687127389062\tValidation Loss: 0.0005151621627858696\n",
      "Epoch 272  \tTraining Loss: 0.0005069109188187658\tValidation Loss: 0.0005143800561851882\n",
      "Epoch 273  \tTraining Loss: 0.0005061379447115858\tValidation Loss: 0.0005136007569814476\n",
      "Epoch 274  \tTraining Loss: 0.000505368105597074\tValidation Loss: 0.000512824254473525\n",
      "Epoch 275  \tTraining Loss: 0.0005046015753020362\tValidation Loss: 0.0005120511154999315\n",
      "Epoch 276  \tTraining Loss: 0.0005038384069195699\tValidation Loss: 0.0005112807893701508\n",
      "Epoch 277  \tTraining Loss: 0.0005030781586592814\tValidation Loss: 0.0005105131428127757\n",
      "Epoch 278  \tTraining Loss: 0.0005023209453938832\tValidation Loss: 0.0005097480925359867\n",
      "Epoch 279  \tTraining Loss: 0.0005015667250051969\tValidation Loss: 0.0005089854508419793\n",
      "Epoch 280  \tTraining Loss: 0.0005008153592927642\tValidation Loss: 0.0005082254700996829\n",
      "Epoch 281  \tTraining Loss: 0.000500066814191228\tValidation Loss: 0.0005074679499683531\n",
      "Epoch 282  \tTraining Loss: 0.0004993211193093774\tValidation Loss: 0.0005067128003934039\n",
      "Epoch 283  \tTraining Loss: 0.0004985780935215021\tValidation Loss: 0.0005059601406187156\n",
      "Epoch 284  \tTraining Loss: 0.0004978377276699361\tValidation Loss: 0.0005052097823518744\n",
      "Epoch 285  \tTraining Loss: 0.0004971002040504856\tValidation Loss: 0.0005044620237592643\n",
      "Epoch 286  \tTraining Loss: 0.0004963655047542695\tValidation Loss: 0.0005037167832631785\n",
      "Epoch 287  \tTraining Loss: 0.00049563344699591\tValidation Loss: 0.0005029739550047429\n",
      "Epoch 288  \tTraining Loss: 0.0004949040468335359\tValidation Loss: 0.0005022334010961452\n",
      "Epoch 289  \tTraining Loss: 0.0004941772305702616\tValidation Loss: 0.0005014951209679001\n",
      "Epoch 290  \tTraining Loss: 0.000493452886710619\tValidation Loss: 0.0005007591676471046\n",
      "Epoch 291  \tTraining Loss: 0.0004927309453687131\tValidation Loss: 0.0005000253581056238\n",
      "Epoch 292  \tTraining Loss: 0.0004920115155087741\tValidation Loss: 0.0004992939855890101\n",
      "Epoch 293  \tTraining Loss: 0.0004912944878811021\tValidation Loss: 0.000498564836755881\n",
      "Epoch 294  \tTraining Loss: 0.0004905798528542007\tValidation Loss: 0.000497837947091145\n",
      "Epoch 295  \tTraining Loss: 0.0004898679789658984\tValidation Loss: 0.0004971133121688715\n",
      "Epoch 296  \tTraining Loss: 0.0004891586451989099\tValidation Loss: 0.000496390988767816\n",
      "Epoch 297  \tTraining Loss: 0.0004884517225456228\tValidation Loss: 0.0004956709622205086\n",
      "Epoch 298  \tTraining Loss: 0.0004877472801032931\tValidation Loss: 0.0004949529831600889\n",
      "Epoch 299  \tTraining Loss: 0.00048704500492188383\tValidation Loss: 0.0004942370620001132\n",
      "Epoch 300  \tTraining Loss: 0.00048634505174566657\tValidation Loss: 0.0004935236112095529\n",
      "Epoch 301  \tTraining Loss: 0.0004856475962290063\tValidation Loss: 0.0004928117788887788\n",
      "Epoch 302  \tTraining Loss: 0.0004849522793358253\tValidation Loss: 0.0004921019123804281\n",
      "Epoch 303  \tTraining Loss: 0.0004842590432065788\tValidation Loss: 0.0004913945827841228\n",
      "Epoch 304  \tTraining Loss: 0.0004835681512919072\tValidation Loss: 0.0004906890799141196\n",
      "Epoch 305  \tTraining Loss: 0.0004828793776838487\tValidation Loss: 0.0004899857362218425\n",
      "Epoch 306  \tTraining Loss: 0.0004821926733981098\tValidation Loss: 0.0004892839900042602\n",
      "Epoch 307  \tTraining Loss: 0.0004815080392402615\tValidation Loss: 0.000488584245654298\n",
      "Epoch 308  \tTraining Loss: 0.0004808254040556044\tValidation Loss: 0.000487886310327685\n",
      "Epoch 309  \tTraining Loss: 0.00048014470247620975\tValidation Loss: 0.00048719030237435003\n",
      "Epoch 310  \tTraining Loss: 0.0004794659952624783\tValidation Loss: 0.00048649620876750437\n",
      "Epoch 311  \tTraining Loss: 0.0004787892881746907\tValidation Loss: 0.00048580424327154353\n",
      "Epoch 312  \tTraining Loss: 0.00047811466723203674\tValidation Loss: 0.0004851144675350503\n",
      "Epoch 313  \tTraining Loss: 0.00047744192914445034\tValidation Loss: 0.0004844265542764816\n",
      "Epoch 314  \tTraining Loss: 0.00047677097613889124\tValidation Loss: 0.0004837404701976029\n",
      "Epoch 315  \tTraining Loss: 0.0004761019631594357\tValidation Loss: 0.0004830568313246595\n",
      "Epoch 316  \tTraining Loss: 0.0004754351580717735\tValidation Loss: 0.000482375170978771\n",
      "Epoch 317  \tTraining Loss: 0.0004747703230234464\tValidation Loss: 0.00048169490412677455\n",
      "Epoch 318  \tTraining Loss: 0.0004741071481456345\tValidation Loss: 0.00048101659074927574\n",
      "Epoch 319  \tTraining Loss: 0.00047344556720310373\tValidation Loss: 0.0004803400672061606\n",
      "Epoch 320  \tTraining Loss: 0.0004727858755293505\tValidation Loss: 0.00047966546097450943\n",
      "Epoch 321  \tTraining Loss: 0.0004721279645886153\tValidation Loss: 0.00047899236892824274\n",
      "Epoch 322  \tTraining Loss: 0.00047147175366353815\tValidation Loss: 0.00047832113293673413\n",
      "Epoch 323  \tTraining Loss: 0.00047081748381964976\tValidation Loss: 0.0004776515481991307\n",
      "Epoch 324  \tTraining Loss: 0.0004701652554717457\tValidation Loss: 0.0004769839376846782\n",
      "Epoch 325  \tTraining Loss: 0.0004695149316962254\tValidation Loss: 0.0004763180404908534\n",
      "Epoch 326  \tTraining Loss: 0.0004688663752904975\tValidation Loss: 0.00047565401598731206\n",
      "Epoch 327  \tTraining Loss: 0.00046821960089500354\tValidation Loss: 0.00047499180183613673\n",
      "Epoch 328  \tTraining Loss: 0.00046757455069386687\tValidation Loss: 0.00047433137473383336\n",
      "Epoch 329  \tTraining Loss: 0.0004669313666503246\tValidation Loss: 0.0004736726201469699\n",
      "Epoch 330  \tTraining Loss: 0.0004662898865277345\tValidation Loss: 0.00047301556665555025\n",
      "Epoch 331  \tTraining Loss: 0.0004656500104466932\tValidation Loss: 0.0004723605655242998\n",
      "Epoch 332  \tTraining Loss: 0.0004650121441578332\tValidation Loss: 0.00047170734031358756\n",
      "Epoch 333  \tTraining Loss: 0.00046437605674218063\tValidation Loss: 0.0004710557899652241\n",
      "Epoch 334  \tTraining Loss: 0.00046374164487942215\tValidation Loss: 0.0004704054672503694\n",
      "Epoch 335  \tTraining Loss: 0.0004631087877339042\tValidation Loss: 0.0004697567231256249\n",
      "Epoch 336  \tTraining Loss: 0.00046247762602392057\tValidation Loss: 0.00046910974080095226\n",
      "Epoch 337  \tTraining Loss: 0.00046184834687728173\tValidation Loss: 0.00046846473199399594\n",
      "Epoch 338  \tTraining Loss: 0.0004612209584754317\tValidation Loss: 0.00046782139707064534\n",
      "Epoch 339  \tTraining Loss: 0.00046059525336157353\tValidation Loss: 0.00046717979781147733\n",
      "Epoch 340  \tTraining Loss: 0.00045997142577562346\tValidation Loss: 0.0004665400451289606\n",
      "Epoch 341  \tTraining Loss: 0.00045934939159581194\tValidation Loss: 0.00046590166769283154\n",
      "Epoch 342  \tTraining Loss: 0.0004587290145546995\tValidation Loss: 0.0004652644992737095\n",
      "Epoch 343  \tTraining Loss: 0.00045811032223176744\tValidation Loss: 0.00046462899160441937\n",
      "Epoch 344  \tTraining Loss: 0.0004574931869716678\tValidation Loss: 0.00046399534493442367\n",
      "Epoch 345  \tTraining Loss: 0.00045687759237152787\tValidation Loss: 0.0004633632574961068\n",
      "Epoch 346  \tTraining Loss: 0.00045626333605071107\tValidation Loss: 0.0004627327715229466\n",
      "Epoch 347  \tTraining Loss: 0.00045565067653098595\tValidation Loss: 0.00046210369243447646\n",
      "Epoch 348  \tTraining Loss: 0.00045503949049681967\tValidation Loss: 0.0004614759109637174\n",
      "Epoch 349  \tTraining Loss: 0.0004544298443738986\tValidation Loss: 0.00046084945292830157\n",
      "Epoch 350  \tTraining Loss: 0.00045382176784730107\tValidation Loss: 0.00046022467869628913\n",
      "Epoch 351  \tTraining Loss: 0.0004532153981649789\tValidation Loss: 0.0004596016363614307\n",
      "Epoch 352  \tTraining Loss: 0.0004526107536217187\tValidation Loss: 0.0004589798308207001\n",
      "Epoch 353  \tTraining Loss: 0.0004520077734308511\tValidation Loss: 0.0004583599856793648\n",
      "Epoch 354  \tTraining Loss: 0.00045140637229917256\tValidation Loss: 0.00045774178868230507\n",
      "Epoch 355  \tTraining Loss: 0.0004508064321642465\tValidation Loss: 0.00045712540522579504\n",
      "Epoch 356  \tTraining Loss: 0.00045020813474746135\tValidation Loss: 0.0004565103552986613\n",
      "Epoch 357  \tTraining Loss: 0.00044961158412907865\tValidation Loss: 0.00045589680033306654\n",
      "Epoch 358  \tTraining Loss: 0.00044901687900889887\tValidation Loss: 0.0004552850452803431\n",
      "Epoch 359  \tTraining Loss: 0.0004484240366315015\tValidation Loss: 0.00045467547920229767\n",
      "Epoch 360  \tTraining Loss: 0.00044783284749084275\tValidation Loss: 0.00045406697236055865\n",
      "Epoch 361  \tTraining Loss: 0.00044724295295276784\tValidation Loss: 0.00045345987420121985\n",
      "Epoch 362  \tTraining Loss: 0.00044665483318154985\tValidation Loss: 0.00045285429944924165\n",
      "Epoch 363  \tTraining Loss: 0.0004460681390921925\tValidation Loss: 0.0004522499782153582\n",
      "Epoch 364  \tTraining Loss: 0.00044548292190441595\tValidation Loss: 0.0004516471736921319\n",
      "Epoch 365  \tTraining Loss: 0.0004448991982692436\tValidation Loss: 0.0004510460736967501\n",
      "Epoch 366  \tTraining Loss: 0.00044431694838491676\tValidation Loss: 0.00045044653447744414\n",
      "Epoch 367  \tTraining Loss: 0.0004437360835514309\tValidation Loss: 0.0004498487471587035\n",
      "Epoch 368  \tTraining Loss: 0.00044315671471942074\tValidation Loss: 0.00044925283479068994\n",
      "Epoch 369  \tTraining Loss: 0.0004425789991474301\tValidation Loss: 0.00044865846065878667\n",
      "Epoch 370  \tTraining Loss: 0.0004420026677672088\tValidation Loss: 0.0004480657018551804\n",
      "Epoch 371  \tTraining Loss: 0.0004414278262405264\tValidation Loss: 0.00044747457642984093\n",
      "Epoch 372  \tTraining Loss: 0.0004408543374572462\tValidation Loss: 0.0004468848536826713\n",
      "Epoch 373  \tTraining Loss: 0.000440282001498339\tValidation Loss: 0.00044629640241538714\n",
      "Epoch 374  \tTraining Loss: 0.00043971104094929606\tValidation Loss: 0.0004457094160164076\n",
      "Epoch 375  \tTraining Loss: 0.00043914139150560885\tValidation Loss: 0.00044512386615815476\n",
      "Epoch 376  \tTraining Loss: 0.00043857308772359433\tValidation Loss: 0.00044453975836705105\n",
      "Epoch 377  \tTraining Loss: 0.00043800613840247227\tValidation Loss: 0.00044395712097887875\n",
      "Epoch 378  \tTraining Loss: 0.0004374405555485489\tValidation Loss: 0.000443376045768867\n",
      "Epoch 379  \tTraining Loss: 0.00043687650366073567\tValidation Loss: 0.00044279637061309834\n",
      "Epoch 380  \tTraining Loss: 0.00043631378104288257\tValidation Loss: 0.0004422180876816955\n",
      "Epoch 381  \tTraining Loss: 0.00043575234250102426\tValidation Loss: 0.00044164108841555076\n",
      "Epoch 382  \tTraining Loss: 0.00043519235081358076\tValidation Loss: 0.0004410656156052815\n",
      "Epoch 383  \tTraining Loss: 0.00043463394479989806\tValidation Loss: 0.00044049154997439035\n",
      "Epoch 384  \tTraining Loss: 0.00043407679777007736\tValidation Loss: 0.00043991865949540715\n",
      "Epoch 385  \tTraining Loss: 0.0004335207354991772\tValidation Loss: 0.0004393471980729979\n",
      "Epoch 386  \tTraining Loss: 0.00043296591281656355\tValidation Loss: 0.0004387768380955001\n",
      "Epoch 387  \tTraining Loss: 0.0004324123638106716\tValidation Loss: 0.00043820812731830124\n",
      "Epoch 388  \tTraining Loss: 0.0004318602688368215\tValidation Loss: 0.0004376405863508552\n",
      "Epoch 389  \tTraining Loss: 0.0004313095946632515\tValidation Loss: 0.00043707472954390464\n",
      "Epoch 390  \tTraining Loss: 0.00043076017193554813\tValidation Loss: 0.00043651019762397196\n",
      "Epoch 391  \tTraining Loss: 0.0004302117608511369\tValidation Loss: 0.0004359471140610823\n",
      "Epoch 392  \tTraining Loss: 0.0004296647090155219\tValidation Loss: 0.0004353849948699672\n",
      "Epoch 393  \tTraining Loss: 0.0004291188795425395\tValidation Loss: 0.0004348244152037726\n",
      "Epoch 394  \tTraining Loss: 0.0004285742649305658\tValidation Loss: 0.0004342650278663292\n",
      "Epoch 395  \tTraining Loss: 0.000428030798968189\tValidation Loss: 0.00043370693232222686\n",
      "Epoch 396  \tTraining Loss: 0.0004274886532341062\tValidation Loss: 0.0004331501214013509\n",
      "Epoch 397  \tTraining Loss: 0.00042694788684138587\tValidation Loss: 0.0004325943198795263\n",
      "Epoch 398  \tTraining Loss: 0.00042640837623829516\tValidation Loss: 0.00043203995396316937\n",
      "Epoch 399  \tTraining Loss: 0.00042587011090061065\tValidation Loss: 0.0004314870774924787\n",
      "Epoch 400  \tTraining Loss: 0.0004253331000772792\tValidation Loss: 0.00043093533075234746\n",
      "Epoch 401  \tTraining Loss: 0.0004247972529727514\tValidation Loss: 0.0004303849381585009\n",
      "Epoch 402  \tTraining Loss: 0.0004242625053407673\tValidation Loss: 0.00042983536495328276\n",
      "Epoch 403  \tTraining Loss: 0.00042372879953548085\tValidation Loss: 0.0004292869491368568\n",
      "Epoch 404  \tTraining Loss: 0.00042319626226359716\tValidation Loss: 0.0004287398675341845\n",
      "Epoch 405  \tTraining Loss: 0.0004226648665181567\tValidation Loss: 0.00042819394110185595\n",
      "Epoch 406  \tTraining Loss: 0.0004221346288755455\tValidation Loss: 0.0004276493712447036\n",
      "Epoch 407  \tTraining Loss: 0.00042160545480643234\tValidation Loss: 0.0004271061209302742\n",
      "Epoch 408  \tTraining Loss: 0.0004210772292422705\tValidation Loss: 0.00042656400284298165\n",
      "Epoch 409  \tTraining Loss: 0.0004205501076353343\tValidation Loss: 0.00042602334685970956\n",
      "Epoch 410  \tTraining Loss: 0.0004200239739075302\tValidation Loss: 0.0004254835825668222\n",
      "Epoch 411  \tTraining Loss: 0.0004194989962160273\tValidation Loss: 0.0004249448866890703\n",
      "Epoch 412  \tTraining Loss: 0.00041897507996520837\tValidation Loss: 0.00042440731693701517\n",
      "Epoch 413  \tTraining Loss: 0.0004184522797641153\tValidation Loss: 0.0004238708958414876\n",
      "Epoch 414  \tTraining Loss: 0.0004179307140761974\tValidation Loss: 0.00042333556008962344\n",
      "Epoch 415  \tTraining Loss: 0.0004174102284762796\tValidation Loss: 0.0004228015688843416\n",
      "Epoch 416  \tTraining Loss: 0.00041689085830115286\tValidation Loss: 0.00042226861008274637\n",
      "Epoch 417  \tTraining Loss: 0.0004163724725239265\tValidation Loss: 0.0004217370197971015\n",
      "Epoch 418  \tTraining Loss: 0.0004158550507233376\tValidation Loss: 0.00042120669965511307\n",
      "Epoch 419  \tTraining Loss: 0.0004153386433301741\tValidation Loss: 0.00042067746271045507\n",
      "Epoch 420  \tTraining Loss: 0.00041482327482710393\tValidation Loss: 0.0004201492124841598\n",
      "Epoch 421  \tTraining Loss: 0.00041430898417938795\tValidation Loss: 0.00041962198171927945\n",
      "Epoch 422  \tTraining Loss: 0.0004137957775162387\tValidation Loss: 0.00041909559609227826\n",
      "Epoch 423  \tTraining Loss: 0.0004132834676765383\tValidation Loss: 0.00041857089615575827\n",
      "Epoch 424  \tTraining Loss: 0.00041277237402781913\tValidation Loss: 0.00041804676034715903\n",
      "Epoch 425  \tTraining Loss: 0.00041226223139527706\tValidation Loss: 0.00041752340502312565\n",
      "Epoch 426  \tTraining Loss: 0.00041175296163546395\tValidation Loss: 0.0004170010491071045\n",
      "Epoch 427  \tTraining Loss: 0.00041124474020915244\tValidation Loss: 0.0004164796382073175\n",
      "Epoch 428  \tTraining Loss: 0.00041073751014476654\tValidation Loss: 0.0004159591791496337\n",
      "Epoch 429  \tTraining Loss: 0.00041023132339547825\tValidation Loss: 0.00041543975099132255\n",
      "Epoch 430  \tTraining Loss: 0.0004097262571030465\tValidation Loss: 0.0004149214372160448\n",
      "Epoch 431  \tTraining Loss: 0.0004092222378989029\tValidation Loss: 0.0004144040745519088\n",
      "Epoch 432  \tTraining Loss: 0.00040871901256687803\tValidation Loss: 0.0004138874358544902\n",
      "Epoch 433  \tTraining Loss: 0.0004082168226507419\tValidation Loss: 0.0004133719440440406\n",
      "Epoch 434  \tTraining Loss: 0.00040771556330380416\tValidation Loss: 0.0004128572406225934\n",
      "Epoch 435  \tTraining Loss: 0.0004072151030432194\tValidation Loss: 0.00041234330621679386\n",
      "Epoch 436  \tTraining Loss: 0.00040671540367211373\tValidation Loss: 0.0004118302906444766\n",
      "Epoch 437  \tTraining Loss: 0.0004062165096443088\tValidation Loss: 0.0004113180941781688\n",
      "Epoch 438  \tTraining Loss: 0.0004057185023977572\tValidation Loss: 0.0004108067753751895\n",
      "Epoch 439  \tTraining Loss: 0.0004052213545406904\tValidation Loss: 0.00041029639923687516\n",
      "Epoch 440  \tTraining Loss: 0.000404725144919437\tValidation Loss: 0.00040978716049605313\n",
      "Epoch 441  \tTraining Loss: 0.0004042298041867081\tValidation Loss: 0.00040927872836643304\n",
      "Epoch 442  \tTraining Loss: 0.00040373529492486335\tValidation Loss: 0.00040877109349815463\n",
      "Epoch 443  \tTraining Loss: 0.00040324161090596696\tValidation Loss: 0.000408264274535194\n",
      "Epoch 444  \tTraining Loss: 0.0004027488559817245\tValidation Loss: 0.00040775834431767456\n",
      "Epoch 445  \tTraining Loss: 0.0004022570345946954\tValidation Loss: 0.00040725341234394024\n",
      "Epoch 446  \tTraining Loss: 0.00040176632428190527\tValidation Loss: 0.0004067492670884587\n",
      "Epoch 447  \tTraining Loss: 0.0004012764518677392\tValidation Loss: 0.0004062460513868507\n",
      "Epoch 448  \tTraining Loss: 0.00040078743668951007\tValidation Loss: 0.00040574404841018383\n",
      "Epoch 449  \tTraining Loss: 0.000400299419628841\tValidation Loss: 0.00040524301017199547\n",
      "Epoch 450  \tTraining Loss: 0.0003998123179196288\tValidation Loss: 0.00040474282333642403\n",
      "Epoch 451  \tTraining Loss: 0.00039932611556968157\tValidation Loss: 0.0004042436323022513\n",
      "Epoch 452  \tTraining Loss: 0.000398840756969492\tValidation Loss: 0.0004037453379865677\n",
      "Epoch 453  \tTraining Loss: 0.0003983561113498861\tValidation Loss: 0.00040324780798236875\n",
      "Epoch 454  \tTraining Loss: 0.00039787228324456546\tValidation Loss: 0.0004027507940583316\n",
      "Epoch 455  \tTraining Loss: 0.0003973892434573982\tValidation Loss: 0.00040225478569444\n",
      "Epoch 456  \tTraining Loss: 0.0003969070073902728\tValidation Loss: 0.00040175952040786\n",
      "Epoch 457  \tTraining Loss: 0.0003964255603879309\tValidation Loss: 0.00040126504683976225\n",
      "Epoch 458  \tTraining Loss: 0.0003959448719087028\tValidation Loss: 0.00040077142205231076\n",
      "Epoch 459  \tTraining Loss: 0.0003954649999854369\tValidation Loss: 0.00040027848324767837\n",
      "Epoch 460  \tTraining Loss: 0.0003949859299672763\tValidation Loss: 0.000399786225451851\n",
      "Epoch 461  \tTraining Loss: 0.00039450765685833405\tValidation Loss: 0.0003992946439853449\n",
      "Epoch 462  \tTraining Loss: 0.0003940302417911429\tValidation Loss: 0.00039880369555345406\n",
      "Epoch 463  \tTraining Loss: 0.00039355371097701396\tValidation Loss: 0.0003983137137891804\n",
      "Epoch 464  \tTraining Loss: 0.00039307798207029415\tValidation Loss: 0.00039782452150585485\n",
      "Epoch 465  \tTraining Loss: 0.000392603027099971\tValidation Loss: 0.0003973360923164333\n",
      "Epoch 466  \tTraining Loss: 0.00039212883957407557\tValidation Loss: 0.0003968483564026346\n",
      "Epoch 467  \tTraining Loss: 0.0003916553875217361\tValidation Loss: 0.00039636126837736246\n",
      "Epoch 468  \tTraining Loss: 0.00039118260118137314\tValidation Loss: 0.00039587474926670586\n",
      "Epoch 469  \tTraining Loss: 0.0003907104658545882\tValidation Loss: 0.0003953888321987145\n",
      "Epoch 470  \tTraining Loss: 0.0003902389688134099\tValidation Loss: 0.0003949035851599518\n",
      "Epoch 471  \tTraining Loss: 0.0003897681794395296\tValidation Loss: 0.0003944192231089853\n",
      "Epoch 472  \tTraining Loss: 0.0003892981363998819\tValidation Loss: 0.00039393542697045936\n",
      "Epoch 473  \tTraining Loss: 0.000388828741576766\tValidation Loss: 0.00039345229608949014\n",
      "Epoch 474  \tTraining Loss: 0.0003883600622517909\tValidation Loss: 0.00039296989240960727\n",
      "Epoch 475  \tTraining Loss: 0.0003878921701179672\tValidation Loss: 0.0003924881313258497\n",
      "Epoch 476  \tTraining Loss: 0.0003874250488737536\tValidation Loss: 0.00039200730219754725\n",
      "Epoch 477  \tTraining Loss: 0.0003869586270000555\tValidation Loss: 0.00039152705871344076\n",
      "Epoch 478  \tTraining Loss: 0.00038649291805502225\tValidation Loss: 0.0003910474085800967\n",
      "Epoch 479  \tTraining Loss: 0.0003860278178096762\tValidation Loss: 0.0003905686615241044\n",
      "Epoch 480  \tTraining Loss: 0.00038556339427502973\tValidation Loss: 0.0003900902712845627\n",
      "Epoch 481  \tTraining Loss: 0.00038509962238598646\tValidation Loss: 0.0003896127698587162\n",
      "Epoch 482  \tTraining Loss: 0.0003846364415849532\tValidation Loss: 0.0003891356157365199\n",
      "Epoch 483  \tTraining Loss: 0.00038417399901506937\tValidation Loss: 0.0003886593288673742\n",
      "Epoch 484  \tTraining Loss: 0.0003837122078943445\tValidation Loss: 0.000388184107043981\n",
      "Epoch 485  \tTraining Loss: 0.00038325111351589024\tValidation Loss: 0.0003877092747624149\n",
      "Epoch 486  \tTraining Loss: 0.0003827908398098858\tValidation Loss: 0.0003872353831392157\n",
      "Epoch 487  \tTraining Loss: 0.00038233135649090767\tValidation Loss: 0.0003867621010065598\n",
      "Epoch 488  \tTraining Loss: 0.00038187250205312445\tValidation Loss: 0.0003862893615715591\n",
      "Epoch 489  \tTraining Loss: 0.0003814142384119903\tValidation Loss: 0.00038581719322980303\n",
      "Epoch 490  \tTraining Loss: 0.00038095665304699496\tValidation Loss: 0.00038534567202333964\n",
      "Epoch 491  \tTraining Loss: 0.00038049974283618115\tValidation Loss: 0.0003848746940931731\n",
      "Epoch 492  \tTraining Loss: 0.0003800434896707109\tValidation Loss: 0.0003844044642331999\n",
      "Epoch 493  \tTraining Loss: 0.0003795879054161233\tValidation Loss: 0.0003839349577059363\n",
      "Epoch 494  \tTraining Loss: 0.000379132982267801\tValidation Loss: 0.00038346592074053887\n",
      "Epoch 495  \tTraining Loss: 0.0003786788078239431\tValidation Loss: 0.00038299746901406315\n",
      "Epoch 496  \tTraining Loss: 0.00037822543700391346\tValidation Loss: 0.0003825295860034833\n",
      "Epoch 497  \tTraining Loss: 0.00037777267980850524\tValidation Loss: 0.00038206210084390713\n",
      "Epoch 498  \tTraining Loss: 0.00037732055054658494\tValidation Loss: 0.0003815952986446181\n",
      "Epoch 499  \tTraining Loss: 0.0003768690981985872\tValidation Loss: 0.0003811289432848716\n",
      "Epoch 500  \tTraining Loss: 0.0003764182093856821\tValidation Loss: 0.00038066319505675243\n",
      "Epoch 501  \tTraining Loss: 0.0003759679133861188\tValidation Loss: 0.00038019806884155935\n",
      "Epoch 502  \tTraining Loss: 0.0003755182462714134\tValidation Loss: 0.0003797334345349329\n",
      "Epoch 503  \tTraining Loss: 0.00037506917021400246\tValidation Loss: 0.00037926941682494875\n",
      "Epoch 504  \tTraining Loss: 0.0003746207203971587\tValidation Loss: 0.0003788058756630891\n",
      "Epoch 505  \tTraining Loss: 0.0003741728184674896\tValidation Loss: 0.0003783429756430976\n",
      "Epoch 506  \tTraining Loss: 0.0003737255554136943\tValidation Loss: 0.00037788061870808133\n",
      "Epoch 507  \tTraining Loss: 0.0003732788095694375\tValidation Loss: 0.00037741895388335265\n",
      "Epoch 508  \tTraining Loss: 0.00037283267281072556\tValidation Loss: 0.0003769577571058671\n",
      "Epoch 509  \tTraining Loss: 0.0003723870847029597\tValidation Loss: 0.0003764971555887292\n",
      "Epoch 510  \tTraining Loss: 0.0003719420935472138\tValidation Loss: 0.0003760374605393603\n",
      "Epoch 511  \tTraining Loss: 0.0003714977435585131\tValidation Loss: 0.0003755781105661879\n",
      "Epoch 512  \tTraining Loss: 0.000371053997311699\tValidation Loss: 0.00037511917846256014\n",
      "Epoch 513  \tTraining Loss: 0.00037061088559461154\tValidation Loss: 0.00037466081345044846\n",
      "Epoch 514  \tTraining Loss: 0.0003701683614597442\tValidation Loss: 0.0003742029686274942\n",
      "Epoch 515  \tTraining Loss: 0.00036972638869677677\tValidation Loss: 0.0003737456607139069\n",
      "Epoch 516  \tTraining Loss: 0.0003692849570630895\tValidation Loss: 0.0003732888485555868\n",
      "Epoch 517  \tTraining Loss: 0.00036884405682998983\tValidation Loss: 0.00037283254367575624\n",
      "Epoch 518  \tTraining Loss: 0.0003684036865405701\tValidation Loss: 0.0003723769450611169\n",
      "Epoch 519  \tTraining Loss: 0.0003679638159751094\tValidation Loss: 0.0003719218261928728\n",
      "Epoch 520  \tTraining Loss: 0.00036752454419885876\tValidation Loss: 0.0003714672618370425\n",
      "Epoch 521  \tTraining Loss: 0.0003670858101962389\tValidation Loss: 0.0003710131981791699\n",
      "Epoch 522  \tTraining Loss: 0.00036664757911283576\tValidation Loss: 0.00037055956538337506\n",
      "Epoch 523  \tTraining Loss: 0.0003662098273071527\tValidation Loss: 0.0003701065357247706\n",
      "Epoch 524  \tTraining Loss: 0.00036577263457507634\tValidation Loss: 0.00036965398450316715\n",
      "Epoch 525  \tTraining Loss: 0.00036533596975792094\tValidation Loss: 0.00036920182705583895\n",
      "Epoch 526  \tTraining Loss: 0.00036489979450765093\tValidation Loss: 0.0003687501577620256\n",
      "Epoch 527  \tTraining Loss: 0.0003644640912638927\tValidation Loss: 0.0003682990049338805\n",
      "Epoch 528  \tTraining Loss: 0.0003640290241699766\tValidation Loss: 0.0003678484556957972\n",
      "Epoch 529  \tTraining Loss: 0.0003635944966106949\tValidation Loss: 0.000367398372228191\n",
      "Epoch 530  \tTraining Loss: 0.0003631604574429271\tValidation Loss: 0.00036694868765240087\n",
      "Epoch 531  \tTraining Loss: 0.00036272692015323996\tValidation Loss: 0.0003664995928791011\n",
      "Epoch 532  \tTraining Loss: 0.00036229389705721727\tValidation Loss: 0.00036605093790075644\n",
      "Epoch 533  \tTraining Loss: 0.0003618613023563187\tValidation Loss: 0.00036560302716382765\n",
      "Epoch 534  \tTraining Loss: 0.0003614292396748943\tValidation Loss: 0.0003651556001332254\n",
      "Epoch 535  \tTraining Loss: 0.00036099771244474485\tValidation Loss: 0.0003647086564036906\n",
      "Epoch 536  \tTraining Loss: 0.000360566742353427\tValidation Loss: 0.0003642622279338402\n",
      "Epoch 537  \tTraining Loss: 0.0003601363421391043\tValidation Loss: 0.0003638162755723321\n",
      "Epoch 538  \tTraining Loss: 0.000359706455490209\tValidation Loss: 0.0003633707448867427\n",
      "Epoch 539  \tTraining Loss: 0.000359277033837124\tValidation Loss: 0.00036292567374519917\n",
      "Epoch 540  \tTraining Loss: 0.0003588480992115014\tValidation Loss: 0.0003624810696189486\n",
      "Epoch 541  \tTraining Loss: 0.00035841972411834413\tValidation Loss: 0.00036203697347608787\n",
      "Epoch 542  \tTraining Loss: 0.0003579919201816686\tValidation Loss: 0.0003615933670674475\n",
      "Epoch 543  \tTraining Loss: 0.0003575646404055363\tValidation Loss: 0.0003611503913613333\n",
      "Epoch 544  \tTraining Loss: 0.00035713793149716704\tValidation Loss: 0.000360707707928373\n",
      "Epoch 545  \tTraining Loss: 0.000356711834851217\tValidation Loss: 0.0003602657626524673\n",
      "Epoch 546  \tTraining Loss: 0.0003562862594717597\tValidation Loss: 0.00035982432168928596\n",
      "Epoch 547  \tTraining Loss: 0.00035586121077802867\tValidation Loss: 0.0003593833163744566\n",
      "Epoch 548  \tTraining Loss: 0.0003554366303877796\tValidation Loss: 0.0003589426625758265\n",
      "Epoch 549  \tTraining Loss: 0.0003550125691594944\tValidation Loss: 0.00035850269646009426\n",
      "Epoch 550  \tTraining Loss: 0.00035458897585206905\tValidation Loss: 0.00035806296303294574\n",
      "Epoch 551  \tTraining Loss: 0.00035416582996655103\tValidation Loss: 0.00035762382865512537\n",
      "Epoch 552  \tTraining Loss: 0.00035374319233980945\tValidation Loss: 0.0003571850712420311\n",
      "Epoch 553  \tTraining Loss: 0.00035332105546051105\tValidation Loss: 0.00035674685516001107\n",
      "Epoch 554  \tTraining Loss: 0.00035289937462973836\tValidation Loss: 0.00035630875551749547\n",
      "Epoch 555  \tTraining Loss: 0.00035247806065071875\tValidation Loss: 0.00035587133150907584\n",
      "Epoch 556  \tTraining Loss: 0.00035205724588954865\tValidation Loss: 0.0003554344715447857\n",
      "Epoch 557  \tTraining Loss: 0.0003516369413307396\tValidation Loss: 0.00035499817252333896\n",
      "Epoch 558  \tTraining Loss: 0.00035121711731828125\tValidation Loss: 0.00035456235049554854\n",
      "Epoch 559  \tTraining Loss: 0.0003507977502142578\tValidation Loss: 0.00035412669069994436\n",
      "Epoch 560  \tTraining Loss: 0.0003503788045792467\tValidation Loss: 0.0003536915681312636\n",
      "Epoch 561  \tTraining Loss: 0.00034996041220361604\tValidation Loss: 0.0003532570603356059\n",
      "Epoch 562  \tTraining Loss: 0.00034954260778239277\tValidation Loss: 0.00035282341541831156\n",
      "Epoch 563  \tTraining Loss: 0.00034912530720517746\tValidation Loss: 0.0003523901600872004\n",
      "Epoch 564  \tTraining Loss: 0.00034870849795917903\tValidation Loss: 0.0003519573736663529\n",
      "Epoch 565  \tTraining Loss: 0.0003482921858813875\tValidation Loss: 0.0003515250226040584\n",
      "Epoch 566  \tTraining Loss: 0.00034787631921912265\tValidation Loss: 0.00035109314530945546\n",
      "Epoch 567  \tTraining Loss: 0.00034746092348831887\tValidation Loss: 0.00035066172850273686\n",
      "Epoch 568  \tTraining Loss: 0.0003470460053282409\tValidation Loss: 0.0003502307663956329\n",
      "Epoch 569  \tTraining Loss: 0.00034663157205321114\tValidation Loss: 0.00034980028577522287\n",
      "Epoch 570  \tTraining Loss: 0.00034621774198958274\tValidation Loss: 0.00034937036293380746\n",
      "Epoch 571  \tTraining Loss: 0.0003458044377899925\tValidation Loss: 0.00034894096563615974\n",
      "Epoch 572  \tTraining Loss: 0.0003453916222271825\tValidation Loss: 0.00034851208498643\n",
      "Epoch 573  \tTraining Loss: 0.0003449793320104847\tValidation Loss: 0.00034808371921187675\n",
      "Epoch 574  \tTraining Loss: 0.00034456763459725533\tValidation Loss: 0.0003476560058658538\n",
      "Epoch 575  \tTraining Loss: 0.0003441565202278827\tValidation Loss: 0.00034722878618912605\n",
      "Epoch 576  \tTraining Loss: 0.00034374592771148213\tValidation Loss: 0.00034680209342724555\n",
      "Epoch 577  \tTraining Loss: 0.0003433358663522075\tValidation Loss: 0.00034637583915000665\n",
      "Epoch 578  \tTraining Loss: 0.00034292632700383895\tValidation Loss: 0.00034595014655349496\n",
      "Epoch 579  \tTraining Loss: 0.0003425172838883993\tValidation Loss: 0.0003455249867176991\n",
      "Epoch 580  \tTraining Loss: 0.0003421087857491182\tValidation Loss: 0.00034510031752234895\n",
      "Epoch 581  \tTraining Loss: 0.00034170080773546246\tValidation Loss: 0.0003446761715755855\n",
      "Epoch 582  \tTraining Loss: 0.00034129343139894017\tValidation Loss: 0.0003442527587764005\n",
      "Epoch 583  \tTraining Loss: 0.0003408866357840689\tValidation Loss: 0.0003438298319289561\n",
      "Epoch 584  \tTraining Loss: 0.00034048035595236886\tValidation Loss: 0.00034340734333040073\n",
      "Epoch 585  \tTraining Loss: 0.00034007459411107283\tValidation Loss: 0.0003429853598380147\n",
      "Epoch 586  \tTraining Loss: 0.00033966947271115295\tValidation Loss: 0.0003425639950552961\n",
      "Epoch 587  \tTraining Loss: 0.00033926496286320435\tValidation Loss: 0.0003421433124131939\n",
      "Epoch 588  \tTraining Loss: 0.00033886099944766457\tValidation Loss: 0.00034172320414481904\n",
      "Epoch 589  \tTraining Loss: 0.0003384575768105109\tValidation Loss: 0.00034130362772257856\n",
      "Epoch 590  \tTraining Loss: 0.0003380546917811697\tValidation Loss: 0.00034088458861044176\n",
      "Epoch 591  \tTraining Loss: 0.00033765233195746984\tValidation Loss: 0.0003404661353741532\n",
      "Epoch 592  \tTraining Loss: 0.00033725047283428874\tValidation Loss: 0.000340048259306057\n",
      "Epoch 593  \tTraining Loss: 0.00033684929315157264\tValidation Loss: 0.0003396310788544988\n",
      "Epoch 594  \tTraining Loss: 0.00033644882175714447\tValidation Loss: 0.0003392143056433149\n",
      "Epoch 595  \tTraining Loss: 0.000336048951377289\tValidation Loss: 0.00033879809560151414\n",
      "Epoch 596  \tTraining Loss: 0.0003356496511080313\tValidation Loss: 0.00033838246731613184\n",
      "Epoch 597  \tTraining Loss: 0.00033525093931272584\tValidation Loss: 0.0003379674131739356\n",
      "Epoch 598  \tTraining Loss: 0.0003348528691513867\tValidation Loss: 0.00033755308652489433\n",
      "Epoch 599  \tTraining Loss: 0.0003344554074196687\tValidation Loss: 0.00033713939611691734\n",
      "Epoch 600  \tTraining Loss: 0.0003340586219816605\tValidation Loss: 0.00033672632243384045\n",
      "Epoch 601  \tTraining Loss: 0.00033366249909175125\tValidation Loss: 0.00033631395836432955\n",
      "Epoch 602  \tTraining Loss: 0.00033326701753701334\tValidation Loss: 0.0003359023692879149\n",
      "Epoch 603  \tTraining Loss: 0.0003328722088369403\tValidation Loss: 0.0003354914556185676\n",
      "Epoch 604  \tTraining Loss: 0.00033247802592849105\tValidation Loss: 0.00033508118019236866\n",
      "Epoch 605  \tTraining Loss: 0.00033208447401438017\tValidation Loss: 0.00033467152819403455\n",
      "Epoch 606  \tTraining Loss: 0.00033169155455338065\tValidation Loss: 0.0003342625036147373\n",
      "Epoch 607  \tTraining Loss: 0.0003312992449933125\tValidation Loss: 0.0003338541073722067\n",
      "Epoch 608  \tTraining Loss: 0.00033090756480276133\tValidation Loss: 0.0003334463399221663\n",
      "Epoch 609  \tTraining Loss: 0.000330516558783328\tValidation Loss: 0.0003330392646508675\n",
      "Epoch 610  \tTraining Loss: 0.00033012619603544626\tValidation Loss: 0.0003326328524467211\n",
      "Epoch 611  \tTraining Loss: 0.00032973651347566093\tValidation Loss: 0.00033222726367606356\n",
      "Epoch 612  \tTraining Loss: 0.0003293476550785454\tValidation Loss: 0.00033182257126372394\n",
      "Epoch 613  \tTraining Loss: 0.00032895959958220754\tValidation Loss: 0.00033141874228333147\n",
      "Epoch 614  \tTraining Loss: 0.0003285722703053207\tValidation Loss: 0.00033101537650823\n",
      "Epoch 615  \tTraining Loss: 0.00032818562415945024\tValidation Loss: 0.00033061266496164254\n",
      "Epoch 616  \tTraining Loss: 0.0003277996695870806\tValidation Loss: 0.0003302105987306522\n",
      "Epoch 617  \tTraining Loss: 0.00032741439531184217\tValidation Loss: 0.00032980922179209817\n",
      "Epoch 618  \tTraining Loss: 0.00032702979482736755\tValidation Loss: 0.00032940852788726797\n",
      "Epoch 619  \tTraining Loss: 0.00032664586904296116\tValidation Loss: 0.00032900854542887534\n",
      "Epoch 620  \tTraining Loss: 0.00032626262629988865\tValidation Loss: 0.0003286092593899762\n",
      "Epoch 621  \tTraining Loss: 0.0003258800942941813\tValidation Loss: 0.00032821081816522627\n",
      "Epoch 622  \tTraining Loss: 0.000325498261312418\tValidation Loss: 0.0003278131091125792\n",
      "Epoch 623  \tTraining Loss: 0.0003251171300369256\tValidation Loss: 0.00032741605675234346\n",
      "Epoch 624  \tTraining Loss: 0.0003247367180684089\tValidation Loss: 0.0003270197199618115\n",
      "Epoch 625  \tTraining Loss: 0.0003243570415002468\tValidation Loss: 0.00032662419472507864\n",
      "Epoch 626  \tTraining Loss: 0.0003239780844754497\tValidation Loss: 0.00032622943595246595\n",
      "Epoch 627  \tTraining Loss: 0.00032359984318091126\tValidation Loss: 0.00032583542528341614\n",
      "Epoch 628  \tTraining Loss: 0.0003232223283450008\tValidation Loss: 0.00032544219833803535\n",
      "Epoch 629  \tTraining Loss: 0.0003228455653699095\tValidation Loss: 0.00032504970777592787\n",
      "Epoch 630  \tTraining Loss: 0.0003224695274716537\tValidation Loss: 0.00032465796680277814\n",
      "Epoch 631  \tTraining Loss: 0.00032209434258661515\tValidation Loss: 0.00032426737882267174\n",
      "Epoch 632  \tTraining Loss: 0.00032172007261506636\tValidation Loss: 0.0003238774411015756\n",
      "Epoch 633  \tTraining Loss: 0.00032134657507943006\tValidation Loss: 0.0003234882335779396\n",
      "Epoch 634  \tTraining Loss: 0.0003209738185576116\tValidation Loss: 0.00032309982723303406\n",
      "Epoch 635  \tTraining Loss: 0.00032060184438897297\tValidation Loss: 0.0003227122217622393\n",
      "Epoch 636  \tTraining Loss: 0.0003202306921086624\tValidation Loss: 0.00032232547737994833\n",
      "Epoch 637  \tTraining Loss: 0.0003198603330612333\tValidation Loss: 0.00032193954731455737\n",
      "Epoch 638  \tTraining Loss: 0.0003194907541136831\tValidation Loss: 0.0003215544465405062\n",
      "Epoch 639  \tTraining Loss: 0.0003191219615603573\tValidation Loss: 0.0003211702153488904\n",
      "Epoch 640  \tTraining Loss: 0.00031875397689334407\tValidation Loss: 0.00032078673237867977\n",
      "Epoch 641  \tTraining Loss: 0.0003183868452634566\tValidation Loss: 0.0003204043693536886\n",
      "Epoch 642  \tTraining Loss: 0.0003180205733569916\tValidation Loss: 0.0003200227170925237\n",
      "Epoch 643  \tTraining Loss: 0.0003176551117029581\tValidation Loss: 0.00031964151975695824\n",
      "Epoch 644  \tTraining Loss: 0.00031729047050048094\tValidation Loss: 0.0003192612511783425\n",
      "Epoch 645  \tTraining Loss: 0.0003169266013678006\tValidation Loss: 0.00031888196304585544\n",
      "Epoch 646  \tTraining Loss: 0.00031656357904447135\tValidation Loss: 0.0003185036757427672\n",
      "Epoch 647  \tTraining Loss: 0.0003162013946134106\tValidation Loss: 0.0003181261859376093\n",
      "Epoch 648  \tTraining Loss: 0.00031584001454227994\tValidation Loss: 0.00031774954239237595\n",
      "Epoch 649  \tTraining Loss: 0.0003154794574643089\tValidation Loss: 0.0003173737935856512\n",
      "Epoch 650  \tTraining Loss: 0.000315119720477554\tValidation Loss: 0.0003169991478910172\n",
      "Epoch 651  \tTraining Loss: 0.00031476085011632616\tValidation Loss: 0.0003166254377238152\n",
      "Epoch 652  \tTraining Loss: 0.0003144028160689455\tValidation Loss: 0.00031625275060839656\n",
      "Epoch 653  \tTraining Loss: 0.0003140456312930564\tValidation Loss: 0.0003158807485612854\n",
      "Epoch 654  \tTraining Loss: 0.0003136893061985971\tValidation Loss: 0.00031550994750458516\n",
      "Epoch 655  \tTraining Loss: 0.000313333846681801\tValidation Loss: 0.0003151399358412834\n",
      "Epoch 656  \tTraining Loss: 0.0003129792790013739\tValidation Loss: 0.0003147708477574112\n",
      "Epoch 657  \tTraining Loss: 0.00031262554809627954\tValidation Loss: 0.0003144026809480836\n",
      "Epoch 658  \tTraining Loss: 0.0003122726811810964\tValidation Loss: 0.00031403547015717133\n",
      "Epoch 659  \tTraining Loss: 0.0003119207519824699\tValidation Loss: 0.0003136688524800739\n",
      "Epoch 660  \tTraining Loss: 0.00031156972630516443\tValidation Loss: 0.0003133033901545634\n",
      "Epoch 661  \tTraining Loss: 0.0003112195870484296\tValidation Loss: 0.0003129389433313784\n",
      "Epoch 662  \tTraining Loss: 0.00031087031991454774\tValidation Loss: 0.0003125754520164224\n",
      "Epoch 663  \tTraining Loss: 0.00031052192626626065\tValidation Loss: 0.0003122128849931527\n",
      "Epoch 664  \tTraining Loss: 0.00031017439790306865\tValidation Loss: 0.0003118512570134299\n",
      "Epoch 665  \tTraining Loss: 0.00030982775898442423\tValidation Loss: 0.00031149056131415096\n",
      "Epoch 666  \tTraining Loss: 0.0003094820024745165\tValidation Loss: 0.0003111308084825556\n",
      "Epoch 667  \tTraining Loss: 0.00030913713050239976\tValidation Loss: 0.00031077204513057334\n",
      "Epoch 668  \tTraining Loss: 0.0003087931467015835\tValidation Loss: 0.00031041415013398355\n",
      "Epoch 669  \tTraining Loss: 0.0003084500661242247\tValidation Loss: 0.00031005727093487374\n",
      "Epoch 670  \tTraining Loss: 0.00030810789161772714\tValidation Loss: 0.000309701767752925\n",
      "Epoch 671  \tTraining Loss: 0.00030776665482739\tValidation Loss: 0.00030934692544246594\n",
      "Epoch 672  \tTraining Loss: 0.0003074263045747569\tValidation Loss: 0.0003089931595384116\n",
      "Epoch 673  \tTraining Loss: 0.0003070868443097735\tValidation Loss: 0.0003086400297601073\n",
      "Epoch 674  \tTraining Loss: 0.0003067482537819668\tValidation Loss: 0.00030828808843480195\n",
      "Epoch 675  \tTraining Loss: 0.0003064105664277259\tValidation Loss: 0.00030793699421995715\n",
      "Epoch 676  \tTraining Loss: 0.0003060738037789762\tValidation Loss: 0.00030758706056130863\n",
      "Epoch 677  \tTraining Loss: 0.00030573796705565885\tValidation Loss: 0.0003072379039147264\n",
      "Epoch 678  \tTraining Loss: 0.0003054030393054624\tValidation Loss: 0.0003068898675902828\n",
      "Epoch 679  \tTraining Loss: 0.00030506904030278687\tValidation Loss: 0.0003065426322831601\n",
      "Epoch 680  \tTraining Loss: 0.0003047360015859802\tValidation Loss: 0.0003061964969110512\n",
      "Epoch 681  \tTraining Loss: 0.0003044038712179716\tValidation Loss: 0.00030585145752032013\n",
      "Epoch 682  \tTraining Loss: 0.0003040726927518164\tValidation Loss: 0.00030550720123728286\n",
      "Epoch 683  \tTraining Loss: 0.00030374244319828864\tValidation Loss: 0.00030516416002375165\n",
      "Epoch 684  \tTraining Loss: 0.0003034131427782335\tValidation Loss: 0.0003048221083050471\n",
      "Epoch 685  \tTraining Loss: 0.0003030847804275099\tValidation Loss: 0.00030448086535634554\n",
      "Epoch 686  \tTraining Loss: 0.0003027573693296036\tValidation Loss: 0.00030414082832672367\n",
      "Epoch 687  \tTraining Loss: 0.00030243091532576214\tValidation Loss: 0.00030380187510428973\n",
      "Epoch 688  \tTraining Loss: 0.00030210539844764824\tValidation Loss: 0.00030346368555137236\n",
      "Epoch 689  \tTraining Loss: 0.00030178080255963255\tValidation Loss: 0.00030312665169648245\n",
      "Epoch 690  \tTraining Loss: 0.00030145713392162627\tValidation Loss: 0.00030279059570477755\n",
      "Epoch 691  \tTraining Loss: 0.00030113440807907953\tValidation Loss: 0.00030245535508305744\n",
      "Epoch 692  \tTraining Loss: 0.00030081263221187526\tValidation Loss: 0.0003021213766886291\n",
      "Epoch 693  \tTraining Loss: 0.00030049180589269476\tValidation Loss: 0.00030178845054399793\n",
      "Epoch 694  \tTraining Loss: 0.0003001719269734016\tValidation Loss: 0.0003014563574797697\n",
      "Epoch 695  \tTraining Loss: 0.0002998530128158329\tValidation Loss: 0.0003011257272155959\n",
      "Epoch 696  \tTraining Loss: 0.000299535069737428\tValidation Loss: 0.0003007959133337346\n",
      "Epoch 697  \tTraining Loss: 0.0002992180685787035\tValidation Loss: 0.0003004670639758561\n",
      "Epoch 698  \tTraining Loss: 0.00029890201937680537\tValidation Loss: 0.00030013941018387297\n",
      "Epoch 699  \tTraining Loss: 0.00029858693177411434\tValidation Loss: 0.00029981303949891874\n",
      "Epoch 700  \tTraining Loss: 0.00029827274994223183\tValidation Loss: 0.0002994871366156417\n",
      "Epoch 701  \tTraining Loss: 0.0002979595304928124\tValidation Loss: 0.0002991627034707975\n",
      "Epoch 702  \tTraining Loss: 0.0002976472834907193\tValidation Loss: 0.0002988388958660341\n",
      "Epoch 703  \tTraining Loss: 0.0002973359588363182\tValidation Loss: 0.00029851622992863275\n",
      "Epoch 704  \tTraining Loss: 0.0002970255809057903\tValidation Loss: 0.00029819455785801504\n",
      "Epoch 705  \tTraining Loss: 0.00029671616431421185\tValidation Loss: 0.00029787357951786375\n",
      "Epoch 706  \tTraining Loss: 0.00029640771682599943\tValidation Loss: 0.00029755410699621305\n",
      "Epoch 707  \tTraining Loss: 0.00029610024366259454\tValidation Loss: 0.0002972357190795566\n",
      "Epoch 708  \tTraining Loss: 0.00029579381925191016\tValidation Loss: 0.0002969176378572485\n",
      "Epoch 709  \tTraining Loss: 0.0002954883493738563\tValidation Loss: 0.0002966012831101297\n",
      "Epoch 710  \tTraining Loss: 0.00029518383946746827\tValidation Loss: 0.00029628582976849266\n",
      "Epoch 711  \tTraining Loss: 0.0002948803019441892\tValidation Loss: 0.0002959710046987568\n",
      "Epoch 712  \tTraining Loss: 0.0002945777280116617\tValidation Loss: 0.0002956577237439966\n",
      "Epoch 713  \tTraining Loss: 0.00029427612099892735\tValidation Loss: 0.00029534496391951773\n",
      "Epoch 714  \tTraining Loss: 0.0002939754534222864\tValidation Loss: 0.00029503369749493246\n",
      "Epoch 715  \tTraining Loss: 0.00029367574302811324\tValidation Loss: 0.0002947229499611434\n",
      "Epoch 716  \tTraining Loss: 0.0002933770208913617\tValidation Loss: 0.0002944137457008494\n",
      "Epoch 717  \tTraining Loss: 0.000293079246976229\tValidation Loss: 0.00029410508029984134\n",
      "Epoch 718  \tTraining Loss: 0.00029278242494717375\tValidation Loss: 0.00029379793575822844\n",
      "Epoch 719  \tTraining Loss: 0.0002924865596790551\tValidation Loss: 0.0002934913248131088\n",
      "Epoch 720  \tTraining Loss: 0.0002921916464444766\tValidation Loss: 0.0002931864311782505\n",
      "Epoch 721  \tTraining Loss: 0.00029189767924800404\tValidation Loss: 0.00029288202481945975\n",
      "Epoch 722  \tTraining Loss: 0.0002916046598093321\tValidation Loss: 0.0002925789253067614\n",
      "Epoch 723  \tTraining Loss: 0.0002913125836096454\tValidation Loss: 0.0002922766065057527\n",
      "Epoch 724  \tTraining Loss: 0.0002910214487224091\tValidation Loss: 0.0002919752388009147\n",
      "Epoch 725  \tTraining Loss: 0.00029073125207025385\tValidation Loss: 0.0002916755259955133\n",
      "Epoch 726  \tTraining Loss: 0.0002904420052375782\tValidation Loss: 0.0002913760782398192\n",
      "Epoch 727  \tTraining Loss: 0.00029015367639532254\tValidation Loss: 0.00029107787710502857\n",
      "Epoch 728  \tTraining Loss: 0.0002898662930571911\tValidation Loss: 0.00029078123730140085\n",
      "Epoch 729  \tTraining Loss: 0.0002895798442570671\tValidation Loss: 0.0002904848130408072\n",
      "Epoch 730  \tTraining Loss: 0.000289294327170648\tValidation Loss: 0.0002901895754776493\n",
      "Epoch 731  \tTraining Loss: 0.00028900973802973087\tValidation Loss: 0.0002898955798099873\n",
      "Epoch 732  \tTraining Loss: 0.0002887260766846747\tValidation Loss: 0.00028960271141689937\n",
      "Epoch 733  \tTraining Loss: 0.0002884433431508567\tValidation Loss: 0.00028931036121462265\n",
      "Epoch 734  \tTraining Loss: 0.0002881615382564064\tValidation Loss: 0.0002890193186669291\n",
      "Epoch 735  \tTraining Loss: 0.00028788066838579074\tValidation Loss: 0.00028872911763034247\n",
      "Epoch 736  \tTraining Loss: 0.0002876007235013063\tValidation Loss: 0.0002884398502499745\n",
      "Epoch 737  \tTraining Loss: 0.00028732168516642994\tValidation Loss: 0.0002881518872638447\n",
      "Epoch 738  \tTraining Loss: 0.00028704357381884\tValidation Loss: 0.00028786462625107187\n",
      "Epoch 739  \tTraining Loss: 0.00028676636167813057\tValidation Loss: 0.00028757870066839595\n",
      "Epoch 740  \tTraining Loss: 0.00028649005019072937\tValidation Loss: 0.00028729323134746624\n",
      "Epoch 741  \tTraining Loss: 0.0002862146533902033\tValidation Loss: 0.00028700903244535236\n",
      "Epoch 742  \tTraining Loss: 0.00028594016967705793\tValidation Loss: 0.0002867256302489045\n",
      "Epoch 743  \tTraining Loss: 0.00028566659070747546\tValidation Loss: 0.00028644325350371286\n",
      "Epoch 744  \tTraining Loss: 0.0002853939119726355\tValidation Loss: 0.0002861622605417899\n",
      "Epoch 745  \tTraining Loss: 0.000285122129081251\tValidation Loss: 0.00028588184746410555\n",
      "Epoch 746  \tTraining Loss: 0.00028485129568070304\tValidation Loss: 0.0002856025354933528\n",
      "Epoch 747  \tTraining Loss: 0.00028458136892436594\tValidation Loss: 0.00028532412739120023\n",
      "Epoch 748  \tTraining Loss: 0.00028431236767001433\tValidation Loss: 0.000285046873960459\n",
      "Epoch 749  \tTraining Loss: 0.00028404427138608863\tValidation Loss: 0.0002847704079350942\n",
      "Epoch 750  \tTraining Loss: 0.0002837771023727771\tValidation Loss: 0.0002844949776653763\n",
      "Epoch 751  \tTraining Loss: 0.0002835108345021178\tValidation Loss: 0.0002842205292786908\n",
      "Epoch 752  \tTraining Loss: 0.00028324545992963404\tValidation Loss: 0.00028394717574828783\n",
      "Epoch 753  \tTraining Loss: 0.00028298097424120025\tValidation Loss: 0.0002836745598670962\n",
      "Epoch 754  \tTraining Loss: 0.00028271738702076384\tValidation Loss: 0.000283403049860087\n",
      "Epoch 755  \tTraining Loss: 0.00028245469596413215\tValidation Loss: 0.0002831324860921567\n",
      "Epoch 756  \tTraining Loss: 0.0002821928742314632\tValidation Loss: 0.0002828628384657026\n",
      "Epoch 757  \tTraining Loss: 0.0002819319262084857\tValidation Loss: 0.000282594115398185\n",
      "Epoch 758  \tTraining Loss: 0.00028167184770744037\tValidation Loss: 0.0002823263274002018\n",
      "Epoch 759  \tTraining Loss: 0.00028141263968797277\tValidation Loss: 0.0002820594870248321\n",
      "Epoch 760  \tTraining Loss: 0.00028115430901971995\tValidation Loss: 0.0002817935490491615\n",
      "Epoch 761  \tTraining Loss: 0.00028089682859131556\tValidation Loss: 0.0002815285251041554\n",
      "Epoch 762  \tTraining Loss: 0.0002806402223902454\tValidation Loss: 0.000281264433871248\n",
      "Epoch 763  \tTraining Loss: 0.0002803845133417649\tValidation Loss: 0.0002810012882514379\n",
      "Epoch 764  \tTraining Loss: 0.00028012967139564833\tValidation Loss: 0.0002807390939942619\n",
      "Epoch 765  \tTraining Loss: 0.00027987568302294085\tValidation Loss: 0.0002804777973833614\n",
      "Epoch 766  \tTraining Loss: 0.00027962254405570927\tValidation Loss: 0.000280217428299686\n",
      "Epoch 767  \tTraining Loss: 0.0002793702592774726\tValidation Loss: 0.0002799579590310703\n",
      "Epoch 768  \tTraining Loss: 0.00027911881210871347\tValidation Loss: 0.00027969939311452027\n",
      "Epoch 769  \tTraining Loss: 0.00027886820669636417\tValidation Loss: 0.0002794417282568009\n",
      "Epoch 770  \tTraining Loss: 0.00027861844164717376\tValidation Loss: 0.000279184963133292\n",
      "Epoch 771  \tTraining Loss: 0.00027836951506908205\tValidation Loss: 0.0002789291620786174\n",
      "Epoch 772  \tTraining Loss: 0.00027812144880956887\tValidation Loss: 0.000278674185750208\n",
      "Epoch 773  \tTraining Loss: 0.0002778741994747673\tValidation Loss: 0.00027842012961398267\n",
      "Epoch 774  \tTraining Loss: 0.00027762778076751674\tValidation Loss: 0.0002781669238284757\n",
      "Epoch 775  \tTraining Loss: 0.0002773821986279194\tValidation Loss: 0.00027791458704178767\n",
      "Epoch 776  \tTraining Loss: 0.00027713743718572854\tValidation Loss: 0.0002776631417241099\n",
      "Epoch 777  \tTraining Loss: 0.0002768934982384301\tValidation Loss: 0.00027741256955239667\n",
      "Epoch 778  \tTraining Loss: 0.0002766503959130161\tValidation Loss: 0.00027716284824537147\n",
      "Epoch 779  \tTraining Loss: 0.00027640810326836355\tValidation Loss: 0.0002769139925943327\n",
      "Epoch 780  \tTraining Loss: 0.00027616661889453523\tValidation Loss: 0.000276666009467295\n",
      "Epoch 781  \tTraining Loss: 0.0002759259415738289\tValidation Loss: 0.0002764188920880433\n",
      "Epoch 782  \tTraining Loss: 0.00027568607131299726\tValidation Loss: 0.0002761726432066487\n",
      "Epoch 783  \tTraining Loss: 0.0002754470024793604\tValidation Loss: 0.00027592759935533205\n",
      "Epoch 784  \tTraining Loss: 0.0002752087613002577\tValidation Loss: 0.0002756831612130873\n",
      "Epoch 785  \tTraining Loss: 0.00027497132265377435\tValidation Loss: 0.0002754396073707636\n",
      "Epoch 786  \tTraining Loss: 0.0002747347569198116\tValidation Loss: 0.0002751970069004761\n",
      "Epoch 787  \tTraining Loss: 0.0002744990900467912\tValidation Loss: 0.000274955142273182\n",
      "Epoch 788  \tTraining Loss: 0.00027426420210298965\tValidation Loss: 0.00027471408678140824\n",
      "Epoch 789  \tTraining Loss: 0.0002740300918976056\tValidation Loss: 0.0002744738591563955\n",
      "Epoch 790  \tTraining Loss: 0.00027379675210273727\tValidation Loss: 0.0002742344471535227\n",
      "Epoch 791  \tTraining Loss: 0.000273564179752692\tValidation Loss: 0.0002739958675922548\n",
      "Epoch 792  \tTraining Loss: 0.00027333237816964175\tValidation Loss: 0.00027375810847640194\n",
      "Epoch 793  \tTraining Loss: 0.00027310136465176434\tValidation Loss: 0.00027352114492266603\n",
      "Epoch 794  \tTraining Loss: 0.0002728711576213762\tValidation Loss: 0.00027328504846401\n",
      "Epoch 795  \tTraining Loss: 0.0002726417046894965\tValidation Loss: 0.0002730497753234953\n",
      "Epoch 796  \tTraining Loss: 0.00027241301338902524\tValidation Loss: 0.0002728153336408274\n",
      "Epoch 797  \tTraining Loss: 0.0002721851168567515\tValidation Loss: 0.0002725816219212345\n",
      "Epoch 798  \tTraining Loss: 0.0002719579752143165\tValidation Loss: 0.00027234876950244777\n",
      "Epoch 799  \tTraining Loss: 0.0002717315806168892\tValidation Loss: 0.00027211673064471225\n",
      "Epoch 800  \tTraining Loss: 0.00027150592289934105\tValidation Loss: 0.0002718854948192947\n",
      "Epoch 801  \tTraining Loss: 0.0002712809985533177\tValidation Loss: 0.00027165505254152705\n",
      "Epoch 802  \tTraining Loss: 0.00027105679671562326\tValidation Loss: 0.00027142537441091763\n",
      "Epoch 803  \tTraining Loss: 0.0002708333057043199\tValidation Loss: 0.00027119649971715245\n",
      "Epoch 804  \tTraining Loss: 0.0002706105505910129\tValidation Loss: 0.0002709684018917447\n",
      "Epoch 805  \tTraining Loss: 0.00027038852151090037\tValidation Loss: 0.00027074109432143723\n",
      "Epoch 806  \tTraining Loss: 0.0002701672175043928\tValidation Loss: 0.00027051455334722695\n",
      "Epoch 807  \tTraining Loss: 0.0002699466375907485\tValidation Loss: 0.0002702887972925016\n",
      "Epoch 808  \tTraining Loss: 0.0002697267903578501\tValidation Loss: 0.000270063788996684\n",
      "Epoch 809  \tTraining Loss: 0.00026950764974145947\tValidation Loss: 0.0002698395457836671\n",
      "Epoch 810  \tTraining Loss: 0.00026928927341082846\tValidation Loss: 0.00026961623618513517\n",
      "Epoch 811  \tTraining Loss: 0.0002690716715202357\tValidation Loss: 0.00026939359724902274\n",
      "Epoch 812  \tTraining Loss: 0.0002688547648112649\tValidation Loss: 0.00026917168757940185\n",
      "Epoch 813  \tTraining Loss: 0.0002686385471797306\tValidation Loss: 0.00026895051071104643\n",
      "Epoch 814  \tTraining Loss: 0.00026842301093194106\tValidation Loss: 0.0002687300709484781\n",
      "Epoch 815  \tTraining Loss: 0.0002682081583037832\tValidation Loss: 0.0002685103652964123\n",
      "Epoch 816  \tTraining Loss: 0.0002679939858548602\tValidation Loss: 0.0002682913898840568\n",
      "Epoch 817  \tTraining Loss: 0.0002677804895998136\tValidation Loss: 0.0002680731384558064\n",
      "Epoch 818  \tTraining Loss: 0.00026756768443320436\tValidation Loss: 0.00026785553938894914\n",
      "Epoch 819  \tTraining Loss: 0.00026735556717566087\tValidation Loss: 0.00026763872758171374\n",
      "Epoch 820  \tTraining Loss: 0.00026714411828439267\tValidation Loss: 0.00026742270458312846\n",
      "Epoch 821  \tTraining Loss: 0.00026693333683823424\tValidation Loss: 0.0002672073708678134\n",
      "Epoch 822  \tTraining Loss: 0.00026672321460272016\tValidation Loss: 0.00026699273651546685\n",
      "Epoch 823  \tTraining Loss: 0.00026651374743811115\tValidation Loss: 0.00026677879437661403\n",
      "Epoch 824  \tTraining Loss: 0.00026630492756306865\tValidation Loss: 0.00026656554279963664\n",
      "Epoch 825  \tTraining Loss: 0.0002660967595543435\tValidation Loss: 0.0002663529797016833\n",
      "Epoch 826  \tTraining Loss: 0.0002658892377326389\tValidation Loss: 0.0002661411093012176\n",
      "Epoch 827  \tTraining Loss: 0.0002656823587581986\tValidation Loss: 0.00026592992847183363\n",
      "Epoch 828  \tTraining Loss: 0.0002654761212195696\tValidation Loss: 0.0002657192597829751\n",
      "Epoch 829  \tTraining Loss: 0.0002652705282435033\tValidation Loss: 0.00026550941526827336\n",
      "Epoch 830  \tTraining Loss: 0.00026506556831683883\tValidation Loss: 0.0002653002730467024\n",
      "Epoch 831  \tTraining Loss: 0.00026486124500390445\tValidation Loss: 0.00026509203516742824\n",
      "Epoch 832  \tTraining Loss: 0.0002646576018081951\tValidation Loss: 0.0002648843493850164\n",
      "Epoch 833  \tTraining Loss: 0.00026445458328772694\tValidation Loss: 0.00026467733268231686\n",
      "Epoch 834  \tTraining Loss: 0.0002642521914051378\tValidation Loss: 0.00026447095798972503\n",
      "Epoch 835  \tTraining Loss: 0.0002640504160023645\tValidation Loss: 0.0002642652873602972\n",
      "Epoch 836  \tTraining Loss: 0.00026384925679051916\tValidation Loss: 0.0002640602815900796\n",
      "Epoch 837  \tTraining Loss: 0.00026364870859827756\tValidation Loss: 0.0002638559067864156\n",
      "Epoch 838  \tTraining Loss: 0.00026344874079500737\tValidation Loss: 0.00026365217108253563\n",
      "Epoch 839  \tTraining Loss: 0.0002632493790523158\tValidation Loss: 0.0002634490778757744\n",
      "Epoch 840  \tTraining Loss: 0.00026305063852381056\tValidation Loss: 0.0002632466442886485\n",
      "Epoch 841  \tTraining Loss: 0.0002628524915857035\tValidation Loss: 0.0002630448449904495\n",
      "Epoch 842  \tTraining Loss: 0.00026265493341288943\tValidation Loss: 0.0002628436996980545\n",
      "Epoch 843  \tTraining Loss: 0.0002624579679703865\tValidation Loss: 0.0002626431741089286\n",
      "Epoch 844  \tTraining Loss: 0.0002622615822814203\tValidation Loss: 0.00026244327690049354\n",
      "Epoch 845  \tTraining Loss: 0.0002620657810493034\tValidation Loss: 0.0002622440053024381\n",
      "Epoch 846  \tTraining Loss: 0.00026187056118026804\tValidation Loss: 0.00026204535609877235\n",
      "Epoch 847  \tTraining Loss: 0.0002616759141483199\tValidation Loss: 0.00026184729667519006\n",
      "Epoch 848  \tTraining Loss: 0.00026148181627612215\tValidation Loss: 0.0002616498483382569\n",
      "Epoch 849  \tTraining Loss: 0.00026128829858603695\tValidation Loss: 0.00026145301356197384\n",
      "Epoch 850  \tTraining Loss: 0.00026109534597891634\tValidation Loss: 0.0002612567789511982\n",
      "Epoch 851  \tTraining Loss: 0.000260902953511052\tValidation Loss: 0.00026106115049213957\n",
      "Epoch 852  \tTraining Loss: 0.0002607111258356494\tValidation Loss: 0.00026086613521924455\n",
      "Epoch 853  \tTraining Loss: 0.0002605198690776105\tValidation Loss: 0.00026067174458315373\n",
      "Epoch 854  \tTraining Loss: 0.00026032920061285484\tValidation Loss: 0.00026047794318928547\n",
      "Epoch 855  \tTraining Loss: 0.0002601391070163522\tValidation Loss: 0.00026028471205293425\n",
      "Epoch 856  \tTraining Loss: 0.0002599495652051339\tValidation Loss: 0.00026009210137540586\n",
      "Epoch 857  \tTraining Loss: 0.00025976057160855676\tValidation Loss: 0.0002599001046153332\n",
      "Epoch 858  \tTraining Loss: 0.000259572131446015\tValidation Loss: 0.00025970847404963704\n",
      "Epoch 859  \tTraining Loss: 0.0002593842537906367\tValidation Loss: 0.00025951781883282166\n",
      "Epoch 860  \tTraining Loss: 0.0002591969238550421\tValidation Loss: 0.0002593273956823195\n",
      "Epoch 861  \tTraining Loss: 0.0002590101353949596\tValidation Loss: 0.0002591376868236583\n",
      "Epoch 862  \tTraining Loss: 0.0002588238890330683\tValidation Loss: 0.0002589485897323616\n",
      "Epoch 863  \tTraining Loss: 0.000258638192986757\tValidation Loss: 0.0002587602418175712\n",
      "Epoch 864  \tTraining Loss: 0.0002584530561988188\tValidation Loss: 0.0002585724087552013\n",
      "Epoch 865  \tTraining Loss: 0.0002582684513197456\tValidation Loss: 0.00025838513604262543\n",
      "Epoch 866  \tTraining Loss: 0.000258084389100529\tValidation Loss: 0.00025819845407093375\n",
      "Epoch 867  \tTraining Loss: 0.00025790085781682265\tValidation Loss: 0.0002580123108670716\n",
      "Epoch 868  \tTraining Loss: 0.0002577178473649945\tValidation Loss: 0.0002578267020633914\n",
      "Epoch 869  \tTraining Loss: 0.00025753535632886917\tValidation Loss: 0.00025764164774537884\n",
      "Epoch 870  \tTraining Loss: 0.00025735338483246916\tValidation Loss: 0.00025745714036493387\n",
      "Epoch 871  \tTraining Loss: 0.00025717192251930714\tValidation Loss: 0.00025727318412126964\n",
      "Epoch 872  \tTraining Loss: 0.000256990966711351\tValidation Loss: 0.0002570897667799219\n",
      "Epoch 873  \tTraining Loss: 0.00025681051570736034\tValidation Loss: 0.00025690690501308086\n",
      "Epoch 874  \tTraining Loss: 0.0002566305800330046\tValidation Loss: 0.00025672457776279887\n",
      "Epoch 875  \tTraining Loss: 0.00025645114294731744\tValidation Loss: 0.00025654278156862943\n",
      "Epoch 876  \tTraining Loss: 0.00025627220399584836\tValidation Loss: 0.0002563614803372067\n",
      "Epoch 877  \tTraining Loss: 0.0002560937711732531\tValidation Loss: 0.00025618076404935493\n",
      "Epoch 878  \tTraining Loss: 0.000255915836091625\tValidation Loss: 0.0002560005828094078\n",
      "Epoch 879  \tTraining Loss: 0.00025573839028732783\tValidation Loss: 0.00025582093985212096\n",
      "Epoch 880  \tTraining Loss: 0.00025556144012999594\tValidation Loss: 0.000255641815541678\n",
      "Epoch 881  \tTraining Loss: 0.0002553849733203601\tValidation Loss: 0.00025546323267286203\n",
      "Epoch 882  \tTraining Loss: 0.0002552089903067284\tValidation Loss: 0.000255285156381278\n",
      "Epoch 883  \tTraining Loss: 0.0002550334796571023\tValidation Loss: 0.0002551075803585818\n",
      "Epoch 884  \tTraining Loss: 0.0002548584360711773\tValidation Loss: 0.0002549305131559022\n",
      "Epoch 885  \tTraining Loss: 0.00025468386083530713\tValidation Loss: 0.00025475394403104515\n",
      "Epoch 886  \tTraining Loss: 0.00025450975130923383\tValidation Loss: 0.00025457787830205654\n",
      "Epoch 887  \tTraining Loss: 0.00025433611011549894\tValidation Loss: 0.0002544023132068174\n",
      "Epoch 888  \tTraining Loss: 0.00025416293485094454\tValidation Loss: 0.00025422724582148794\n",
      "Epoch 889  \tTraining Loss: 0.0002539902241368101\tValidation Loss: 0.0002540526867412326\n",
      "Epoch 890  \tTraining Loss: 0.0002538179782926603\tValidation Loss: 0.000253878617830066\n",
      "Epoch 891  \tTraining Loss: 0.0002536461826422912\tValidation Loss: 0.00025370503773233794\n",
      "Epoch 892  \tTraining Loss: 0.00025347484679074316\tValidation Loss: 0.00025353206437751574\n",
      "Epoch 893  \tTraining Loss: 0.00025330397188052015\tValidation Loss: 0.0002533595242684268\n",
      "Epoch 894  \tTraining Loss: 0.0002531335446586621\tValidation Loss: 0.00025318753393570537\n",
      "Epoch 895  \tTraining Loss: 0.00025296356885591564\tValidation Loss: 0.0002530159625407877\n",
      "Epoch 896  \tTraining Loss: 0.00025279404126576276\tValidation Loss: 0.0002528448487248659\n",
      "Epoch 897  \tTraining Loss: 0.0002526249596741659\tValidation Loss: 0.00025267420126464946\n",
      "Epoch 898  \tTraining Loss: 0.0002524563207784627\tValidation Loss: 0.0002525040303373463\n",
      "Epoch 899  \tTraining Loss: 0.0002522881203071454\tValidation Loss: 0.0002523343257377245\n",
      "Epoch 900  \tTraining Loss: 0.00025212036521768925\tValidation Loss: 0.00025216506210803356\n",
      "Epoch 901  \tTraining Loss: 0.0002519530603991404\tValidation Loss: 0.00025199627679172197\n",
      "Epoch 902  \tTraining Loss: 0.000251786223796809\tValidation Loss: 0.000251827946727664\n",
      "Epoch 903  \tTraining Loss: 0.00025161982214838287\tValidation Loss: 0.00025166014163540063\n",
      "Epoch 904  \tTraining Loss: 0.00025145385600745557\tValidation Loss: 0.0002514927593540086\n",
      "Epoch 905  \tTraining Loss: 0.00025128831690362835\tValidation Loss: 0.0002513258784908783\n",
      "Epoch 906  \tTraining Loss: 0.00025112320358976697\tValidation Loss: 0.00025115948660689517\n",
      "Epoch 907  \tTraining Loss: 0.00025095851383236914\tValidation Loss: 0.00025099355756183327\n",
      "Epoch 908  \tTraining Loss: 0.00025079424586670274\tValidation Loss: 0.00025082809567536523\n",
      "Epoch 909  \tTraining Loss: 0.00025063039918585945\tValidation Loss: 0.0002506631043675357\n",
      "Epoch 910  \tTraining Loss: 0.0002504669698960841\tValidation Loss: 0.0002504985587969302\n",
      "Epoch 911  \tTraining Loss: 0.00025030395597129074\tValidation Loss: 0.0002503344562127697\n",
      "Epoch 912  \tTraining Loss: 0.00025014135833735344\tValidation Loss: 0.0002501707782272029\n",
      "Epoch 913  \tTraining Loss: 0.0002499791828186407\tValidation Loss: 0.00025000753446619613\n",
      "Epoch 914  \tTraining Loss: 0.0002498174263631561\tValidation Loss: 0.00024984472733748407\n",
      "Epoch 915  \tTraining Loss: 0.0002496560783728078\tValidation Loss: 0.0002496823636207461\n",
      "Epoch 916  \tTraining Loss: 0.000249495136347299\tValidation Loss: 0.00024952044897415374\n",
      "Epoch 917  \tTraining Loss: 0.00024933460966940766\tValidation Loss: 0.0002493589285095696\n",
      "Epoch 918  \tTraining Loss: 0.00024917450521545173\tValidation Loss: 0.0002491978937962284\n",
      "Epoch 919  \tTraining Loss: 0.0002490148042061236\tValidation Loss: 0.00024903731161800903\n",
      "Epoch 920  \tTraining Loss: 0.00024885550482441705\tValidation Loss: 0.0002488772007227524\n",
      "Epoch 921  \tTraining Loss: 0.0002486966009216016\tValidation Loss: 0.00024871751444607176\n",
      "Epoch 922  \tTraining Loss: 0.0002485380851240641\tValidation Loss: 0.00024855825871517175\n",
      "Epoch 923  \tTraining Loss: 0.00024837996935045837\tValidation Loss: 0.00024839942957667193\n",
      "Epoch 924  \tTraining Loss: 0.00024822224934892604\tValidation Loss: 0.00024824101691086204\n",
      "Epoch 925  \tTraining Loss: 0.0002480649105423855\tValidation Loss: 0.000248083019817086\n",
      "Epoch 926  \tTraining Loss: 0.00024790796409046356\tValidation Loss: 0.0002479254399555729\n",
      "Epoch 927  \tTraining Loss: 0.00024775141138712943\tValidation Loss: 0.00024776826908528735\n",
      "Epoch 928  \tTraining Loss: 0.00024759524361922264\tValidation Loss: 0.00024761150767458343\n",
      "Epoch 929  \tTraining Loss: 0.00024743946002366804\tValidation Loss: 0.0002474551510584861\n",
      "Epoch 930  \tTraining Loss: 0.00024728405645204195\tValidation Loss: 0.0002472992223728858\n",
      "Epoch 931  \tTraining Loss: 0.00024712903901263835\tValidation Loss: 0.0002471436724445255\n",
      "Epoch 932  \tTraining Loss: 0.00024697440030246436\tValidation Loss: 0.0002469885266010897\n",
      "Epoch 933  \tTraining Loss: 0.0002468201434475265\tValidation Loss: 0.00024683376239386024\n",
      "Epoch 934  \tTraining Loss: 0.0002466662624166626\tValidation Loss: 0.0002466793857613412\n",
      "Epoch 935  \tTraining Loss: 0.0002465127605817567\tValidation Loss: 0.0002465253789355942\n",
      "Epoch 936  \tTraining Loss: 0.0002463596272559304\tValidation Loss: 0.00024637177973156003\n",
      "Epoch 937  \tTraining Loss: 0.00024620686659616376\tValidation Loss: 0.0002462185085137553\n",
      "Epoch 938  \tTraining Loss: 0.0002460544855657816\tValidation Loss: 0.0002460656673573234\n",
      "Epoch 939  \tTraining Loss: 0.0002459024885320418\tValidation Loss: 0.00024591319467498463\n",
      "Epoch 940  \tTraining Loss: 0.00024575085548310166\tValidation Loss: 0.0002457611435430614\n",
      "Epoch 941  \tTraining Loss: 0.000245599587235022\tValidation Loss: 0.0002456094473209102\n",
      "Epoch 942  \tTraining Loss: 0.00024544867767611493\tValidation Loss: 0.00024545831804631167\n",
      "Epoch 943  \tTraining Loss: 0.000245298139639203\tValidation Loss: 0.0002453074123245664\n",
      "Epoch 944  \tTraining Loss: 0.0002451479536430495\tValidation Loss: 0.00024515688942800283\n",
      "Epoch 945  \tTraining Loss: 0.00024499812056796936\tValidation Loss: 0.0002450066726677661\n",
      "Epoch 946  \tTraining Loss: 0.0002448486418112609\tValidation Loss: 0.00024485686421005427\n",
      "Epoch 947  \tTraining Loss: 0.00024469951212279477\tValidation Loss: 0.00024470740918574777\n",
      "Epoch 948  \tTraining Loss: 0.00024455073486835536\tValidation Loss: 0.0002445583429578606\n",
      "Epoch 949  \tTraining Loss: 0.00024440231410240853\tValidation Loss: 0.0002444096675481005\n",
      "Epoch 950  \tTraining Loss: 0.000244254240356489\tValidation Loss: 0.0002442613114129534\n",
      "Epoch 951  \tTraining Loss: 0.00024410651281507104\tValidation Loss: 0.00024411335400210908\n",
      "Epoch 952  \tTraining Loss: 0.0002439591303352513\tValidation Loss: 0.00024396574326236546\n",
      "Epoch 953  \tTraining Loss: 0.00024381209672164335\tValidation Loss: 0.0002438184744816966\n",
      "Epoch 954  \tTraining Loss: 0.00024366540242903544\tValidation Loss: 0.0002436716064865184\n",
      "Epoch 955  \tTraining Loss: 0.00024351905044654436\tValidation Loss: 0.0002435250512443758\n",
      "Epoch 956  \tTraining Loss: 0.00024337303699688834\tValidation Loss: 0.00024337889482957536\n",
      "Epoch 957  \tTraining Loss: 0.00024322735741786418\tValidation Loss: 0.00024323288332884328\n",
      "Epoch 958  \tTraining Loss: 0.00024308201447951545\tValidation Loss: 0.00024308739114994648\n",
      "Epoch 959  \tTraining Loss: 0.0002429370112981647\tValidation Loss: 0.00024294224717469584\n",
      "Epoch 960  \tTraining Loss: 0.00024279234187303954\tValidation Loss: 0.00024279743172032244\n",
      "Epoch 961  \tTraining Loss: 0.00024264800371941365\tValidation Loss: 0.0002426530129042746\n",
      "Epoch 962  \tTraining Loss: 0.00024250399569779692\tValidation Loss: 0.00024250892516269144\n",
      "Epoch 963  \tTraining Loss: 0.00024236032303857682\tValidation Loss: 0.00024236516682685967\n",
      "Epoch 964  \tTraining Loss: 0.00024221698494697642\tValidation Loss: 0.00024222180243688526\n",
      "Epoch 965  \tTraining Loss: 0.00024207397250747194\tValidation Loss: 0.00024207872679033772\n",
      "Epoch 966  \tTraining Loss: 0.00024193128641266366\tValidation Loss: 0.00024193600474808205\n",
      "Epoch 967  \tTraining Loss: 0.0002417889225904263\tValidation Loss: 0.00024179364015022332\n",
      "Epoch 968  \tTraining Loss: 0.00024164688208900848\tValidation Loss: 0.0002416515979277892\n",
      "Epoch 969  \tTraining Loss: 0.00024150516594827496\tValidation Loss: 0.00024150996427435898\n",
      "Epoch 970  \tTraining Loss: 0.0002413637755408917\tValidation Loss: 0.00024136860293995659\n",
      "Epoch 971  \tTraining Loss: 0.00024122270546855302\tValidation Loss: 0.00024122763322659865\n",
      "Epoch 972  \tTraining Loss: 0.0002410819491533697\tValidation Loss: 0.0002410869382931034\n",
      "Epoch 973  \tTraining Loss: 0.00024094151058962136\tValidation Loss: 0.00024094663354575673\n",
      "Epoch 974  \tTraining Loss: 0.00024080137728915553\tValidation Loss: 0.00024080661861044915\n",
      "Epoch 975  \tTraining Loss: 0.00024066154722132213\tValidation Loss: 0.00024066689797769733\n",
      "Epoch 976  \tTraining Loss: 0.00024052202628885747\tValidation Loss: 0.00024052756128825989\n",
      "Epoch 977  \tTraining Loss: 0.00024038280984457793\tValidation Loss: 0.0002403884864525597\n",
      "Epoch 978  \tTraining Loss: 0.00024024390316603925\tValidation Loss: 0.00024024979997997712\n",
      "Epoch 979  \tTraining Loss: 0.00024010530351007821\tValidation Loss: 0.00024011137211335382\n",
      "Epoch 980  \tTraining Loss: 0.0002399670164479924\tValidation Loss: 0.00023997314004535253\n",
      "Epoch 981  \tTraining Loss: 0.00023982906224894612\tValidation Loss: 0.00023983532923733212\n",
      "Epoch 982  \tTraining Loss: 0.00023969141459494105\tValidation Loss: 0.00023969784341668846\n",
      "Epoch 983  \tTraining Loss: 0.00023955406935442455\tValidation Loss: 0.00023956076013327524\n",
      "Epoch 984  \tTraining Loss: 0.00023941702608161304\tValidation Loss: 0.00023942393059781796\n",
      "Epoch 985  \tTraining Loss: 0.00023928028517055628\tValidation Loss: 0.0002392874894040796\n",
      "Epoch 986  \tTraining Loss: 0.00023914384240722274\tValidation Loss: 0.00023915133530198328\n",
      "Epoch 987  \tTraining Loss: 0.0002390077003132586\tValidation Loss: 0.00023901545416713958\n",
      "Epoch 988  \tTraining Loss: 0.00023887185499695096\tValidation Loss: 0.0002388799584241126\n",
      "Epoch 989  \tTraining Loss: 0.0002387363059423001\tValidation Loss: 0.00023874469959888183\n",
      "Epoch 990  \tTraining Loss: 0.00023860105412604155\tValidation Loss: 0.0002386098243329934\n",
      "Epoch 991  \tTraining Loss: 0.00023846609487716068\tValidation Loss: 0.0002384752271615958\n",
      "Epoch 992  \tTraining Loss: 0.00023833143032672764\tValidation Loss: 0.00023834089296843007\n",
      "Epoch 993  \tTraining Loss: 0.00023819705819295647\tValidation Loss: 0.00023820694233141467\n",
      "Epoch 994  \tTraining Loss: 0.00023806297623607406\tValidation Loss: 0.00023807326480573945\n",
      "Epoch 995  \tTraining Loss: 0.00023792918728828027\tValidation Loss: 0.00023793984703505183\n",
      "Epoch 996  \tTraining Loss: 0.0002377956885386457\tValidation Loss: 0.00023780681203434853\n",
      "Epoch 997  \tTraining Loss: 0.00023766247540942271\tValidation Loss: 0.00023767403593125542\n",
      "Epoch 998  \tTraining Loss: 0.00023752954126946463\tValidation Loss: 0.00023754151073419012\n",
      "Epoch 999  \tTraining Loss: 0.00023739689347598462\tValidation Loss: 0.0002374093795373124\n",
      "Epoch 1000  \tTraining Loss: 0.00023726453739295164\tValidation Loss: 0.0002372775139297771\n",
      "Epoch 1001  \tTraining Loss: 0.00023713246715876578\tValidation Loss: 0.00023714589433906233\n",
      "Epoch 1002  \tTraining Loss: 0.00023700067627073468\tValidation Loss: 0.00023701463419805948\n",
      "Epoch 1003  \tTraining Loss: 0.00023686915720125845\tValidation Loss: 0.00023688364186027203\n",
      "Epoch 1004  \tTraining Loss: 0.00023673792094077594\tValidation Loss: 0.00023675289213366372\n",
      "Epoch 1005  \tTraining Loss: 0.00023660696418309726\tValidation Loss: 0.00023662252437183415\n",
      "Epoch 1006  \tTraining Loss: 0.00023647628610663605\tValidation Loss: 0.00023649241239526695\n",
      "Epoch 1007  \tTraining Loss: 0.00023634588714299259\tValidation Loss: 0.0002363625363733405\n",
      "Epoch 1008  \tTraining Loss: 0.00023621576600414487\tValidation Loss: 0.00023623303965333723\n",
      "Epoch 1009  \tTraining Loss: 0.00023608592170422786\tValidation Loss: 0.00023610379941359097\n",
      "Epoch 1010  \tTraining Loss: 0.00023595635370514926\tValidation Loss: 0.0002359748164407121\n",
      "Epoch 1011  \tTraining Loss: 0.00023582705925158577\tValidation Loss: 0.00023584609333644364\n",
      "Epoch 1012  \tTraining Loss: 0.00023569803853327093\tValidation Loss: 0.00023571775624262614\n",
      "Epoch 1013  \tTraining Loss: 0.00023556929058334957\tValidation Loss: 0.00023558967231445535\n",
      "Epoch 1014  \tTraining Loss: 0.00023544081465756527\tValidation Loss: 0.0002354618719411564\n",
      "Epoch 1015  \tTraining Loss: 0.00023531261080302632\tValidation Loss: 0.00023533429740181587\n",
      "Epoch 1016  \tTraining Loss: 0.00023518467612767782\tValidation Loss: 0.000235207100987613\n",
      "Epoch 1017  \tTraining Loss: 0.00023505701012388943\tValidation Loss: 0.0002350801462386805\n",
      "Epoch 1018  \tTraining Loss: 0.00023492961258835695\tValidation Loss: 0.0002349534936618123\n",
      "Epoch 1019  \tTraining Loss: 0.00023480248347726174\tValidation Loss: 0.00023482706957587035\n",
      "Epoch 1020  \tTraining Loss: 0.00023467562021250554\tValidation Loss: 0.00023470102300516873\n",
      "Epoch 1021  \tTraining Loss: 0.0002345490212453846\tValidation Loss: 0.00023457521091902896\n",
      "Epoch 1022  \tTraining Loss: 0.0002344226862121565\tValidation Loss: 0.0002344496715577351\n",
      "Epoch 1023  \tTraining Loss: 0.00023429661571100897\tValidation Loss: 0.00023432434522736655\n",
      "Epoch 1024  \tTraining Loss: 0.00023417080662165992\tValidation Loss: 0.00023419939300926376\n",
      "Epoch 1025  \tTraining Loss: 0.0002340452570653882\tValidation Loss: 0.0002340746608579619\n",
      "Epoch 1026  \tTraining Loss: 0.00023391996912496417\tValidation Loss: 0.0002339501977517526\n",
      "Epoch 1027  \tTraining Loss: 0.00023379494198791592\tValidation Loss: 0.00023382598855882278\n",
      "Epoch 1028  \tTraining Loss: 0.00023367019125101674\tValidation Loss: 0.00023370197261848\n",
      "Epoch 1029  \tTraining Loss: 0.0002335456992250993\tValidation Loss: 0.0002335783337996535\n",
      "Epoch 1030  \tTraining Loss: 0.0002334214648128068\tValidation Loss: 0.00023345492152443553\n",
      "Epoch 1031  \tTraining Loss: 0.00023329748800385427\tValidation Loss: 0.0002333317736283529\n",
      "Epoch 1032  \tTraining Loss: 0.00023317376823337946\tValidation Loss: 0.00023320889644653663\n",
      "Epoch 1033  \tTraining Loss: 0.00023305031214320275\tValidation Loss: 0.00023308621881327148\n",
      "Epoch 1034  \tTraining Loss: 0.00023292711114224297\tValidation Loss: 0.000232963916683839\n",
      "Epoch 1035  \tTraining Loss: 0.00023280416590840494\tValidation Loss: 0.00023284184356627438\n",
      "Epoch 1036  \tTraining Loss: 0.00023268148041767398\tValidation Loss: 0.000232720030782192\n",
      "Epoch 1037  \tTraining Loss: 0.00023255904757144328\tValidation Loss: 0.0002325984954629718\n",
      "Epoch 1038  \tTraining Loss: 0.0002324368665813481\tValidation Loss: 0.00023247722126913743\n",
      "Epoch 1039  \tTraining Loss: 0.00023231493975483502\tValidation Loss: 0.00023235612610331613\n",
      "Epoch 1040  \tTraining Loss: 0.00023219326170253202\tValidation Loss: 0.00023223539208478845\n",
      "Epoch 1041  \tTraining Loss: 0.00023207181309481325\tValidation Loss: 0.0002321148822775153\n",
      "Epoch 1042  \tTraining Loss: 0.00023195061632693526\tValidation Loss: 0.0002319946297154152\n",
      "Epoch 1043  \tTraining Loss: 0.0002318296678442143\tValidation Loss: 0.00023187463377403873\n",
      "Epoch 1044  \tTraining Loss: 0.000231708966892648\tValidation Loss: 0.00023175489365184255\n",
      "Epoch 1045  \tTraining Loss: 0.00023158851273348622\tValidation Loss: 0.00023163540838265796\n",
      "Epoch 1046  \tTraining Loss: 0.00023146830480366655\tValidation Loss: 0.00023151610517065604\n",
      "Epoch 1047  \tTraining Loss: 0.0002313483420900802\tValidation Loss: 0.00023139718301481673\n",
      "Epoch 1048  \tTraining Loss: 0.00023122861911104305\tValidation Loss: 0.0002312784710776288\n",
      "Epoch 1049  \tTraining Loss: 0.00023110915004651142\tValidation Loss: 0.0002311600114286911\n",
      "Epoch 1050  \tTraining Loss: 0.00023098992873521573\tValidation Loss: 0.00023104187849333756\n",
      "Epoch 1051  \tTraining Loss: 0.0002308709401155846\tValidation Loss: 0.00023092392497903365\n",
      "Epoch 1052  \tTraining Loss: 0.0002307521858335214\tValidation Loss: 0.00023080622086752497\n",
      "Epoch 1053  \tTraining Loss: 0.0002306336824766006\tValidation Loss: 0.00023068876634936813\n",
      "Epoch 1054  \tTraining Loss: 0.0002305154193185848\tValidation Loss: 0.00023057155915677522\n",
      "Epoch 1055  \tTraining Loss: 0.00023039739666908134\tValidation Loss: 0.00023045458792921627\n",
      "Epoch 1056  \tTraining Loss: 0.00023027961227326424\tValidation Loss: 0.00023033778312666904\n",
      "Epoch 1057  \tTraining Loss: 0.00023016206693893842\tValidation Loss: 0.00023022136512683284\n",
      "Epoch 1058  \tTraining Loss: 0.00023004475742431055\tValidation Loss: 0.000230105141019402\n",
      "Epoch 1059  \tTraining Loss: 0.00022992767143842658\tValidation Loss: 0.0002299891876045252\n",
      "Epoch 1060  \tTraining Loss: 0.00022981082434069667\tValidation Loss: 0.00022987346788368808\n",
      "Epoch 1061  \tTraining Loss: 0.00022969421341349067\tValidation Loss: 0.00022975797812519662\n",
      "Epoch 1062  \tTraining Loss: 0.0002295778366089882\tValidation Loss: 0.000229642731044838\n",
      "Epoch 1063  \tTraining Loss: 0.00022946169414530277\tValidation Loss: 0.00022952772006240658\n",
      "Epoch 1064  \tTraining Loss: 0.00022934578539834184\tValidation Loss: 0.00022941295160802233\n",
      "Epoch 1065  \tTraining Loss: 0.00022923011573461764\tValidation Loss: 0.00022929839597619708\n",
      "Epoch 1066  \tTraining Loss: 0.00022911468825810282\tValidation Loss: 0.0002291841076999415\n",
      "Epoch 1067  \tTraining Loss: 0.00022899949160374666\tValidation Loss: 0.00022907006915332266\n",
      "Epoch 1068  \tTraining Loss: 0.00022888452171028996\tValidation Loss: 0.00022895627666878773\n",
      "Epoch 1069  \tTraining Loss: 0.00022876977584801312\tValidation Loss: 0.00022884271034759727\n",
      "Epoch 1070  \tTraining Loss: 0.00022865526106777292\tValidation Loss: 0.00022872936929151875\n",
      "Epoch 1071  \tTraining Loss: 0.0002285409766018667\tValidation Loss: 0.0002286162686071434\n",
      "Epoch 1072  \tTraining Loss: 0.00022842692039611793\tValidation Loss: 0.0002285033993650028\n",
      "Epoch 1073  \tTraining Loss: 0.00022831309117183528\tValidation Loss: 0.00022839076550323266\n",
      "Epoch 1074  \tTraining Loss: 0.00022819948684563794\tValidation Loss: 0.0002282783592966929\n",
      "Epoch 1075  \tTraining Loss: 0.00022808609579085955\tValidation Loss: 0.0002281661844839061\n",
      "Epoch 1076  \tTraining Loss: 0.00022797293316553603\tValidation Loss: 0.00022805426202381692\n",
      "Epoch 1077  \tTraining Loss: 0.00022786000220989546\tValidation Loss: 0.000227942555569498\n",
      "Epoch 1078  \tTraining Loss: 0.00022774729993834793\tValidation Loss: 0.00022783107531604432\n",
      "Epoch 1079  \tTraining Loss: 0.00022763481993621343\tValidation Loss: 0.0002277198137496417\n",
      "Epoch 1080  \tTraining Loss: 0.0002275225623093542\tValidation Loss: 0.0002276087815782121\n",
      "Epoch 1081  \tTraining Loss: 0.0002274105275869323\tValidation Loss: 0.0002274979742661388\n",
      "Epoch 1082  \tTraining Loss: 0.00022729871413507783\tValidation Loss: 0.00022738739394083184\n",
      "Epoch 1083  \tTraining Loss: 0.0002271871223954179\tValidation Loss: 0.00022727703753067853\n",
      "Epoch 1084  \tTraining Loss: 0.0002270757535551869\tValidation Loss: 0.0002271669086773485\n",
      "Epoch 1085  \tTraining Loss: 0.00022696461612358392\tValidation Loss: 0.00022705700539438182\n",
      "Epoch 1086  \tTraining Loss: 0.0002268536969867226\tValidation Loss: 0.0002269473184802382\n",
      "Epoch 1087  \tTraining Loss: 0.00022674299181683297\tValidation Loss: 0.00022683785325842255\n",
      "Epoch 1088  \tTraining Loss: 0.00022663250486986317\tValidation Loss: 0.00022672861022228128\n",
      "Epoch 1089  \tTraining Loss: 0.00022652223555932402\tValidation Loss: 0.00022661958875547938\n",
      "Epoch 1090  \tTraining Loss: 0.00022641218330286624\tValidation Loss: 0.00022651078817175206\n",
      "Epoch 1091  \tTraining Loss: 0.0002263023475209413\tValidation Loss: 0.00022640220779962192\n",
      "Epoch 1092  \tTraining Loss: 0.00022619272763656505\tValidation Loss: 0.00022629384698412733\n",
      "Epoch 1093  \tTraining Loss: 0.00022608332109245546\tValidation Loss: 0.0002261857507850962\n",
      "Epoch 1094  \tTraining Loss: 0.00022597412171224788\tValidation Loss: 0.00022607779050378272\n",
      "Epoch 1095  \tTraining Loss: 0.00022586514853571024\tValidation Loss: 0.00022597009788873253\n",
      "Epoch 1096  \tTraining Loss: 0.00022575638917549475\tValidation Loss: 0.00022586263209433603\n",
      "Epoch 1097  \tTraining Loss: 0.00022564784280366075\tValidation Loss: 0.00022575538472347254\n",
      "Epoch 1098  \tTraining Loss: 0.0002255395089327581\tValidation Loss: 0.00022564835531150578\n",
      "Epoch 1099  \tTraining Loss: 0.00022543138696762638\tValidation Loss: 0.0002255415452180394\n",
      "Epoch 1100  \tTraining Loss: 0.00022532347616424156\tValidation Loss: 0.0002254349459527395\n",
      "Epoch 1101  \tTraining Loss: 0.00022521577617054935\tValidation Loss: 0.0002253285600910699\n",
      "Epoch 1102  \tTraining Loss: 0.00022510828642981052\tValidation Loss: 0.0002252223872419751\n",
      "Epoch 1103  \tTraining Loss: 0.00022500100638791964\tValidation Loss: 0.0002251164267565305\n",
      "Epoch 1104  \tTraining Loss: 0.00022489393549303822\tValidation Loss: 0.00022501067798398684\n",
      "Epoch 1105  \tTraining Loss: 0.00022478707508576101\tValidation Loss: 0.0002249051430126302\n",
      "Epoch 1106  \tTraining Loss: 0.0002246804303390354\tValidation Loss: 0.00022479987363059988\n",
      "Epoch 1107  \tTraining Loss: 0.00022457400180826616\tValidation Loss: 0.00022469478185289227\n",
      "Epoch 1108  \tTraining Loss: 0.0002244677802827403\tValidation Loss: 0.0002245898948891949\n",
      "Epoch 1109  \tTraining Loss: 0.0002243617651472135\tValidation Loss: 0.00022448521571751516\n",
      "Epoch 1110  \tTraining Loss: 0.00022425595573876333\tValidation Loss: 0.00022438074395354896\n",
      "Epoch 1111  \tTraining Loss: 0.0002241503497125465\tValidation Loss: 0.00022427647912123325\n",
      "Epoch 1112  \tTraining Loss: 0.00022404494844173193\tValidation Loss: 0.00022417242115269274\n",
      "Epoch 1113  \tTraining Loss: 0.00022393974960061523\tValidation Loss: 0.00022406846887787387\n",
      "Epoch 1114  \tTraining Loss: 0.00022383475016575106\tValidation Loss: 0.00022396488573259626\n",
      "Epoch 1115  \tTraining Loss: 0.00022372995378872458\tValidation Loss: 0.00022386145088484498\n",
      "Epoch 1116  \tTraining Loss: 0.00022362536170184092\tValidation Loss: 0.00022375821440894574\n",
      "Epoch 1117  \tTraining Loss: 0.00022352097182257527\tValidation Loss: 0.00022365519077044326\n",
      "Epoch 1118  \tTraining Loss: 0.0002234167857902913\tValidation Loss: 0.0002235522663129376\n",
      "Epoch 1119  \tTraining Loss: 0.00022331280014038316\tValidation Loss: 0.00022344971051943956\n",
      "Epoch 1120  \tTraining Loss: 0.0002232090145239204\tValidation Loss: 0.0002233472972330474\n",
      "Epoch 1121  \tTraining Loss: 0.00022310542905649823\tValidation Loss: 0.0002232450774262815\n",
      "Epoch 1122  \tTraining Loss: 0.00022300204463433302\tValidation Loss: 0.00022314295907298608\n",
      "Epoch 1123  \tTraining Loss: 0.0002228988634981175\tValidation Loss: 0.00022304121128511724\n",
      "Epoch 1124  \tTraining Loss: 0.00022279587982425078\tValidation Loss: 0.00022293960350407534\n",
      "Epoch 1125  \tTraining Loss: 0.00022269309418833944\tValidation Loss: 0.00022283818686769772\n",
      "Epoch 1126  \tTraining Loss: 0.0002225904933819676\tValidation Loss: 0.00022273696262998112\n",
      "Epoch 1127  \tTraining Loss: 0.00022248807262130425\tValidation Loss: 0.00022263577977150632\n",
      "Epoch 1128  \tTraining Loss: 0.00022238585597024704\tValidation Loss: 0.00022253500085830412\n",
      "Epoch 1129  \tTraining Loss: 0.00022228383458296137\tValidation Loss: 0.00022243437693150042\n",
      "Epoch 1130  \tTraining Loss: 0.0002221820088469108\tValidation Loss: 0.00022233396074123143\n",
      "Epoch 1131  \tTraining Loss: 0.0002220803800002156\tValidation Loss: 0.00022223374293431006\n",
      "Epoch 1132  \tTraining Loss: 0.00022197894997774985\tValidation Loss: 0.000222133722812651\n",
      "Epoch 1133  \tTraining Loss: 0.00022187771697517535\tValidation Loss: 0.00022203389940515247\n",
      "Epoch 1134  \tTraining Loss: 0.0002217766803844588\tValidation Loss: 0.0002219341634203993\n",
      "Epoch 1135  \tTraining Loss: 0.00022167583674369916\tValidation Loss: 0.00022183479706339924\n",
      "Epoch 1136  \tTraining Loss: 0.0002215751851744455\tValidation Loss: 0.0002217355629069291\n",
      "Epoch 1137  \tTraining Loss: 0.000221474725962649\tValidation Loss: 0.00022163650437182384\n",
      "Epoch 1138  \tTraining Loss: 0.00022137445705856432\tValidation Loss: 0.0002215376418894293\n",
      "Epoch 1139  \tTraining Loss: 0.00022127437911652096\tValidation Loss: 0.00022143897287970967\n",
      "Epoch 1140  \tTraining Loss: 0.00022117449164556723\tValidation Loss: 0.00022134049584795603\n",
      "Epoch 1141  \tTraining Loss: 0.00022107479415780384\tValidation Loss: 0.00022124220988578773\n",
      "Epoch 1142  \tTraining Loss: 0.00022097528622696782\tValidation Loss: 0.0002211441682729779\n",
      "Epoch 1143  \tTraining Loss: 0.00022087597004316433\tValidation Loss: 0.0002210462715964338\n",
      "Epoch 1144  \tTraining Loss: 0.0002207768425741361\tValidation Loss: 0.0002209484451675705\n",
      "Epoch 1145  \tTraining Loss: 0.00022067790328595674\tValidation Loss: 0.00022085098586096993\n",
      "Epoch 1146  \tTraining Loss: 0.00022057915131465117\tValidation Loss: 0.00022075364804317464\n",
      "Epoch 1147  \tTraining Loss: 0.0002204805864024601\tValidation Loss: 0.0002206564872114236\n",
      "Epoch 1148  \tTraining Loss: 0.00022038220791928122\tValidation Loss: 0.00022055951081193087\n",
      "Epoch 1149  \tTraining Loss: 0.00022028401743129386\tValidation Loss: 0.0002204627228699151\n",
      "Epoch 1150  \tTraining Loss: 0.0002201860151903907\tValidation Loss: 0.0002203661170076595\n",
      "Epoch 1151  \tTraining Loss: 0.00022008819933917633\tValidation Loss: 0.00022026969787776379\n",
      "Epoch 1152  \tTraining Loss: 0.00021999056799899076\tValidation Loss: 0.0002201734639693763\n",
      "Epoch 1153  \tTraining Loss: 0.0002198931207254553\tValidation Loss: 0.00022007741460722266\n",
      "Epoch 1154  \tTraining Loss: 0.0002197958570411916\tValidation Loss: 0.00021998154934612957\n",
      "Epoch 1155  \tTraining Loss: 0.00021969877644315524\tValidation Loss: 0.0002198858676733967\n",
      "Epoch 1156  \tTraining Loss: 0.00021960187846429272\tValidation Loss: 0.00021979036908606998\n",
      "Epoch 1157  \tTraining Loss: 0.00021950516263873136\tValidation Loss: 0.00021969505308226412\n",
      "Epoch 1158  \tTraining Loss: 0.00021940862850176532\tValidation Loss: 0.0002195999191615526\n",
      "Epoch 1159  \tTraining Loss: 0.00021931227558984252\tValidation Loss: 0.00021950496682513295\n",
      "Epoch 1160  \tTraining Loss: 0.00021921610344055318\tValidation Loss: 0.0002194101955758191\n",
      "Epoch 1161  \tTraining Loss: 0.00021912011159261842\tValidation Loss: 0.00021931560491799234\n",
      "Epoch 1162  \tTraining Loss: 0.00021902429958587904\tValidation Loss: 0.00021922119435755308\n",
      "Epoch 1163  \tTraining Loss: 0.00021892866862653954\tValidation Loss: 0.00021912694800032368\n",
      "Epoch 1164  \tTraining Loss: 0.00021883322055928135\tValidation Loss: 0.0002190328971094897\n",
      "Epoch 1165  \tTraining Loss: 0.00021873795092424515\tValidation Loss: 0.00021893902598638882\n",
      "Epoch 1166  \tTraining Loss: 0.0002186428592587492\tValidation Loss: 0.00021884533285164114\n",
      "Epoch 1167  \tTraining Loss: 0.00021854794548862588\tValidation Loss: 0.00021875174624793673\n",
      "Epoch 1168  \tTraining Loss: 0.00021845320754675373\tValidation Loss: 0.00021865837215329672\n",
      "Epoch 1169  \tTraining Loss: 0.00021835864641638584\tValidation Loss: 0.00021856526072071744\n",
      "Epoch 1170  \tTraining Loss: 0.00021826426141379927\tValidation Loss: 0.00021847222441995673\n",
      "Epoch 1171  \tTraining Loss: 0.00021817005194054047\tValidation Loss: 0.00021837938915968645\n",
      "Epoch 1172  \tTraining Loss: 0.00021807601773874322\tValidation Loss: 0.0002182867485319592\n",
      "Epoch 1173  \tTraining Loss: 0.00021798215844248111\tValidation Loss: 0.00021819436637373008\n",
      "Epoch 1174  \tTraining Loss: 0.00021788847360098816\tValidation Loss: 0.00021810205110217714\n",
      "Epoch 1175  \tTraining Loss: 0.0002177949646952386\tValidation Loss: 0.00021800993161779597\n",
      "Epoch 1176  \tTraining Loss: 0.0002177016320718637\tValidation Loss: 0.00021791799817861\n",
      "Epoch 1177  \tTraining Loss: 0.00021760845908715758\tValidation Loss: 0.00021782623068470334\n",
      "Epoch 1178  \tTraining Loss: 0.0002175154524638835\tValidation Loss: 0.00021773470572734595\n",
      "Epoch 1179  \tTraining Loss: 0.00021742261795083028\tValidation Loss: 0.0002176432420544124\n",
      "Epoch 1180  \tTraining Loss: 0.00021732995516806936\tValidation Loss: 0.00021755197286388132\n",
      "Epoch 1181  \tTraining Loss: 0.0002172374637799345\tValidation Loss: 0.00021746088227485148\n",
      "Epoch 1182  \tTraining Loss: 0.0002171451433650642\tValidation Loss: 0.0002173700396597049\n",
      "Epoch 1183  \tTraining Loss: 0.00021705298837698835\tValidation Loss: 0.0002172791295746774\n",
      "Epoch 1184  \tTraining Loss: 0.00021696099274535352\tValidation Loss: 0.00021718842785240064\n",
      "Epoch 1185  \tTraining Loss: 0.0002168691827354358\tValidation Loss: 0.0002170979769647633\n",
      "Epoch 1186  \tTraining Loss: 0.00021677753876143615\tValidation Loss: 0.00021700772384795897\n",
      "Epoch 1187  \tTraining Loss: 0.00021668605879237053\tValidation Loss: 0.00021691771249180675\n",
      "Epoch 1188  \tTraining Loss: 0.00021659474738516213\tValidation Loss: 0.00021682775689886463\n",
      "Epoch 1189  \tTraining Loss: 0.00021650360369733227\tValidation Loss: 0.0002167379793285324\n",
      "Epoch 1190  \tTraining Loss: 0.0002164126227446835\tValidation Loss: 0.00021664839242078505\n",
      "Epoch 1191  \tTraining Loss: 0.00021632179915017253\tValidation Loss: 0.0002165589619987465\n",
      "Epoch 1192  \tTraining Loss: 0.0002162311445543927\tValidation Loss: 0.00021646970960989198\n",
      "Epoch 1193  \tTraining Loss: 0.00021614066431430032\tValidation Loss: 0.0002163806803375452\n",
      "Epoch 1194  \tTraining Loss: 0.00021605035019180092\tValidation Loss: 0.00021629170944533125\n",
      "Epoch 1195  \tTraining Loss: 0.00021596020088791637\tValidation Loss: 0.00021620292742085442\n",
      "Epoch 1196  \tTraining Loss: 0.00021587021652203425\tValidation Loss: 0.00021611431701194406\n",
      "Epoch 1197  \tTraining Loss: 0.00021578039664854902\tValidation Loss: 0.00021602587578349296\n",
      "Epoch 1198  \tTraining Loss: 0.000215690732377865\tValidation Loss: 0.0002159376798026911\n",
      "Epoch 1199  \tTraining Loss: 0.00021560122227521405\tValidation Loss: 0.0002158495247360642\n",
      "Epoch 1200  \tTraining Loss: 0.00021551187534672486\tValidation Loss: 0.00021576155751441484\n",
      "Epoch 1201  \tTraining Loss: 0.0002154226913249038\tValidation Loss: 0.00021567376015342558\n",
      "Epoch 1202  \tTraining Loss: 0.00021533366978031263\tValidation Loss: 0.00021558612714263597\n",
      "Epoch 1203  \tTraining Loss: 0.00021524481406181258\tValidation Loss: 0.00021549865088719633\n",
      "Epoch 1204  \tTraining Loss: 0.00021515612189983304\tValidation Loss: 0.00021541141694706557\n",
      "Epoch 1205  \tTraining Loss: 0.0002150675984812458\tValidation Loss: 0.00021532414817544875\n",
      "Epoch 1206  \tTraining Loss: 0.00021497924750163381\tValidation Loss: 0.00021523712171463093\n",
      "Epoch 1207  \tTraining Loss: 0.00021489105603279018\tValidation Loss: 0.000215150286225852\n",
      "Epoch 1208  \tTraining Loss: 0.0002148029960276327\tValidation Loss: 0.0002150636101415961\n",
      "Epoch 1209  \tTraining Loss: 0.00021471509563858766\tValidation Loss: 0.00021497709423087187\n",
      "Epoch 1210  \tTraining Loss: 0.0002146273572098238\tValidation Loss: 0.00021489079667667949\n",
      "Epoch 1211  \tTraining Loss: 0.00021453978309391128\tValidation Loss: 0.00021480450140994064\n",
      "Epoch 1212  \tTraining Loss: 0.00021445236017125934\tValidation Loss: 0.00021471842473252245\n",
      "Epoch 1213  \tTraining Loss: 0.00021436508499581851\tValidation Loss: 0.00021463251282149523\n",
      "Epoch 1214  \tTraining Loss: 0.00021427796738290728\tValidation Loss: 0.00021454675930420957\n",
      "Epoch 1215  \tTraining Loss: 0.0002141910040288603\tValidation Loss: 0.00021446123653405546\n",
      "Epoch 1216  \tTraining Loss: 0.00021410419642528997\tValidation Loss: 0.00021437583423011155\n",
      "Epoch 1217  \tTraining Loss: 0.0002140175455631896\tValidation Loss: 0.0002142906501799966\n",
      "Epoch 1218  \tTraining Loss: 0.00021393105046131107\tValidation Loss: 0.00021420549008247105\n",
      "Epoch 1219  \tTraining Loss: 0.0002138447107909176\tValidation Loss: 0.0002141205063985746\n",
      "Epoch 1220  \tTraining Loss: 0.00021375852621167824\tValidation Loss: 0.00021403568203355925\n",
      "Epoch 1221  \tTraining Loss: 0.00021367249608962574\tValidation Loss: 0.00021395101661791203\n",
      "Epoch 1222  \tTraining Loss: 0.00021358660833227508\tValidation Loss: 0.00021386650152994903\n",
      "Epoch 1223  \tTraining Loss: 0.00021350087555584645\tValidation Loss: 0.0002137822164104818\n",
      "Epoch 1224  \tTraining Loss: 0.00021341529726453143\tValidation Loss: 0.00021369795932775636\n",
      "Epoch 1225  \tTraining Loss: 0.00021332987186175735\tValidation Loss: 0.00021361389380210834\n",
      "Epoch 1226  \tTraining Loss: 0.00021324459915440845\tValidation Loss: 0.00021352999597801795\n",
      "Epoch 1227  \tTraining Loss: 0.00021315947916540086\tValidation Loss: 0.00021344625148497271\n",
      "Epoch 1228  \tTraining Loss: 0.0002130745155031742\tValidation Loss: 0.00021336264696856743\n",
      "Epoch 1229  \tTraining Loss: 0.00021298970654748002\tValidation Loss: 0.0002132791897062515\n",
      "Epoch 1230  \tTraining Loss: 0.00021290504857736944\tValidation Loss: 0.00021319588220936617\n",
      "Epoch 1231  \tTraining Loss: 0.00021282053594707612\tValidation Loss: 0.00021311281210567617\n",
      "Epoch 1232  \tTraining Loss: 0.00021273616448972695\tValidation Loss: 0.00021302975877950727\n",
      "Epoch 1233  \tTraining Loss: 0.0002126519428067707\tValidation Loss: 0.00021294688090041955\n",
      "Epoch 1234  \tTraining Loss: 0.00021256787057931194\tValidation Loss: 0.0002128641587247511\n",
      "Epoch 1235  \tTraining Loss: 0.0002124839473931718\tValidation Loss: 0.00021278158530278418\n",
      "Epoch 1236  \tTraining Loss: 0.0002124001742477866\tValidation Loss: 0.00021269916881766008\n",
      "Epoch 1237  \tTraining Loss: 0.00021231655342529996\tValidation Loss: 0.00021261689042661512\n",
      "Epoch 1238  \tTraining Loss: 0.00021223308065754552\tValidation Loss: 0.00021253483768698958\n",
      "Epoch 1239  \tTraining Loss: 0.00021214975524452197\tValidation Loss: 0.0002124528022934798\n",
      "Epoch 1240  \tTraining Loss: 0.0002120665767660484\tValidation Loss: 0.00021237094068191786\n",
      "Epoch 1241  \tTraining Loss: 0.00021198354496962905\tValidation Loss: 0.00021228923218933983\n",
      "Epoch 1242  \tTraining Loss: 0.00021190065944610357\tValidation Loss: 0.00021220767191461483\n",
      "Epoch 1243  \tTraining Loss: 0.000211817918158451\tValidation Loss: 0.00021212634386123485\n",
      "Epoch 1244  \tTraining Loss: 0.00021173531445389248\tValidation Loss: 0.00021204502739139631\n",
      "Epoch 1245  \tTraining Loss: 0.00021165285175024835\tValidation Loss: 0.00021196388573396473\n",
      "Epoch 1246  \tTraining Loss: 0.00021157052695889192\tValidation Loss: 0.00021188289268136529\n",
      "Epoch 1247  \tTraining Loss: 0.00021148834933675507\tValidation Loss: 0.00021180219521646675\n",
      "Epoch 1248  \tTraining Loss: 0.00021140632106315415\tValidation Loss: 0.00021172147314128882\n",
      "Epoch 1249  \tTraining Loss: 0.00021132443621744854\tValidation Loss: 0.00021164091249326985\n",
      "Epoch 1250  \tTraining Loss: 0.00021124269078048654\tValidation Loss: 0.00021156053675255138\n",
      "Epoch 1251  \tTraining Loss: 0.000211161107049217\tValidation Loss: 0.0002114802700857499\n",
      "Epoch 1252  \tTraining Loss: 0.0002110796665473577\tValidation Loss: 0.00021140023013520914\n",
      "Epoch 1253  \tTraining Loss: 0.00021099836826131657\tValidation Loss: 0.0002113201999394191\n",
      "Epoch 1254  \tTraining Loss: 0.00021091721152591576\tValidation Loss: 0.00021124033954879394\n",
      "Epoch 1255  \tTraining Loss: 0.00021083619174041716\tValidation Loss: 0.00021116063434931977\n",
      "Epoch 1256  \tTraining Loss: 0.00021075530759259899\tValidation Loss: 0.00021108105927148812\n",
      "Epoch 1257  \tTraining Loss: 0.0002106745641173461\tValidation Loss: 0.00021100168529814204\n",
      "Epoch 1258  \tTraining Loss: 0.00021059396084359986\tValidation Loss: 0.00021092231607403278\n",
      "Epoch 1259  \tTraining Loss: 0.00021051349855205883\tValidation Loss: 0.0002108431153918266\n",
      "Epoch 1260  \tTraining Loss: 0.00021043317560101913\tValidation Loss: 0.00021076406060092977\n",
      "Epoch 1261  \tTraining Loss: 0.00021035299335772173\tValidation Loss: 0.00021068514417681925\n",
      "Epoch 1262  \tTraining Loss: 0.00021027295231355333\tValidation Loss: 0.0002106064535646663\n",
      "Epoch 1263  \tTraining Loss: 0.00021019304922912234\tValidation Loss: 0.00021052776649011241\n",
      "Epoch 1264  \tTraining Loss: 0.00021011327847347434\tValidation Loss: 0.0002104492525516727\n",
      "Epoch 1265  \tTraining Loss: 0.00021003363895428557\tValidation Loss: 0.00021037087737033373\n",
      "Epoch 1266  \tTraining Loss: 0.0002099541365322693\tValidation Loss: 0.00021029264043233035\n",
      "Epoch 1267  \tTraining Loss: 0.0002098747717794681\tValidation Loss: 0.00021021462401296084\n",
      "Epoch 1268  \tTraining Loss: 0.00020979554632966015\tValidation Loss: 0.00021013661209267243\n",
      "Epoch 1269  \tTraining Loss: 0.00020971645675081912\tValidation Loss: 0.0002100587684802625\n",
      "Epoch 1270  \tTraining Loss: 0.00020963750277270534\tValidation Loss: 0.0002099810681387291\n",
      "Epoch 1271  \tTraining Loss: 0.00020955868400906907\tValidation Loss: 0.00020990350531698948\n",
      "Epoch 1272  \tTraining Loss: 0.00020948000027034926\tValidation Loss: 0.00020982616353852672\n",
      "Epoch 1273  \tTraining Loss: 0.00020940145029195244\tValidation Loss: 0.00020974882362197143\n",
      "Epoch 1274  \tTraining Loss: 0.00020932303337042765\tValidation Loss: 0.00020967164558112044\n",
      "Epoch 1275  \tTraining Loss: 0.00020924475024969793\tValidation Loss: 0.00020959460684168775\n",
      "Epoch 1276  \tTraining Loss: 0.00020916660053719855\tValidation Loss: 0.00020951770291362476\n",
      "Epoch 1277  \tTraining Loss: 0.00020908858398811813\tValidation Loss: 0.00020944101828941037\n",
      "Epoch 1278  \tTraining Loss: 0.0002090107001279339\tValidation Loss: 0.00020936432828078184\n",
      "Epoch 1279  \tTraining Loss: 0.00020893295065894655\tValidation Loss: 0.00020928787033887094\n",
      "Epoch 1280  \tTraining Loss: 0.00020885533466078414\tValidation Loss: 0.00020921150960698628\n",
      "Epoch 1281  \tTraining Loss: 0.00020877784918367054\tValidation Loss: 0.00020913527031005193\n",
      "Epoch 1282  \tTraining Loss: 0.00020870049789632945\tValidation Loss: 0.0002090592488402145\n",
      "Epoch 1283  \tTraining Loss: 0.00020862327819372374\tValidation Loss: 0.00020898321572141479\n",
      "Epoch 1284  \tTraining Loss: 0.00020854619283749878\tValidation Loss: 0.00020890734956471925\n",
      "Epoch 1285  \tTraining Loss: 0.00020846923383221062\tValidation Loss: 0.00020883161814074088\n",
      "Epoch 1286  \tTraining Loss: 0.00020839240455074214\tValidation Loss: 0.0002087560183925258\n",
      "Epoch 1287  \tTraining Loss: 0.0002083157042716801\tValidation Loss: 0.00020868048962122533\n",
      "Epoch 1288  \tTraining Loss: 0.0002082391223724034\tValidation Loss: 0.00020860524172253926\n",
      "Epoch 1289  \tTraining Loss: 0.00020816267464502292\tValidation Loss: 0.000208529987872401\n",
      "Epoch 1290  \tTraining Loss: 0.00020808635901237887\tValidation Loss: 0.00020845489995761866\n",
      "Epoch 1291  \tTraining Loss: 0.00020801017129905622\tValidation Loss: 0.00020837994796965093\n",
      "Epoch 1292  \tTraining Loss: 0.000207934111126877\tValidation Loss: 0.00020830512589438855\n",
      "Epoch 1293  \tTraining Loss: 0.0002078581770919354\tValidation Loss: 0.0002082304334398805\n",
      "Epoch 1294  \tTraining Loss: 0.0002077823696173078\tValidation Loss: 0.00020815603930880228\n",
      "Epoch 1295  \tTraining Loss: 0.00020770669453062184\tValidation Loss: 0.0002080815848432614\n",
      "Epoch 1296  \tTraining Loss: 0.00020763114419340229\tValidation Loss: 0.0002080072700915224\n",
      "Epoch 1297  \tTraining Loss: 0.00020755571729654416\tValidation Loss: 0.00020793307797429511\n",
      "Epoch 1298  \tTraining Loss: 0.00020748041749441343\tValidation Loss: 0.00020785899898915372\n",
      "Epoch 1299  \tTraining Loss: 0.00020740524493163972\tValidation Loss: 0.00020778505140055687\n",
      "Epoch 1300  \tTraining Loss: 0.0002073301982863327\tValidation Loss: 0.0002077113127457553\n",
      "Epoch 1301  \tTraining Loss: 0.00020725527804537152\tValidation Loss: 0.0002076375584775253\n",
      "Epoch 1302  \tTraining Loss: 0.0002071804816549809\tValidation Loss: 0.00020756396063791995\n",
      "Epoch 1303  \tTraining Loss: 0.0002071058089102285\tValidation Loss: 0.0002074904906541314\n",
      "Epoch 1304  \tTraining Loss: 0.00020703126028673904\tValidation Loss: 0.00020741713741876929\n",
      "Epoch 1305  \tTraining Loss: 0.00020695683627887403\tValidation Loss: 0.00020734391059290736\n",
      "Epoch 1306  \tTraining Loss: 0.00020688254241350905\tValidation Loss: 0.00020727083724348755\n",
      "Epoch 1307  \tTraining Loss: 0.0002068083863323426\tValidation Loss: 0.00020719791286091018\n",
      "Epoch 1308  \tTraining Loss: 0.00020673435430343918\tValidation Loss: 0.00020712504846728062\n",
      "Epoch 1309  \tTraining Loss: 0.00020666044339868255\tValidation Loss: 0.00020705224370527294\n",
      "Epoch 1310  \tTraining Loss: 0.00020658665398042998\tValidation Loss: 0.0002069796616394465\n",
      "Epoch 1311  \tTraining Loss: 0.0002065129849699498\tValidation Loss: 0.0002069071629546413\n",
      "Epoch 1312  \tTraining Loss: 0.00020643943526231243\tValidation Loss: 0.00020683473193364998\n",
      "Epoch 1313  \tTraining Loss: 0.00020636600550247893\tValidation Loss: 0.00020676251799107298\n",
      "Epoch 1314  \tTraining Loss: 0.00020629269590626044\tValidation Loss: 0.00020669032623845405\n",
      "Epoch 1315  \tTraining Loss: 0.0002062195068824443\tValidation Loss: 0.00020661834648979998\n",
      "Epoch 1316  \tTraining Loss: 0.00020614643730228237\tValidation Loss: 0.00020654648197982793\n",
      "Epoch 1317  \tTraining Loss: 0.0002060734862334164\tValidation Loss: 0.00020647468273372118\n",
      "Epoch 1318  \tTraining Loss: 0.00020600065383346832\tValidation Loss: 0.0002064029434267771\n",
      "Epoch 1319  \tTraining Loss: 0.00020592793934257056\tValidation Loss: 0.00020633136953045132\n",
      "Epoch 1320  \tTraining Loss: 0.00020585534300483142\tValidation Loss: 0.00020625997000885664\n",
      "Epoch 1321  \tTraining Loss: 0.00020578286419957996\tValidation Loss: 0.00020618859422972386\n",
      "Epoch 1322  \tTraining Loss: 0.00020571050253551478\tValidation Loss: 0.00020611742531768628\n",
      "Epoch 1323  \tTraining Loss: 0.00020563825788612618\tValidation Loss: 0.0002060462785395165\n",
      "Epoch 1324  \tTraining Loss: 0.0002055661295687515\tValidation Loss: 0.00020597528578508245\n",
      "Epoch 1325  \tTraining Loss: 0.00020549411767889408\tValidation Loss: 0.00020590455253146348\n",
      "Epoch 1326  \tTraining Loss: 0.00020542222170555802\tValidation Loss: 0.00020583369316434067\n",
      "Epoch 1327  \tTraining Loss: 0.0002053504410642587\tValidation Loss: 0.00020576302070877725\n",
      "Epoch 1328  \tTraining Loss: 0.00020527877586528723\tValidation Loss: 0.00020569252456326457\n",
      "Epoch 1329  \tTraining Loss: 0.00020520722552041024\tValidation Loss: 0.00020562205408629014\n",
      "Epoch 1330  \tTraining Loss: 0.0002051357896907592\tValidation Loss: 0.00020555173681167954\n",
      "Epoch 1331  \tTraining Loss: 0.00020506446829950288\tValidation Loss: 0.00020548158572357753\n",
      "Epoch 1332  \tTraining Loss: 0.00020499326085307167\tValidation Loss: 0.0002054114561958353\n",
      "Epoch 1333  \tTraining Loss: 0.00020492216695182564\tValidation Loss: 0.00020534147685716302\n",
      "Epoch 1334  \tTraining Loss: 0.00020485118645877302\tValidation Loss: 0.0002052716614781835\n",
      "Epoch 1335  \tTraining Loss: 0.00020478031909972386\tValidation Loss: 0.00020520186964024476\n",
      "Epoch 1336  \tTraining Loss: 0.00020470955551076366\tValidation Loss: 0.0002051323327899023\n",
      "Epoch 1337  \tTraining Loss: 0.0002046388930725738\tValidation Loss: 0.00020506275249013026\n",
      "Epoch 1338  \tTraining Loss: 0.00020456834275704865\tValidation Loss: 0.0002049933183942507\n",
      "Epoch 1339  \tTraining Loss: 0.00020449790445205016\tValidation Loss: 0.00020492405334890856\n",
      "Epoch 1340  \tTraining Loss: 0.00020442757744309792\tValidation Loss: 0.0002048548143555759\n",
      "Epoch 1341  \tTraining Loss: 0.00020435736167723412\tValidation Loss: 0.00020478574308273442\n",
      "Epoch 1342  \tTraining Loss: 0.00020428725696087504\tValidation Loss: 0.0002047167843697699\n",
      "Epoch 1343  \tTraining Loss: 0.0002042172629488609\tValidation Loss: 0.0002046479619691504\n",
      "Epoch 1344  \tTraining Loss: 0.00020414737939485875\tValidation Loss: 0.00020457935562705608\n",
      "Epoch 1345  \tTraining Loss: 0.0002040776058981359\tValidation Loss: 0.00020451076965951652\n",
      "Epoch 1346  \tTraining Loss: 0.000204007942096463\tValidation Loss: 0.00020444232889323626\n",
      "Epoch 1347  \tTraining Loss: 0.00020393838781105313\tValidation Loss: 0.00020437408489500482\n",
      "Epoch 1348  \tTraining Loss: 0.00020386894280419147\tValidation Loss: 0.00020430580698734685\n",
      "Epoch 1349  \tTraining Loss: 0.00020379961091080342\tValidation Loss: 0.00020423783750392842\n",
      "Epoch 1350  \tTraining Loss: 0.00020373039213991584\tValidation Loss: 0.0002041698846093181\n",
      "Epoch 1351  \tTraining Loss: 0.0002036612818450801\tValidation Loss: 0.00020410201070152854\n",
      "Epoch 1352  \tTraining Loss: 0.0002035922796116124\tValidation Loss: 0.0002040342324305278\n",
      "Epoch 1353  \tTraining Loss: 0.000203523385124578\tValidation Loss: 0.00020396655452680444\n",
      "Epoch 1354  \tTraining Loss: 0.00020345459805103775\tValidation Loss: 0.00020389899061683944\n",
      "Epoch 1355  \tTraining Loss: 0.00020338590857443725\tValidation Loss: 0.00020383152232595543\n",
      "Epoch 1356  \tTraining Loss: 0.00020331732599719143\tValidation Loss: 0.00020376415764550613\n",
      "Epoch 1357  \tTraining Loss: 0.00020324885025532806\tValidation Loss: 0.0002036968982184596\n",
      "Epoch 1358  \tTraining Loss: 0.00020318048112682272\tValidation Loss: 0.00020362974163604922\n",
      "Epoch 1359  \tTraining Loss: 0.00020311221796947166\tValidation Loss: 0.00020356268876900723\n",
      "Epoch 1360  \tTraining Loss: 0.0002030440604857955\tValidation Loss: 0.00020349573967206786\n",
      "Epoch 1361  \tTraining Loss: 0.0002029760083806347\tValidation Loss: 0.0002034288942167808\n",
      "Epoch 1362  \tTraining Loss: 0.00020290806135975692\tValidation Loss: 0.00020336215219601117\n",
      "Epoch 1363  \tTraining Loss: 0.00020284021912978073\tValidation Loss: 0.00020329551336410872\n",
      "Epoch 1364  \tTraining Loss: 0.00020277248377583148\tValidation Loss: 0.00020322914396534122\n",
      "Epoch 1365  \tTraining Loss: 0.00020270485223060486\tValidation Loss: 0.00020316277896607521\n",
      "Epoch 1366  \tTraining Loss: 0.0002026373107924146\tValidation Loss: 0.0002030964703829634\n",
      "Epoch 1367  \tTraining Loss: 0.0002025698731638606\tValidation Loss: 0.00020303024736172777\n",
      "Epoch 1368  \tTraining Loss: 0.00020250253901851618\tValidation Loss: 0.0002029641188008575\n",
      "Epoch 1369  \tTraining Loss: 0.00020243530805983732\tValidation Loss: 0.00020289808848667157\n",
      "Epoch 1370  \tTraining Loss: 0.0002023681799975995\tValidation Loss: 0.00020283215797219214\n",
      "Epoch 1371  \tTraining Loss: 0.00020230115454380402\tValidation Loss: 0.0002027663278084077\n",
      "Epoch 1372  \tTraining Loss: 0.0002022342314117129\tValidation Loss: 0.00020270059809511274\n",
      "Epoch 1373  \tTraining Loss: 0.00020216741006472952\tValidation Loss: 0.0002026349678526189\n",
      "Epoch 1374  \tTraining Loss: 0.00020210068891607526\tValidation Loss: 0.00020256943589446803\n",
      "Epoch 1375  \tTraining Loss: 0.0002020340691925628\tValidation Loss: 0.00020250400279583976\n",
      "Epoch 1376  \tTraining Loss: 0.00020196755060516967\tValidation Loss: 0.00020243866876860485\n",
      "Epoch 1377  \tTraining Loss: 0.00020190113373034726\tValidation Loss: 0.00020237342444600815\n",
      "Epoch 1378  \tTraining Loss: 0.00020183481819365567\tValidation Loss: 0.00020230828491780352\n",
      "Epoch 1379  \tTraining Loss: 0.00020176860274085837\tValidation Loss: 0.00020224307464864022\n",
      "Epoch 1380  \tTraining Loss: 0.00020170248782708304\tValidation Loss: 0.00020217824890305636\n",
      "Epoch 1381  \tTraining Loss: 0.00020163647240885148\tValidation Loss: 0.0002021134324114073\n",
      "Epoch 1382  \tTraining Loss: 0.00020157055660392675\tValidation Loss: 0.00020204869502269207\n",
      "Epoch 1383  \tTraining Loss: 0.00020150473997373\tValidation Loss: 0.0002019840477058666\n",
      "Epoch 1384  \tTraining Loss: 0.00020143902253163295\tValidation Loss: 0.00020191949153087606\n",
      "Epoch 1385  \tTraining Loss: 0.00020137340584516802\tValidation Loss: 0.00020185503368825845\n",
      "Epoch 1386  \tTraining Loss: 0.00020130788746055948\tValidation Loss: 0.00020179067257595349\n",
      "Epoch 1387  \tTraining Loss: 0.00020124246710335393\tValidation Loss: 0.00020172640809681594\n",
      "Epoch 1388  \tTraining Loss: 0.0002011771445006083\tValidation Loss: 0.00020166224008675532\n",
      "Epoch 1389  \tTraining Loss: 0.000201111913344492\tValidation Loss: 0.00020159821190019367\n",
      "Epoch 1390  \tTraining Loss: 0.00020104677075966646\tValidation Loss: 0.0002015342167336754\n",
      "Epoch 1391  \tTraining Loss: 0.00020098172409188208\tValidation Loss: 0.00020147032650381038\n",
      "Epoch 1392  \tTraining Loss: 0.00020091677101573516\tValidation Loss: 0.00020140652747014978\n",
      "Epoch 1393  \tTraining Loss: 0.00020085191516067735\tValidation Loss: 0.0002013427311653042\n",
      "Epoch 1394  \tTraining Loss: 0.0002007871684668083\tValidation Loss: 0.00020127913554184788\n",
      "Epoch 1395  \tTraining Loss: 0.00020072253696474816\tValidation Loss: 0.00020121560028284974\n",
      "Epoch 1396  \tTraining Loss: 0.0002006580092634848\tValidation Loss: 0.0002011521861402258\n",
      "Epoch 1397  \tTraining Loss: 0.00020059357705116816\tValidation Loss: 0.00020108887737149624\n",
      "Epoch 1398  \tTraining Loss: 0.00020052924646155366\tValidation Loss: 0.00020102557651213376\n",
      "Epoch 1399  \tTraining Loss: 0.0002004650179151016\tValidation Loss: 0.00020096243819556518\n",
      "Epoch 1400  \tTraining Loss: 0.00020040088407690982\tValidation Loss: 0.00020089940696480586\n",
      "Epoch 1401  \tTraining Loss: 0.00020033684492268172\tValidation Loss: 0.00020083647290729592\n",
      "Epoch 1402  \tTraining Loss: 0.00020027289979678361\tValidation Loss: 0.00020077363282759868\n",
      "Epoch 1403  \tTraining Loss: 0.00020020904843494604\tValidation Loss: 0.00020071088572759806\n",
      "Epoch 1404  \tTraining Loss: 0.0002001452905755829\tValidation Loss: 0.0002006482311251811\n",
      "Epoch 1405  \tTraining Loss: 0.00020008162595863696\tValidation Loss: 0.00020058568346503539\n",
      "Epoch 1406  \tTraining Loss: 0.00020001805440129992\tValidation Loss: 0.00020052322700620372\n",
      "Epoch 1407  \tTraining Loss: 0.0001999545786900901\tValidation Loss: 0.00020046086488436642\n",
      "Epoch 1408  \tTraining Loss: 0.00019989119545085366\tValidation Loss: 0.00020039860130037783\n",
      "Epoch 1409  \tTraining Loss: 0.00019982790442687996\tValidation Loss: 0.00020033643221058472\n",
      "Epoch 1410  \tTraining Loss: 0.00019976470536289692\tValidation Loss: 0.00020027435379798945\n",
      "Epoch 1411  \tTraining Loss: 0.0001997015984772276\tValidation Loss: 0.00020021235394321918\n",
      "Epoch 1412  \tTraining Loss: 0.0001996385842292767\tValidation Loss: 0.00020015043526424916\n",
      "Epoch 1413  \tTraining Loss: 0.0001995756616865184\tValidation Loss: 0.00020008861696165485\n",
      "Epoch 1414  \tTraining Loss: 0.0001995128301472936\tValidation Loss: 0.00020002689270497893\n",
      "Epoch 1415  \tTraining Loss: 0.00019945008950698748\tValidation Loss: 0.00019996524787592524\n",
      "Epoch 1416  \tTraining Loss: 0.00019938744058906855\tValidation Loss: 0.00019990369876352144\n",
      "Epoch 1417  \tTraining Loss: 0.00019932488183569408\tValidation Loss: 0.00019984223945748362\n",
      "Epoch 1418  \tTraining Loss: 0.00019926241299481805\tValidation Loss: 0.0001997808691520944\n",
      "Epoch 1419  \tTraining Loss: 0.00019920003381802988\tValidation Loss: 0.0001997195904414111\n",
      "Epoch 1420  \tTraining Loss: 0.00019913774405815943\tValidation Loss: 0.00019965840103997967\n",
      "Epoch 1421  \tTraining Loss: 0.00019907554346897534\tValidation Loss: 0.0001995972996653429\n",
      "Epoch 1422  \tTraining Loss: 0.00019901343180512258\tValidation Loss: 0.00019953628607929407\n",
      "Epoch 1423  \tTraining Loss: 0.00019895140882210494\tValidation Loss: 0.00019947536004699201\n",
      "Epoch 1424  \tTraining Loss: 0.0001988894742762789\tValidation Loss: 0.00019941452133379395\n",
      "Epoch 1425  \tTraining Loss: 0.0001988276279248493\tValidation Loss: 0.00019935376970473516\n",
      "Epoch 1426  \tTraining Loss: 0.00019876586952586678\tValidation Loss: 0.00019929310492476713\n",
      "Epoch 1427  \tTraining Loss: 0.00019870419883822585\tValidation Loss: 0.0001992325267591095\n",
      "Epoch 1428  \tTraining Loss: 0.00019864261562166239\tValidation Loss: 0.0001991720349735348\n",
      "Epoch 1429  \tTraining Loss: 0.00019858111963675225\tValidation Loss: 0.00019911165491986877\n",
      "Epoch 1430  \tTraining Loss: 0.00019851971064490919\tValidation Loss: 0.00019905137114974698\n",
      "Epoch 1431  \tTraining Loss: 0.00019845838863909147\tValidation Loss: 0.00019899117035029835\n",
      "Epoch 1432  \tTraining Loss: 0.00019839715520987198\tValidation Loss: 0.00019893102851474353\n",
      "Epoch 1433  \tTraining Loss: 0.00019833601363007653\tValidation Loss: 0.00019887098652274004\n",
      "Epoch 1434  \tTraining Loss: 0.0001982749580464585\tValidation Loss: 0.00019881103645331087\n",
      "Epoch 1435  \tTraining Loss: 0.00019821398820772784\tValidation Loss: 0.00019875117385058757\n",
      "Epoch 1436  \tTraining Loss: 0.00019815310288690338\tValidation Loss: 0.00019869141772112936\n",
      "Epoch 1437  \tTraining Loss: 0.00019809229380783383\tValidation Loss: 0.00019863173357198723\n",
      "Epoch 1438  \tTraining Loss: 0.0001980315698260481\tValidation Loss: 0.00019857213217645722\n",
      "Epoch 1439  \tTraining Loss: 0.0001979709306983267\tValidation Loss: 0.0001985126142831859\n",
      "Epoch 1440  \tTraining Loss: 0.00019791037756771224\tValidation Loss: 0.0001984531746127824\n",
      "Epoch 1441  \tTraining Loss: 0.000197849910776285\tValidation Loss: 0.00019839382144893185\n",
      "Epoch 1442  \tTraining Loss: 0.00019778952815441956\tValidation Loss: 0.0001983345519509231\n",
      "Epoch 1443  \tTraining Loss: 0.00019772922947378378\tValidation Loss: 0.00019827536564740375\n",
      "Epoch 1444  \tTraining Loss: 0.0001976690145073177\tValidation Loss: 0.00019821626222887547\n",
      "Epoch 1445  \tTraining Loss: 0.0001976088830288217\tValidation Loss: 0.00019815724144196152\n",
      "Epoch 1446  \tTraining Loss: 0.00019754883481291758\tValidation Loss: 0.00019809830305661988\n",
      "Epoch 1447  \tTraining Loss: 0.00019748886831945196\tValidation Loss: 0.0001980394596821904\n",
      "Epoch 1448  \tTraining Loss: 0.0001974289797441629\tValidation Loss: 0.00019798067958978652\n",
      "Epoch 1449  \tTraining Loss: 0.00019736917378712106\tValidation Loss: 0.00019792197941719168\n",
      "Epoch 1450  \tTraining Loss: 0.00019730945438334622\tValidation Loss: 0.0001978634702798074\n",
      "Epoch 1451  \tTraining Loss: 0.0001972498236047116\tValidation Loss: 0.0001978049680005653\n",
      "Epoch 1452  \tTraining Loss: 0.00019719027496836215\tValidation Loss: 0.00019774653160449808\n",
      "Epoch 1453  \tTraining Loss: 0.0001971308063339527\tValidation Loss: 0.00019768808666926197\n",
      "Epoch 1454  \tTraining Loss: 0.0001970714112988673\tValidation Loss: 0.000197629766854749\n",
      "Epoch 1455  \tTraining Loss: 0.00019701209776881902\tValidation Loss: 0.00019757154029786258\n",
      "Epoch 1456  \tTraining Loss: 0.0001969528653562227\tValidation Loss: 0.0001975133984812466\n",
      "Epoch 1457  \tTraining Loss: 0.000196893713836957\tValidation Loss: 0.00019745533866147834\n",
      "Epoch 1458  \tTraining Loss: 0.0001968346429918494\tValidation Loss: 0.0001973973599643122\n",
      "Epoch 1459  \tTraining Loss: 0.00019677565255433164\tValidation Loss: 0.00019733946132883062\n",
      "Epoch 1460  \tTraining Loss: 0.00019671674242298848\tValidation Loss: 0.00019728178688602587\n",
      "Epoch 1461  \tTraining Loss: 0.00019665792620445017\tValidation Loss: 0.00019722410085788064\n",
      "Epoch 1462  \tTraining Loss: 0.00019659918994829428\tValidation Loss: 0.0001971664670243322\n",
      "Epoch 1463  \tTraining Loss: 0.00019654053333766102\tValidation Loss: 0.0001971089003537068\n",
      "Epoch 1464  \tTraining Loss: 0.00019648195614860366\tValidation Loss: 0.0001970514072259569\n",
      "Epoch 1465  \tTraining Loss: 0.0001964234581676399\tValidation Loss: 0.00019699399035398563\n",
      "Epoch 1466  \tTraining Loss: 0.00019636503918403668\tValidation Loss: 0.0001969366508319181\n",
      "Epoch 1467  \tTraining Loss: 0.0001963066991560685\tValidation Loss: 0.0001968793801582523\n",
      "Epoch 1468  \tTraining Loss: 0.00019624843910760252\tValidation Loss: 0.00019682219237268146\n",
      "Epoch 1469  \tTraining Loss: 0.00019619025741147772\tValidation Loss: 0.00019676508284213946\n",
      "Epoch 1470  \tTraining Loss: 0.00019613215385909114\tValidation Loss: 0.00019670805102817108\n",
      "Epoch 1471  \tTraining Loss: 0.00019607412824432192\tValidation Loss: 0.00019665109662242122\n",
      "Epoch 1472  \tTraining Loss: 0.00019601618036204415\tValidation Loss: 0.00019659421938784282\n",
      "Epoch 1473  \tTraining Loss: 0.0001959583100079582\tValidation Loss: 0.000196537419113372\n",
      "Epoch 1474  \tTraining Loss: 0.0001959005169785594\tValidation Loss: 0.00019648069325927444\n",
      "Epoch 1475  \tTraining Loss: 0.00019584280107112943\tValidation Loss: 0.000196424033394833\n",
      "Epoch 1476  \tTraining Loss: 0.00019578516208373301\tValidation Loss: 0.00019636744991767397\n",
      "Epoch 1477  \tTraining Loss: 0.00019572760046951166\tValidation Loss: 0.00019631095274220227\n",
      "Epoch 1478  \tTraining Loss: 0.0001956701165498886\tValidation Loss: 0.0001962545260420505\n",
      "Epoch 1479  \tTraining Loss: 0.00019561270895804143\tValidation Loss: 0.00019619817451957156\n",
      "Epoch 1480  \tTraining Loss: 0.00019555537749252173\tValidation Loss: 0.00019614189839971645\n",
      "Epoch 1481  \tTraining Loss: 0.00019549812195471288\tValidation Loss: 0.00019608569762208372\n",
      "Epoch 1482  \tTraining Loss: 0.00019544094591777308\tValidation Loss: 0.00019602947065247236\n",
      "Epoch 1483  \tTraining Loss: 0.00019538386246412693\tValidation Loss: 0.00019597340952110897\n",
      "Epoch 1484  \tTraining Loss: 0.00019532685461519486\tValidation Loss: 0.0001959174402932027\n",
      "Epoch 1485  \tTraining Loss: 0.00019526992192627374\tValidation Loss: 0.00019586155152815797\n",
      "Epoch 1486  \tTraining Loss: 0.0001952130626332791\tValidation Loss: 0.0001958057418121761\n",
      "Epoch 1487  \tTraining Loss: 0.00019515627641613452\tValidation Loss: 0.00019575000466553734\n",
      "Epoch 1488  \tTraining Loss: 0.00019509956513643535\tValidation Loss: 0.00019569434608855785\n",
      "Epoch 1489  \tTraining Loss: 0.0001950429281590512\tValidation Loss: 0.00019563875275666078\n",
      "Epoch 1490  \tTraining Loss: 0.000194986365376944\tValidation Loss: 0.0001955832302504849\n",
      "Epoch 1491  \tTraining Loss: 0.00019492987998305194\tValidation Loss: 0.000195527777895389\n",
      "Epoch 1492  \tTraining Loss: 0.00019487347444623726\tValidation Loss: 0.00019547240721385818\n",
      "Epoch 1493  \tTraining Loss: 0.00019481714253140488\tValidation Loss: 0.00019541711422812439\n",
      "Epoch 1494  \tTraining Loss: 0.0001947608840225285\tValidation Loss: 0.0001953618905831779\n",
      "Epoch 1495  \tTraining Loss: 0.00019470469868188519\tValidation Loss: 0.00019530675323580925\n",
      "Epoch 1496  \tTraining Loss: 0.00019464858464665757\tValidation Loss: 0.00019525169303178443\n",
      "Epoch 1497  \tTraining Loss: 0.00019459254014819047\tValidation Loss: 0.00019519669949283092\n",
      "Epoch 1498  \tTraining Loss: 0.0001945365683142997\tValidation Loss: 0.00019514178456405516\n",
      "Epoch 1499  \tTraining Loss: 0.00019448066866347398\tValidation Loss: 0.00019508693092034512\n",
      "Epoch 1500  \tTraining Loss: 0.00019442483977550538\tValidation Loss: 0.0001950321797362594\n",
      "Epoch 1501  \tTraining Loss: 0.0001943690760524294\tValidation Loss: 0.000194977463744183\n",
      "Epoch 1502  \tTraining Loss: 0.00019431338432622375\tValidation Loss: 0.000194922819942132\n",
      "Epoch 1503  \tTraining Loss: 0.0001942577644102256\tValidation Loss: 0.00019486824447400992\n",
      "Epoch 1504  \tTraining Loss: 0.00019420221604777042\tValidation Loss: 0.00019481375313035065\n",
      "Epoch 1505  \tTraining Loss: 0.00019414673898615387\tValidation Loss: 0.00019475932795229208\n",
      "Epoch 1506  \tTraining Loss: 0.00019409133315578862\tValidation Loss: 0.0001947049714790504\n",
      "Epoch 1507  \tTraining Loss: 0.0001940359975522523\tValidation Loss: 0.00019465069464353913\n",
      "Epoch 1508  \tTraining Loss: 0.0001939807300852455\tValidation Loss: 0.00019459648131622188\n",
      "Epoch 1509  \tTraining Loss: 0.00019392553326257555\tValidation Loss: 0.0001945423649547342\n",
      "Epoch 1510  \tTraining Loss: 0.00019387040874059732\tValidation Loss: 0.00019448831307536046\n",
      "Epoch 1511  \tTraining Loss: 0.00019381535449001223\tValidation Loss: 0.00019443431789612344\n",
      "Epoch 1512  \tTraining Loss: 0.00019376037047747768\tValidation Loss: 0.00019438038842679623\n",
      "Epoch 1513  \tTraining Loss: 0.00019370545649623853\tValidation Loss: 0.00019432651667255834\n",
      "Epoch 1514  \tTraining Loss: 0.00019365061236797902\tValidation Loss: 0.00019427271282089468\n",
      "Epoch 1515  \tTraining Loss: 0.00019359583791787524\tValidation Loss: 0.00019421897817808022\n",
      "Epoch 1516  \tTraining Loss: 0.00019354113303059656\tValidation Loss: 0.0001941653089325977\n",
      "Epoch 1517  \tTraining Loss: 0.00019348649747549392\tValidation Loss: 0.00019411171927903183\n",
      "Epoch 1518  \tTraining Loss: 0.00019343193099235348\tValidation Loss: 0.0001940581933264724\n",
      "Epoch 1519  \tTraining Loss: 0.00019337743352616308\tValidation Loss: 0.00019400473372144643\n",
      "Epoch 1520  \tTraining Loss: 0.00019332300488053932\tValidation Loss: 0.0001939513419989797\n",
      "Epoch 1521  \tTraining Loss: 0.0001932686448833593\tValidation Loss: 0.00019389801839371772\n",
      "Epoch 1522  \tTraining Loss: 0.00019321435343441548\tValidation Loss: 0.0001938447595691349\n",
      "Epoch 1523  \tTraining Loss: 0.00019316013028046692\tValidation Loss: 0.00019379157875141448\n",
      "Epoch 1524  \tTraining Loss: 0.00019310597518703082\tValidation Loss: 0.00019373846079257293\n",
      "Epoch 1525  \tTraining Loss: 0.0001930518867316818\tValidation Loss: 0.00019368546236307074\n",
      "Epoch 1526  \tTraining Loss: 0.0001929978556912865\tValidation Loss: 0.00019363245810914752\n",
      "Epoch 1527  \tTraining Loss: 0.00019294388767583843\tValidation Loss: 0.00019357951856280593\n",
      "Epoch 1528  \tTraining Loss: 0.000192889987184977\tValidation Loss: 0.00019352666151928178\n",
      "Epoch 1529  \tTraining Loss: 0.00019283615385668257\tValidation Loss: 0.00019347386947528576\n",
      "Epoch 1530  \tTraining Loss: 0.0001927823879100026\tValidation Loss: 0.0001934211630870691\n",
      "Epoch 1531  \tTraining Loss: 0.00019272868923714097\tValidation Loss: 0.0001933685091037971\n",
      "Epoch 1532  \tTraining Loss: 0.00019267505746518477\tValidation Loss: 0.00019331592992020757\n",
      "Epoch 1533  \tTraining Loss: 0.0001926214922425225\tValidation Loss: 0.0001932634123278169\n",
      "Epoch 1534  \tTraining Loss: 0.0001925679940552908\tValidation Loss: 0.00019321095211810016\n",
      "Epoch 1535  \tTraining Loss: 0.00019251456360843468\tValidation Loss: 0.00019315856009619842\n",
      "Epoch 1536  \tTraining Loss: 0.0001924611993453923\tValidation Loss: 0.0001931062434341905\n",
      "Epoch 1537  \tTraining Loss: 0.0001924078997458504\tValidation Loss: 0.00019305399593026482\n",
      "Epoch 1538  \tTraining Loss: 0.00019235466419672648\tValidation Loss: 0.00019300180651872896\n",
      "Epoch 1539  \tTraining Loss: 0.00019230149449675214\tValidation Loss: 0.00019294967935698932\n",
      "Epoch 1540  \tTraining Loss: 0.00019224839025684502\tValidation Loss: 0.00019289762571931168\n",
      "Epoch 1541  \tTraining Loss: 0.00019219535135558014\tValidation Loss: 0.00019284563273313593\n",
      "Epoch 1542  \tTraining Loss: 0.00019214237778913067\tValidation Loss: 0.00019279370086837586\n",
      "Epoch 1543  \tTraining Loss: 0.00019208946935539344\tValidation Loss: 0.00019274184168432753\n",
      "Epoch 1544  \tTraining Loss: 0.00019203662572269342\tValidation Loss: 0.00019269004278603142\n",
      "Epoch 1545  \tTraining Loss: 0.00019198384691853705\tValidation Loss: 0.00019263830480142853\n",
      "Epoch 1546  \tTraining Loss: 0.00019193113287688753\tValidation Loss: 0.00019258663880087788\n",
      "Epoch 1547  \tTraining Loss: 0.00019187848312983324\tValidation Loss: 0.00019253503272314383\n",
      "Epoch 1548  \tTraining Loss: 0.00019182589791502504\tValidation Loss: 0.0001924834840099309\n",
      "Epoch 1549  \tTraining Loss: 0.00019177337728778913\tValidation Loss: 0.00019243200136513896\n",
      "Epoch 1550  \tTraining Loss: 0.00019172092056931087\tValidation Loss: 0.00019238059088945097\n",
      "Epoch 1551  \tTraining Loss: 0.000191668527658498\tValidation Loss: 0.00019232924015443286\n",
      "Epoch 1552  \tTraining Loss: 0.00019161619861689008\tValidation Loss: 0.0001922779498763341\n",
      "Epoch 1553  \tTraining Loss: 0.0001915639330746947\tValidation Loss: 0.00019222672999192898\n",
      "Epoch 1554  \tTraining Loss: 0.00019151173087309534\tValidation Loss: 0.00019217556915753994\n",
      "Epoch 1555  \tTraining Loss: 0.00019145959210379267\tValidation Loss: 0.0001921244684481626\n",
      "Epoch 1556  \tTraining Loss: 0.0001914075163834228\tValidation Loss: 0.0001920734374173049\n",
      "Epoch 1557  \tTraining Loss: 0.00019135550356660285\tValidation Loss: 0.00019202246506335553\n",
      "Epoch 1558  \tTraining Loss: 0.0001913035537666305\tValidation Loss: 0.00019197155260286726\n",
      "Epoch 1559  \tTraining Loss: 0.00019125166653729362\tValidation Loss: 0.00019192070914704484\n",
      "Epoch 1560  \tTraining Loss: 0.00019119984181668388\tValidation Loss: 0.00019186992304276112\n",
      "Epoch 1561  \tTraining Loss: 0.0001911480796795793\tValidation Loss: 0.0001918192040312078\n",
      "Epoch 1562  \tTraining Loss: 0.0001910963796022725\tValidation Loss: 0.00019176854282946983\n",
      "Epoch 1563  \tTraining Loss: 0.00019104474176170473\tValidation Loss: 0.0001917179411344464\n",
      "Epoch 1564  \tTraining Loss: 0.00019099316591541642\tValidation Loss: 0.00019166740735694778\n",
      "Epoch 1565  \tTraining Loss: 0.00019094165177549393\tValidation Loss: 0.00019161693134249908\n",
      "Epoch 1566  \tTraining Loss: 0.00019089019998489416\tValidation Loss: 0.00019156652386684072\n",
      "Epoch 1567  \tTraining Loss: 0.00019083881206670704\tValidation Loss: 0.0001915161813136655\n",
      "Epoch 1568  \tTraining Loss: 0.00019078748554869342\tValidation Loss: 0.0001914658920425623\n",
      "Epoch 1569  \tTraining Loss: 0.00019073622040991794\tValidation Loss: 0.00019141566674596783\n",
      "Epoch 1570  \tTraining Loss: 0.00019068501622213123\tValidation Loss: 0.00019136549773873462\n",
      "Epoch 1571  \tTraining Loss: 0.00019063387320793202\tValidation Loss: 0.00019131538737488427\n",
      "Epoch 1572  \tTraining Loss: 0.00019058279092648954\tValidation Loss: 0.00019126534308021303\n",
      "Epoch 1573  \tTraining Loss: 0.0001905317692687716\tValidation Loss: 0.00019121534878610322\n",
      "Epoch 1574  \tTraining Loss: 0.00019048080744038442\tValidation Loss: 0.00019116542356999703\n",
      "Epoch 1575  \tTraining Loss: 0.00019042990588876942\tValidation Loss: 0.0001911155555718535\n",
      "Epoch 1576  \tTraining Loss: 0.00019037906482623744\tValidation Loss: 0.00019106575179846406\n",
      "Epoch 1577  \tTraining Loss: 0.00019032828369413968\tValidation Loss: 0.00019101600427387966\n",
      "Epoch 1578  \tTraining Loss: 0.00019027756276089648\tValidation Loss: 0.00019096631523976324\n",
      "Epoch 1579  \tTraining Loss: 0.0001902268989682789\tValidation Loss: 0.00019091670897842738\n",
      "Epoch 1580  \tTraining Loss: 0.0001901762839580898\tValidation Loss: 0.00019086714367430908\n",
      "Epoch 1581  \tTraining Loss: 0.00019012572917242882\tValidation Loss: 0.000190817596847441\n",
      "Epoch 1582  \tTraining Loss: 0.00019007523623165614\tValidation Loss: 0.00019076813608028627\n",
      "Epoch 1583  \tTraining Loss: 0.00019002480280855508\tValidation Loss: 0.00019071874609321205\n",
      "Epoch 1584  \tTraining Loss: 0.00018997442917268478\tValidation Loss: 0.0001906694075072043\n",
      "Epoch 1585  \tTraining Loss: 0.00018992411555810918\tValidation Loss: 0.0001906201373266108\n",
      "Epoch 1586  \tTraining Loss: 0.00018987386072340327\tValidation Loss: 0.00019057092390979984\n",
      "Epoch 1587  \tTraining Loss: 0.00018982366496386787\tValidation Loss: 0.00019052177271641047\n",
      "Epoch 1588  \tTraining Loss: 0.00018977352769821383\tValidation Loss: 0.00019047267715106497\n",
      "Epoch 1589  \tTraining Loss: 0.00018972344813978406\tValidation Loss: 0.00019042365497436153\n",
      "Epoch 1590  \tTraining Loss: 0.0001896734252481962\tValidation Loss: 0.00019037468873138282\n",
      "Epoch 1591  \tTraining Loss: 0.0001896234609536239\tValidation Loss: 0.00019032578314342002\n",
      "Epoch 1592  \tTraining Loss: 0.00018957355464964136\tValidation Loss: 0.0001902769327574512\n",
      "Epoch 1593  \tTraining Loss: 0.00018952370667965043\tValidation Loss: 0.0001902281430467863\n",
      "Epoch 1594  \tTraining Loss: 0.00018947391646966414\tValidation Loss: 0.00019017940856447366\n",
      "Epoch 1595  \tTraining Loss: 0.00018942418431972763\tValidation Loss: 0.00019013073442615896\n",
      "Epoch 1596  \tTraining Loss: 0.00018937450971624577\tValidation Loss: 0.00019008211541686435\n",
      "Epoch 1597  \tTraining Loss: 0.0001893248928861328\tValidation Loss: 0.0001900335563834092\n",
      "Epoch 1598  \tTraining Loss: 0.00018927533340619713\tValidation Loss: 0.0001899850523693182\n",
      "Epoch 1599  \tTraining Loss: 0.00018922583140009155\tValidation Loss: 0.000189936607966477\n",
      "Epoch 1600  \tTraining Loss: 0.00018917638656506584\tValidation Loss: 0.00018988821847459219\n",
      "Epoch 1601  \tTraining Loss: 0.00018912699889158546\tValidation Loss: 0.0001898398882349066\n",
      "Epoch 1602  \tTraining Loss: 0.0001890776669218343\tValidation Loss: 0.000189791646945435\n",
      "Epoch 1603  \tTraining Loss: 0.00018902838775458264\tValidation Loss: 0.00018974342488326578\n",
      "Epoch 1604  \tTraining Loss: 0.00018897916551603795\tValidation Loss: 0.00018969525238710214\n",
      "Epoch 1605  \tTraining Loss: 0.0001889299997842709\tValidation Loss: 0.0001896471373262787\n",
      "Epoch 1606  \tTraining Loss: 0.00018888089651365465\tValidation Loss: 0.0001895990500780815\n",
      "Epoch 1607  \tTraining Loss: 0.00018883186134522703\tValidation Loss: 0.000189551004204478\n",
      "Epoch 1608  \tTraining Loss: 0.00018878288239314092\tValidation Loss: 0.0001895030741034451\n",
      "Epoch 1609  \tTraining Loss: 0.0001887339593171267\tValidation Loss: 0.0001894552158580408\n",
      "Epoch 1610  \tTraining Loss: 0.00018868509603800945\tValidation Loss: 0.00018940756298670394\n",
      "Epoch 1611  \tTraining Loss: 0.00018863629588694667\tValidation Loss: 0.00018935985754718512\n",
      "Epoch 1612  \tTraining Loss: 0.00018858755194437184\tValidation Loss: 0.0001893121445609379\n",
      "Epoch 1613  \tTraining Loss: 0.000188538863800645\tValidation Loss: 0.00018926451608536902\n",
      "Epoch 1614  \tTraining Loss: 0.00018849023099235852\tValidation Loss: 0.00018921694833691694\n",
      "Epoch 1615  \tTraining Loss: 0.0001884416541026213\tValidation Loss: 0.0001891694279256757\n",
      "Epoch 1616  \tTraining Loss: 0.00018839313187456002\tValidation Loss: 0.00018912196240799525\n",
      "Epoch 1617  \tTraining Loss: 0.0001883446618635345\tValidation Loss: 0.00018907454259996738\n",
      "Epoch 1618  \tTraining Loss: 0.00018829624703768793\tValidation Loss: 0.00018902714610175025\n",
      "Epoch 1619  \tTraining Loss: 0.00018824788777322253\tValidation Loss: 0.00018897984054752433\n",
      "Epoch 1620  \tTraining Loss: 0.0001881995815502454\tValidation Loss: 0.00018893261452035181\n",
      "Epoch 1621  \tTraining Loss: 0.0001881513282825333\tValidation Loss: 0.0001888854246569457\n",
      "Epoch 1622  \tTraining Loss: 0.00018810312975300314\tValidation Loss: 0.00018883828255457691\n",
      "Epoch 1623  \tTraining Loss: 0.0001880549857823807\tValidation Loss: 0.00018879119592871884\n",
      "Epoch 1624  \tTraining Loss: 0.00018800689496771536\tValidation Loss: 0.0001887441514092867\n",
      "Epoch 1625  \tTraining Loss: 0.00018795885664332442\tValidation Loss: 0.00018869713977180672\n",
      "Epoch 1626  \tTraining Loss: 0.000187910872611913\tValidation Loss: 0.0001886501774066134\n",
      "Epoch 1627  \tTraining Loss: 0.0001878629426032734\tValidation Loss: 0.00018860327064540363\n",
      "Epoch 1628  \tTraining Loss: 0.00018781506511703606\tValidation Loss: 0.00018855638768742604\n",
      "Epoch 1629  \tTraining Loss: 0.00018776724191103372\tValidation Loss: 0.00018850959528655305\n",
      "Epoch 1630  \tTraining Loss: 0.000187719471910643\tValidation Loss: 0.00018846286681105192\n",
      "Epoch 1631  \tTraining Loss: 0.00018767175576076355\tValidation Loss: 0.00018841618385020145\n",
      "Epoch 1632  \tTraining Loss: 0.00018762409333796028\tValidation Loss: 0.00018836955618113449\n",
      "Epoch 1633  \tTraining Loss: 0.00018757648455867677\tValidation Loss: 0.00018832298249552498\n",
      "Epoch 1634  \tTraining Loss: 0.00018752892912309512\tValidation Loss: 0.00018827646411505314\n",
      "Epoch 1635  \tTraining Loss: 0.00018748142706987087\tValidation Loss: 0.00018822999714308077\n",
      "Epoch 1636  \tTraining Loss: 0.00018743397838473253\tValidation Loss: 0.0001881835821018514\n",
      "Epoch 1637  \tTraining Loss: 0.00018738658432183682\tValidation Loss: 0.00018813723052674567\n",
      "Epoch 1638  \tTraining Loss: 0.00018733924522967366\tValidation Loss: 0.00018809092574374037\n",
      "Epoch 1639  \tTraining Loss: 0.00018729195912097036\tValidation Loss: 0.0001880446704162347\n",
      "Epoch 1640  \tTraining Loss: 0.00018724472541248446\tValidation Loss: 0.00018799846953798476\n",
      "Epoch 1641  \tTraining Loss: 0.00018719754217925814\tValidation Loss: 0.00018795232752720422\n",
      "Epoch 1642  \tTraining Loss: 0.00018715040844789318\tValidation Loss: 0.00018790623152284926\n",
      "Epoch 1643  \tTraining Loss: 0.00018710332734110492\tValidation Loss: 0.00018786018372821397\n",
      "Epoch 1644  \tTraining Loss: 0.00018705629865318696\tValidation Loss: 0.00018781418694912582\n",
      "Epoch 1645  \tTraining Loss: 0.00018700932226252165\tValidation Loss: 0.000187768244658718\n",
      "Epoch 1646  \tTraining Loss: 0.00018696239814121668\tValidation Loss: 0.00018772235529313438\n",
      "Epoch 1647  \tTraining Loss: 0.00018691552621385765\tValidation Loss: 0.00018767651492793106\n",
      "Epoch 1648  \tTraining Loss: 0.00018686870634306574\tValidation Loss: 0.00018763072603425086\n",
      "Epoch 1649  \tTraining Loss: 0.0001868219383613843\tValidation Loss: 0.0001875849914614824\n",
      "Epoch 1650  \tTraining Loss: 0.00018677522225510828\tValidation Loss: 0.0001875393095240511\n",
      "Epoch 1651  \tTraining Loss: 0.00018672855475173272\tValidation Loss: 0.00018749368581901578\n",
      "Epoch 1652  \tTraining Loss: 0.00018668193003028137\tValidation Loss: 0.00018744809304921606\n",
      "Epoch 1653  \tTraining Loss: 0.00018663535532366369\tValidation Loss: 0.00018740254211038678\n",
      "Epoch 1654  \tTraining Loss: 0.00018658883207457781\tValidation Loss: 0.00018735704101609642\n",
      "Epoch 1655  \tTraining Loss: 0.0001865423602311836\tValidation Loss: 0.0001873115899863818\n",
      "Epoch 1656  \tTraining Loss: 0.00018649593967928706\tValidation Loss: 0.0001872661889702469\n",
      "Epoch 1657  \tTraining Loss: 0.00018644957032179884\tValidation Loss: 0.00018722083828295405\n",
      "Epoch 1658  \tTraining Loss: 0.00018640325206363014\tValidation Loss: 0.00018717553812707384\n",
      "Epoch 1659  \tTraining Loss: 0.0001863569852529298\tValidation Loss: 0.00018713028571678547\n",
      "Epoch 1660  \tTraining Loss: 0.00018631077097476352\tValidation Loss: 0.00018708508506936377\n",
      "Epoch 1661  \tTraining Loss: 0.00018626460777862873\tValidation Loss: 0.0001870399345280693\n",
      "Epoch 1662  \tTraining Loss: 0.00018621849544439087\tValidation Loss: 0.00018699483922273002\n",
      "Epoch 1663  \tTraining Loss: 0.00018617243398662\tValidation Loss: 0.0001869497946017406\n",
      "Epoch 1664  \tTraining Loss: 0.00018612642307489646\tValidation Loss: 0.0001869047980124622\n",
      "Epoch 1665  \tTraining Loss: 0.00018608046261312322\tValidation Loss: 0.00018685983197330635\n",
      "Epoch 1666  \tTraining Loss: 0.00018603455250957738\tValidation Loss: 0.0001868149157848548\n",
      "Epoch 1667  \tTraining Loss: 0.00018598869267330594\tValidation Loss: 0.00018677004947845828\n",
      "Epoch 1668  \tTraining Loss: 0.00018594288301382978\tValidation Loss: 0.00018672523305164916\n",
      "Epoch 1669  \tTraining Loss: 0.00018589712976873355\tValidation Loss: 0.00018668031002802343\n",
      "Epoch 1670  \tTraining Loss: 0.00018585143819393796\tValidation Loss: 0.00018663556104095625\n",
      "Epoch 1671  \tTraining Loss: 0.0001858057970284043\tValidation Loss: 0.00018659088659115475\n",
      "Epoch 1672  \tTraining Loss: 0.00018576020459569638\tValidation Loss: 0.0001865462773956492\n",
      "Epoch 1673  \tTraining Loss: 0.0001857146574150862\tValidation Loss: 0.00018650171504094165\n",
      "Epoch 1674  \tTraining Loss: 0.00018566915990486955\tValidation Loss: 0.00018645720338482524\n",
      "Epoch 1675  \tTraining Loss: 0.00018562371197112406\tValidation Loss: 0.0001864127420038765\n",
      "Epoch 1676  \tTraining Loss: 0.00018557831352416262\tValidation Loss: 0.00018636833056512523\n",
      "Epoch 1677  \tTraining Loss: 0.00018553296447545658\tValidation Loss: 0.00018632396880465272\n",
      "Epoch 1678  \tTraining Loss: 0.00018548766474818355\tValidation Loss: 0.00018627965269778788\n",
      "Epoch 1679  \tTraining Loss: 0.00018544241424965798\tValidation Loss: 0.00018623538486861382\n",
      "Epoch 1680  \tTraining Loss: 0.00018539721284979005\tValidation Loss: 0.00018619116614715178\n",
      "Epoch 1681  \tTraining Loss: 0.0001853520605145122\tValidation Loss: 0.0001861469973221049\n",
      "Epoch 1682  \tTraining Loss: 0.00018530695714431638\tValidation Loss: 0.00018610287737226276\n",
      "Epoch 1683  \tTraining Loss: 0.00018526190265249524\tValidation Loss: 0.0001860588060872537\n",
      "Epoch 1684  \tTraining Loss: 0.00018521689695372918\tValidation Loss: 0.00018601478339905987\n",
      "Epoch 1685  \tTraining Loss: 0.00018517193923663625\tValidation Loss: 0.0001859708128316426\n",
      "Epoch 1686  \tTraining Loss: 0.00018512702817687285\tValidation Loss: 0.0001859268866046739\n",
      "Epoch 1687  \tTraining Loss: 0.0001850821656740062\tValidation Loss: 0.0001858830083367655\n",
      "Epoch 1688  \tTraining Loss: 0.00018503735164316612\tValidation Loss: 0.0001858391783000947\n",
      "Epoch 1689  \tTraining Loss: 0.000184992586000834\tValidation Loss: 0.0001857953965338803\n",
      "Epoch 1690  \tTraining Loss: 0.00018494786866399238\tValidation Loss: 0.00018575166422937469\n",
      "Epoch 1691  \tTraining Loss: 0.00018490319758498552\tValidation Loss: 0.0001857080117523962\n",
      "Epoch 1692  \tTraining Loss: 0.00018485857026625\tValidation Loss: 0.00018566438918108888\n",
      "Epoch 1693  \tTraining Loss: 0.00018481399110045004\tValidation Loss: 0.0001856208126616298\n",
      "Epoch 1694  \tTraining Loss: 0.00018476945998363033\tValidation Loss: 0.00018557728347229441\n",
      "Epoch 1695  \tTraining Loss: 0.0001847249768313422\tValidation Loss: 0.0001855338019709022\n",
      "Epoch 1696  \tTraining Loss: 0.00018468054276050692\tValidation Loss: 0.0001854903558142145\n",
      "Epoch 1697  \tTraining Loss: 0.0001846361577460942\tValidation Loss: 0.00018544697412166988\n",
      "Epoch 1698  \tTraining Loss: 0.00018459182060498797\tValidation Loss: 0.00018540364395033823\n",
      "Epoch 1699  \tTraining Loss: 0.00018454753109812245\tValidation Loss: 0.00018536037217176219\n",
      "Epoch 1700  \tTraining Loss: 0.0001845032891368715\tValidation Loss: 0.00018531714951175465\n",
      "Epoch 1701  \tTraining Loss: 0.0001844590946404701\tValidation Loss: 0.00018527397484844149\n",
      "Epoch 1702  \tTraining Loss: 0.0001844149475294896\tValidation Loss: 0.00018523084775422152\n",
      "Epoch 1703  \tTraining Loss: 0.00018437084772512341\tValidation Loss: 0.00018518776797465039\n",
      "Epoch 1704  \tTraining Loss: 0.00018432679514901497\tValidation Loss: 0.00018514473534627428\n",
      "Epoch 1705  \tTraining Loss: 0.00018428278972319308\tValidation Loss: 0.0001851017497516856\n",
      "Epoch 1706  \tTraining Loss: 0.00018423883137004223\tValidation Loss: 0.0001850588110963073\n",
      "Epoch 1707  \tTraining Loss: 0.00018419492001228683\tValidation Loss: 0.00018501591929674654\n",
      "Epoch 1708  \tTraining Loss: 0.0001841510555729824\tValidation Loss: 0.00018497307251732977\n",
      "Epoch 1709  \tTraining Loss: 0.00018410723799078266\tValidation Loss: 0.00018493024918158908\n",
      "Epoch 1710  \tTraining Loss: 0.0001840634676999237\tValidation Loss: 0.00018488748336353185\n",
      "Epoch 1711  \tTraining Loss: 0.0001840197440822943\tValidation Loss: 0.00018484476508057765\n",
      "Epoch 1712  \tTraining Loss: 0.00018397606705399086\tValidation Loss: 0.00018480209349487067\n",
      "Epoch 1713  \tTraining Loss: 0.00018393243653881916\tValidation Loss: 0.00018475946829474638\n",
      "Epoch 1714  \tTraining Loss: 0.0001838888535682433\tValidation Loss: 0.00018471688691829694\n",
      "Epoch 1715  \tTraining Loss: 0.00018384531864971824\tValidation Loss: 0.0001846743569252023\n",
      "Epoch 1716  \tTraining Loss: 0.00018380183002682257\tValidation Loss: 0.00018463187415302006\n",
      "Epoch 1717  \tTraining Loss: 0.00018375838760894034\tValidation Loss: 0.00018458943812825325\n",
      "Epoch 1718  \tTraining Loss: 0.0001837149913215118\tValidation Loss: 0.00018454704846205183\n",
      "Epoch 1719  \tTraining Loss: 0.00018367164109103768\tValidation Loss: 0.00018450470489064208\n",
      "Epoch 1720  \tTraining Loss: 0.00018362833659011938\tValidation Loss: 0.00018446240897969599\n",
      "Epoch 1721  \tTraining Loss: 0.0001835850777169011\tValidation Loss: 0.00018442015758913356\n",
      "Epoch 1722  \tTraining Loss: 0.0001835418646825807\tValidation Loss: 0.00018437795178533222\n",
      "Epoch 1723  \tTraining Loss: 0.00018349869741481937\tValidation Loss: 0.00018433579607207544\n",
      "Epoch 1724  \tTraining Loss: 0.00018345558451799714\tValidation Loss: 0.0001842936825542569\n",
      "Epoch 1725  \tTraining Loss: 0.00018341253296622363\tValidation Loss: 0.0001842516081028677\n",
      "Epoch 1726  \tTraining Loss: 0.00018336952729840848\tValidation Loss: 0.00018420957846555455\n",
      "Epoch 1727  \tTraining Loss: 0.00018332656713309264\tValidation Loss: 0.0001841675965455947\n",
      "Epoch 1728  \tTraining Loss: 0.0001832836523661002\tValidation Loss: 0.00018412566208872986\n",
      "Epoch 1729  \tTraining Loss: 0.0001832407829205325\tValidation Loss: 0.00018408377445954508\n",
      "Epoch 1730  \tTraining Loss: 0.00018319795872403713\tValidation Loss: 0.00018404193311813533\n",
      "Epoch 1731  \tTraining Loss: 0.00018315517970560134\tValidation Loss: 0.00018400013766230593\n",
      "Epoch 1732  \tTraining Loss: 0.00018311244579486769\tValidation Loss: 0.00018395838779566197\n",
      "Epoch 1733  \tTraining Loss: 0.00018306975692192635\tValidation Loss: 0.00018391668329385258\n",
      "Epoch 1734  \tTraining Loss: 0.00018302711301723342\tValidation Loss: 0.00018387502398021366\n",
      "Epoch 1735  \tTraining Loss: 0.0001829845140115724\tValidation Loss: 0.00018383340970973643\n",
      "Epoch 1736  \tTraining Loss: 0.00018294196050246066\tValidation Loss: 0.00018379183280473055\n",
      "Epoch 1737  \tTraining Loss: 0.00018289945430822105\tValidation Loss: 0.00018375030733880772\n",
      "Epoch 1738  \tTraining Loss: 0.00018285699279100393\tValidation Loss: 0.0001837088272577528\n",
      "Epoch 1739  \tTraining Loss: 0.0001828145749392172\tValidation Loss: 0.00018366739462401612\n",
      "Epoch 1740  \tTraining Loss: 0.00018277220058111253\tValidation Loss: 0.00018362600353440784\n",
      "Epoch 1741  \tTraining Loss: 0.00018272987069429479\tValidation Loss: 0.00018358465655758278\n",
      "Epoch 1742  \tTraining Loss: 0.00018268758521109723\tValidation Loss: 0.00018354335386113382\n",
      "Epoch 1743  \tTraining Loss: 0.00018264534565500364\tValidation Loss: 0.00018350219975334564\n",
      "Epoch 1744  \tTraining Loss: 0.00018260315288736448\tValidation Loss: 0.00018346101822532107\n",
      "Epoch 1745  \tTraining Loss: 0.00018256100241389176\tValidation Loss: 0.00018341986165514834\n",
      "Epoch 1746  \tTraining Loss: 0.00018251889612307595\tValidation Loss: 0.00018337875922430535\n",
      "Epoch 1747  \tTraining Loss: 0.00018247683398234433\tValidation Loss: 0.0001833376747882257\n",
      "Epoch 1748  \tTraining Loss: 0.00018243481581098833\tValidation Loss: 0.00018329664317049118\n",
      "Epoch 1749  \tTraining Loss: 0.00018239284163800457\tValidation Loss: 0.00018325565152836647\n",
      "Epoch 1750  \tTraining Loss: 0.00018235091029941995\tValidation Loss: 0.00018321468363966488\n",
      "Epoch 1751  \tTraining Loss: 0.0001823090183089489\tValidation Loss: 0.00018317377583166247\n",
      "Epoch 1752  \tTraining Loss: 0.0001822671700040259\tValidation Loss: 0.00018313293264809273\n",
      "Epoch 1753  \tTraining Loss: 0.00018222536282405265\tValidation Loss: 0.00018309210357179875\n",
      "Epoch 1754  \tTraining Loss: 0.0001821836012129166\tValidation Loss: 0.00018305131875224657\n",
      "Epoch 1755  \tTraining Loss: 0.00018214188468818513\tValidation Loss: 0.0001830105828904937\n",
      "Epoch 1756  \tTraining Loss: 0.00018210021178725243\tValidation Loss: 0.00018296989647517667\n",
      "Epoch 1757  \tTraining Loss: 0.00018205858233358654\tValidation Loss: 0.00018292925627729086\n",
      "Epoch 1758  \tTraining Loss: 0.00018201699625597148\tValidation Loss: 0.00018288866112307854\n",
      "Epoch 1759  \tTraining Loss: 0.00018197545348923685\tValidation Loss: 0.0001828481103383116\n",
      "Epoch 1760  \tTraining Loss: 0.00018193395396937716\tValidation Loss: 0.0001828076034782017\n",
      "Epoch 1761  \tTraining Loss: 0.00018189249763293514\tValidation Loss: 0.00018276714023818128\n",
      "Epoch 1762  \tTraining Loss: 0.0001818510844168271\tValidation Loss: 0.00018272672002519412\n",
      "Epoch 1763  \tTraining Loss: 0.00018180971425826978\tValidation Loss: 0.00018268634280197056\n",
      "Epoch 1764  \tTraining Loss: 0.0001817683870947441\tValidation Loss: 0.0001826460086660697\n",
      "Epoch 1765  \tTraining Loss: 0.00018172710286397525\tValidation Loss: 0.0001826057175100153\n",
      "Epoch 1766  \tTraining Loss: 0.00018168586150392186\tValidation Loss: 0.00018256546923988519\n",
      "Epoch 1767  \tTraining Loss: 0.0001816446629527685\tValidation Loss: 0.00018252526377067097\n",
      "Epoch 1768  \tTraining Loss: 0.00018160350714892088\tValidation Loss: 0.0001824851010233752\n",
      "Epoch 1769  \tTraining Loss: 0.00018156239403100213\tValidation Loss: 0.0001824449809231539\n",
      "Epoch 1770  \tTraining Loss: 0.00018152132353784937\tValidation Loss: 0.00018240490339810244\n",
      "Epoch 1771  \tTraining Loss: 0.00018148029677117437\tValidation Loss: 0.00018236486734753298\n",
      "Epoch 1772  \tTraining Loss: 0.0001814393159901454\tValidation Loss: 0.00018232487344267952\n",
      "Epoch 1773  \tTraining Loss: 0.00018139837767747796\tValidation Loss: 0.00018228492382338046\n",
      "Epoch 1774  \tTraining Loss: 0.00018135748175034872\tValidation Loss: 0.00018224501714366468\n",
      "Epoch 1775  \tTraining Loss: 0.00018131662814742199\tValidation Loss: 0.00018220515289851135\n",
      "Epoch 1776  \tTraining Loss: 0.00018127581680854935\tValidation Loss: 0.00018216533088801054\n",
      "Epoch 1777  \tTraining Loss: 0.00018123505276422387\tValidation Loss: 0.00018212563738382058\n",
      "Epoch 1778  \tTraining Loss: 0.0001811943517599032\tValidation Loss: 0.00018208594624448307\n",
      "Epoch 1779  \tTraining Loss: 0.00018115368682359255\tValidation Loss: 0.00018204628164146213\n",
      "Epoch 1780  \tTraining Loss: 0.00018111306423467774\tValidation Loss: 0.00018200665705203972\n",
      "Epoch 1781  \tTraining Loss: 0.00018107248386774888\tValidation Loss: 0.00018196707416712137\n",
      "Epoch 1782  \tTraining Loss: 0.00018103194565143914\tValidation Loss: 0.00018192753332703607\n",
      "Epoch 1783  \tTraining Loss: 0.00018099144952338835\tValidation Loss: 0.00018188803566949148\n",
      "Epoch 1784  \tTraining Loss: 0.0001809509954253929\tValidation Loss: 0.00018184858009288592\n",
      "Epoch 1785  \tTraining Loss: 0.0001809105835143114\tValidation Loss: 0.00018180916628151154\n",
      "Epoch 1786  \tTraining Loss: 0.00018087021350605945\tValidation Loss: 0.00018176979406421765\n",
      "Epoch 1787  \tTraining Loss: 0.0001808298853420985\tValidation Loss: 0.00018173046338741216\n",
      "Epoch 1788  \tTraining Loss: 0.00018078959896412598\tValidation Loss: 0.0001816911742008679\n",
      "Epoch 1789  \tTraining Loss: 0.00018074935431404852\tValidation Loss: 0.0001816519264561991\n",
      "Epoch 1790  \tTraining Loss: 0.00018070915133396704\tValidation Loss: 0.00018161272010521024\n",
      "Epoch 1791  \tTraining Loss: 0.00018066899065968827\tValidation Loss: 0.0001815735406853759\n",
      "Epoch 1792  \tTraining Loss: 0.00018062887255670604\tValidation Loss: 0.00018153441176392194\n",
      "Epoch 1793  \tTraining Loss: 0.00018058879593290268\tValidation Loss: 0.00018149532489946613\n",
      "Epoch 1794  \tTraining Loss: 0.00018054876072505992\tValidation Loss: 0.0001814562794253634\n",
      "Epoch 1795  \tTraining Loss: 0.00018050876687556732\tValidation Loss: 0.00018141727510387999\n",
      "Epoch 1796  \tTraining Loss: 0.00018046881432757885\tValidation Loss: 0.00018137831184674352\n",
      "Epoch 1797  \tTraining Loss: 0.00018042890302451595\tValidation Loss: 0.00018133938960693074\n",
      "Epoch 1798  \tTraining Loss: 0.00018038903372018185\tValidation Loss: 0.00018130050390341765\n",
      "Epoch 1799  \tTraining Loss: 0.000180349207724982\tValidation Loss: 0.00018126166366889585\n",
      "Epoch 1800  \tTraining Loss: 0.00018030941988587444\tValidation Loss: 0.00018122287126482378\n",
      "Epoch 1801  \tTraining Loss: 0.00018026966961521212\tValidation Loss: 0.00018118411291287464\n",
      "Epoch 1802  \tTraining Loss: 0.00018022996032735292\tValidation Loss: 0.00018114539464943766\n",
      "Epoch 1803  \tTraining Loss: 0.00018019029196375704\tValidation Loss: 0.0001811067169000058\n",
      "Epoch 1804  \tTraining Loss: 0.00018015066446858423\tValidation Loss: 0.0001810680797665579\n",
      "Epoch 1805  \tTraining Loss: 0.00018011107778642294\tValidation Loss: 0.00018102948323176477\n",
      "Epoch 1806  \tTraining Loss: 0.00018007153186206447\tValidation Loss: 0.00018099092724142648\n",
      "Epoch 1807  \tTraining Loss: 0.00018003202664046322\tValidation Loss: 0.0001809524117339654\n",
      "Epoch 1808  \tTraining Loss: 0.00017999256206672722\tValidation Loss: 0.00018091393664875795\n",
      "Epoch 1809  \tTraining Loss: 0.00017995313808611413\tValidation Loss: 0.00018087550192747825\n",
      "Epoch 1810  \tTraining Loss: 0.0001799137600140075\tValidation Loss: 0.00018083707862152245\n",
      "Epoch 1811  \tTraining Loss: 0.0001798744270757962\tValidation Loss: 0.0001807987155131575\n",
      "Epoch 1812  \tTraining Loss: 0.00017983513472624678\tValidation Loss: 0.00018076040832280585\n",
      "Epoch 1813  \tTraining Loss: 0.00017979588274340133\tValidation Loss: 0.00018072214647670327\n",
      "Epoch 1814  \tTraining Loss: 0.00017975667106177046\tValidation Loss: 0.00018068392652991576\n",
      "Epoch 1815  \tTraining Loss: 0.00017971749962522666\tValidation Loss: 0.00018064574727632977\n",
      "Epoch 1816  \tTraining Loss: 0.00017967836837903077\tValidation Loss: 0.0001806076082565541\n",
      "Epoch 1817  \tTraining Loss: 0.00017963927726890629\tValidation Loss: 0.00018056950926964013\n",
      "Epoch 1818  \tTraining Loss: 0.00017960022624084821\tValidation Loss: 0.00018053145020682017\n",
      "Epoch 1819  \tTraining Loss: 0.00017956121524106107\tValidation Loss: 0.00018049343099270975\n",
      "Epoch 1820  \tTraining Loss: 0.00017952224296857575\tValidation Loss: 0.00018045546341901247\n",
      "Epoch 1821  \tTraining Loss: 0.00017948330755176696\tValidation Loss: 0.00018041752596763342\n",
      "Epoch 1822  \tTraining Loss: 0.00017944441202193947\tValidation Loss: 0.00018037962732503427\n",
      "Epoch 1823  \tTraining Loss: 0.0001794055563189181\tValidation Loss: 0.00018034176807313465\n",
      "Epoch 1824  \tTraining Loss: 0.0001793667403888322\tValidation Loss: 0.00018030394834544163\n",
      "Epoch 1825  \tTraining Loss: 0.0001793279682565096\tValidation Loss: 0.00018026615216769557\n",
      "Epoch 1826  \tTraining Loss: 0.0001792892403574935\tValidation Loss: 0.00018022840680106223\n",
      "Epoch 1827  \tTraining Loss: 0.00017925055207003115\tValidation Loss: 0.00018019070060952257\n",
      "Epoch 1828  \tTraining Loss: 0.00017921190332791475\tValidation Loss: 0.0001801530340204002\n",
      "Epoch 1829  \tTraining Loss: 0.0001791732940773588\tValidation Loss: 0.00018011540666724925\n",
      "Epoch 1830  \tTraining Loss: 0.00017913472426603908\tValidation Loss: 0.000180077818435826\n",
      "Epoch 1831  \tTraining Loss: 0.0001790961938419858\tValidation Loss: 0.0001800402692830647\n",
      "Epoch 1832  \tTraining Loss: 0.00017905770275340605\tValidation Loss: 0.00018000275917588594\n",
      "Epoch 1833  \tTraining Loss: 0.00017901925143538633\tValidation Loss: 0.00017996523337916097\n",
      "Epoch 1834  \tTraining Loss: 0.00017898084031202078\tValidation Loss: 0.00017992778720107164\n",
      "Epoch 1835  \tTraining Loss: 0.00017894246840274673\tValidation Loss: 0.00017989038937499586\n",
      "Epoch 1836  \tTraining Loss: 0.00017890413562842537\tValidation Loss: 0.00017985303445541384\n",
      "Epoch 1837  \tTraining Loss: 0.0001788658419356663\tValidation Loss: 0.00017981572003654783\n",
      "Epoch 1838  \tTraining Loss: 0.00017882758727303953\tValidation Loss: 0.0001797784448046449\n",
      "Epoch 1839  \tTraining Loss: 0.00017878937158951583\tValidation Loss: 0.00017974120485470096\n",
      "Epoch 1840  \tTraining Loss: 0.0001787511948342434\tValidation Loss: 0.00017970400367695524\n",
      "Epoch 1841  \tTraining Loss: 0.00017871305695650128\tValidation Loss: 0.00017966684114394013\n",
      "Epoch 1842  \tTraining Loss: 0.00017867495790568671\tValidation Loss: 0.00017962971717097205\n",
      "Epoch 1843  \tTraining Loss: 0.00017863689763130986\tValidation Loss: 0.00017959262345141282\n",
      "Epoch 1844  \tTraining Loss: 0.0001785988758149449\tValidation Loss: 0.00017955556124464898\n",
      "Epoch 1845  \tTraining Loss: 0.00017856089115691874\tValidation Loss: 0.0001795185293332234\n",
      "Epoch 1846  \tTraining Loss: 0.00017852294515333979\tValidation Loss: 0.0001794815345668984\n",
      "Epoch 1847  \tTraining Loss: 0.00017848503771854016\tValidation Loss: 0.00017944457721214252\n",
      "Epoch 1848  \tTraining Loss: 0.0001784471688006934\tValidation Loss: 0.00017940765758721897\n",
      "Epoch 1849  \tTraining Loss: 0.0001784093383496863\tValidation Loss: 0.0001793707758816574\n",
      "Epoch 1850  \tTraining Loss: 0.00017837154631572018\tValidation Loss: 0.00017933393217548098\n",
      "Epoch 1851  \tTraining Loss: 0.0001783337926491553\tValidation Loss: 0.00017929712648368973\n",
      "Epoch 1852  \tTraining Loss: 0.00017829607732753214\tValidation Loss: 0.00017926036113043932\n",
      "Epoch 1853  \tTraining Loss: 0.00017825840047922174\tValidation Loss: 0.00017922363168556562\n",
      "Epoch 1854  \tTraining Loss: 0.00017822076183915037\tValidation Loss: 0.00017918694005753498\n",
      "Epoch 1855  \tTraining Loss: 0.00017818316135799974\tValidation Loss: 0.00017915028622953103\n",
      "Epoch 1856  \tTraining Loss: 0.0001781455987129871\tValidation Loss: 0.0001791136713306755\n",
      "Epoch 1857  \tTraining Loss: 0.00017810807367056909\tValidation Loss: 0.00017907709273721017\n",
      "Epoch 1858  \tTraining Loss: 0.0001780705866412582\tValidation Loss: 0.0001790405517435406\n",
      "Epoch 1859  \tTraining Loss: 0.0001780331375760679\tValidation Loss: 0.00017900404602039344\n",
      "Epoch 1860  \tTraining Loss: 0.0001779957210298512\tValidation Loss: 0.00017896734756776324\n",
      "Epoch 1861  \tTraining Loss: 0.00017795833137666556\tValidation Loss: 0.00017893081000836203\n",
      "Epoch 1862  \tTraining Loss: 0.00017792098003249898\tValidation Loss: 0.00017889434463739688\n",
      "Epoch 1863  \tTraining Loss: 0.00017788366654670416\tValidation Loss: 0.00017885792846471373\n",
      "Epoch 1864  \tTraining Loss: 0.00017784639084804472\tValidation Loss: 0.00017882155483636195\n",
      "Epoch 1865  \tTraining Loss: 0.0001778091528828459\tValidation Loss: 0.0001787852274002153\n",
      "Epoch 1866  \tTraining Loss: 0.00017777195260013976\tValidation Loss: 0.0001787489394948323\n",
      "Epoch 1867  \tTraining Loss: 0.0001777347899498517\tValidation Loss: 0.00017871269003394037\n",
      "Epoch 1868  \tTraining Loss: 0.00017769766460286684\tValidation Loss: 0.0001786764901048125\n",
      "Epoch 1869  \tTraining Loss: 0.00017766057302271175\tValidation Loss: 0.0001786403199688597\n",
      "Epoch 1870  \tTraining Loss: 0.0001776235189357769\tValidation Loss: 0.00017860418700959055\n",
      "Epoch 1871  \tTraining Loss: 0.00017758650228741907\tValidation Loss: 0.00017856809160725297\n",
      "Epoch 1872  \tTraining Loss: 0.00017754952302808012\tValidation Loss: 0.00017853203380890643\n",
      "Epoch 1873  \tTraining Loss: 0.00017751258110890056\tValidation Loss: 0.00017849601355209734\n",
      "Epoch 1874  \tTraining Loss: 0.00017747567648125786\tValidation Loss: 0.0001784600307526475\n",
      "Epoch 1875  \tTraining Loss: 0.00017743880909668427\tValidation Loss: 0.00017842408533073821\n",
      "Epoch 1876  \tTraining Loss: 0.00017740197890684566\tValidation Loss: 0.00017838817721487377\n",
      "Epoch 1877  \tTraining Loss: 0.00017736518586353197\tValidation Loss: 0.00017835230634005955\n",
      "Epoch 1878  \tTraining Loss: 0.00017732842991865223\tValidation Loss: 0.00017831647709459477\n",
      "Epoch 1879  \tTraining Loss: 0.0001772917110242297\tValidation Loss: 0.0001782806865167102\n",
      "Epoch 1880  \tTraining Loss: 0.00017725502913239837\tValidation Loss: 0.00017824493294451074\n",
      "Epoch 1881  \tTraining Loss: 0.00017721838419539968\tValidation Loss: 0.00017820921632196444\n",
      "Epoch 1882  \tTraining Loss: 0.00017718177616557934\tValidation Loss: 0.00017817353659361544\n",
      "Epoch 1883  \tTraining Loss: 0.0001771452049953845\tValidation Loss: 0.00017813789370443026\n",
      "Epoch 1884  \tTraining Loss: 0.00017710867063736087\tValidation Loss: 0.00017810228759972016\n",
      "Epoch 1885  \tTraining Loss: 0.00017707216924716144\tValidation Loss: 0.0001780667431573651\n",
      "Epoch 1886  \tTraining Loss: 0.00017703568459367851\tValidation Loss: 0.0001780312234563525\n",
      "Epoch 1887  \tTraining Loss: 0.00017699923776148834\tValidation Loss: 0.00017799574068407982\n",
      "Epoch 1888  \tTraining Loss: 0.0001769628271581752\tValidation Loss: 0.00017796028763602073\n",
      "Epoch 1889  \tTraining Loss: 0.00017692645322221022\tValidation Loss: 0.0001779248658888639\n",
      "Epoch 1890  \tTraining Loss: 0.0001768901158887097\tValidation Loss: 0.00017788947555381595\n",
      "Epoch 1891  \tTraining Loss: 0.0001768538151026556\tValidation Loss: 0.00017785410712022805\n",
      "Epoch 1892  \tTraining Loss: 0.0001768175508126706\tValidation Loss: 0.00017781877275877326\n",
      "Epoch 1893  \tTraining Loss: 0.00017678132296913333\tValidation Loss: 0.00017778347302495445\n",
      "Epoch 1894  \tTraining Loss: 0.00017674513152336538\tValidation Loss: 0.0001777482089101904\n",
      "Epoch 1895  \tTraining Loss: 0.00017670897642723716\tValidation Loss: 0.00017771298055930252\n",
      "Epoch 1896  \tTraining Loss: 0.0001766728576329662\tValidation Loss: 0.00017767778805166686\n",
      "Epoch 1897  \tTraining Loss: 0.00017663677509301192\tValidation Loss: 0.0001776426314247143\n",
      "Epoch 1898  \tTraining Loss: 0.00017660072876001854\tValidation Loss: 0.00017760751068849348\n",
      "Epoch 1899  \tTraining Loss: 0.0001765647185867831\tValidation Loss: 0.00017757242583489596\n",
      "Epoch 1900  \tTraining Loss: 0.00017652874452623666\tValidation Loss: 0.00017753736151667943\n",
      "Epoch 1901  \tTraining Loss: 0.00017649280653143248\tValidation Loss: 0.0001775023314593702\n",
      "Epoch 1902  \tTraining Loss: 0.00017645690430785106\tValidation Loss: 0.0001774673427511132\n",
      "Epoch 1903  \tTraining Loss: 0.00017642103233696412\tValidation Loss: 0.00017743238081517209\n",
      "Epoch 1904  \tTraining Loss: 0.00017638519629060545\tValidation Loss: 0.00017739745397462765\n",
      "Epoch 1905  \tTraining Loss: 0.00017634939611488754\tValidation Loss: 0.00017736256273105254\n",
      "Epoch 1906  \tTraining Loss: 0.00017631363176248132\tValidation Loss: 0.0001773277071901831\n",
      "Epoch 1907  \tTraining Loss: 0.0001762779031868752\tValidation Loss: 0.0001772928873178311\n",
      "Epoch 1908  \tTraining Loss: 0.0001762422103417826\tValidation Loss: 0.00017725810304776158\n",
      "Epoch 1909  \tTraining Loss: 0.00017620655318104005\tValidation Loss: 0.0001772233543149525\n",
      "Epoch 1910  \tTraining Loss: 0.00017617093165858146\tValidation Loss: 0.00017718864106162829\n",
      "Epoch 1911  \tTraining Loss: 0.00017613534285438595\tValidation Loss: 0.00017715395737479087\n",
      "Epoch 1912  \tTraining Loss: 0.00017609978291644257\tValidation Loss: 0.0001771193015308427\n",
      "Epoch 1913  \tTraining Loss: 0.00017606425855102307\tValidation Loss: 0.000177084687513638\n",
      "Epoch 1914  \tTraining Loss: 0.00017602876964890884\tValidation Loss: 0.00017705011144357179\n",
      "Epoch 1915  \tTraining Loss: 0.0001759933161607918\tValidation Loss: 0.00017701557179633675\n",
      "Epoch 1916  \tTraining Loss: 0.0001759578980404316\tValidation Loss: 0.0001769810679003105\n",
      "Epoch 1917  \tTraining Loss: 0.00017592251524202782\tValidation Loss: 0.0001769465994325429\n",
      "Epoch 1918  \tTraining Loss: 0.0001758871677199345\tValidation Loss: 0.00017691216622009223\n",
      "Epoch 1919  \tTraining Loss: 0.00017585185542860358\tValidation Loss: 0.00017687776815684521\n",
      "Epoch 1920  \tTraining Loss: 0.00017581657832256825\tValidation Loss: 0.00017684340516729003\n",
      "Epoch 1921  \tTraining Loss: 0.0001757813363564363\tValidation Loss: 0.00017680907719028356\n",
      "Epoch 1922  \tTraining Loss: 0.00017574612840550325\tValidation Loss: 0.00017677479109471557\n",
      "Epoch 1923  \tTraining Loss: 0.0001757109430406888\tValidation Loss: 0.00017674052657890073\n",
      "Epoch 1924  \tTraining Loss: 0.00017567579368225328\tValidation Loss: 0.0001767062858824501\n",
      "Epoch 1925  \tTraining Loss: 0.00017564068516882583\tValidation Loss: 0.0001766720931983727\n",
      "Epoch 1926  \tTraining Loss: 0.00017560561160697086\tValidation Loss: 0.0001766379362796363\n",
      "Epoch 1927  \tTraining Loss: 0.00017557057293834873\tValidation Loss: 0.0001766038143626355\n",
      "Epoch 1928  \tTraining Loss: 0.0001755355691165263\tValidation Loss: 0.00017656972717075832\n",
      "Epoch 1929  \tTraining Loss: 0.00017550060009622888\tValidation Loss: 0.00017653567461839691\n",
      "Epoch 1930  \tTraining Loss: 0.0001754656658324045\tValidation Loss: 0.0001765016566703312\n",
      "Epoch 1931  \tTraining Loss: 0.00017543076726096016\tValidation Loss: 0.00017646766964854902\n",
      "Epoch 1932  \tTraining Loss: 0.00017539590584975107\tValidation Loss: 0.0001764337220192676\n",
      "Epoch 1933  \tTraining Loss: 0.0001753610790432644\tValidation Loss: 0.00017639980916621845\n",
      "Epoch 1934  \tTraining Loss: 0.000175326288569646\tValidation Loss: 0.00017636591038663676\n",
      "Epoch 1935  \tTraining Loss: 0.0001752915374356207\tValidation Loss: 0.00017633206100286155\n",
      "Epoch 1936  \tTraining Loss: 0.00017525682016675316\tValidation Loss: 0.00017629825296619478\n",
      "Epoch 1937  \tTraining Loss: 0.0001752221357086883\tValidation Loss: 0.00017626447428596294\n",
      "Epoch 1938  \tTraining Loss: 0.000175187485611112\tValidation Loss: 0.00017623072933493388\n",
      "Epoch 1939  \tTraining Loss: 0.00017515286982852264\tValidation Loss: 0.00017619701834750383\n",
      "Epoch 1940  \tTraining Loss: 0.0001751182883165405\tValidation Loss: 0.00017616334139806296\n",
      "Epoch 1941  \tTraining Loss: 0.0001750837410309111\tValidation Loss: 0.00017612969848097566\n",
      "Epoch 1942  \tTraining Loss: 0.00017504922792743792\tValidation Loss: 0.00017609608956003313\n",
      "Epoch 1943  \tTraining Loss: 0.0001750147480901425\tValidation Loss: 0.00017606251760368056\n",
      "Epoch 1944  \tTraining Loss: 0.0001749802999423966\tValidation Loss: 0.0001760289755199661\n",
      "Epoch 1945  \tTraining Loss: 0.00017494588585912753\tValidation Loss: 0.000175995466993032\n",
      "Epoch 1946  \tTraining Loss: 0.00017491150579461434\tValidation Loss: 0.00017596199221252796\n",
      "Epoch 1947  \tTraining Loss: 0.0001748771597045469\tValidation Loss: 0.0001759285511940324\n",
      "Epoch 1948  \tTraining Loss: 0.00017484284754480802\tValidation Loss: 0.0001758951438936542\n",
      "Epoch 1949  \tTraining Loss: 0.0001748085694069448\tValidation Loss: 0.00017586176485561502\n",
      "Epoch 1950  \tTraining Loss: 0.00017477432848615708\tValidation Loss: 0.0001758284269654258\n",
      "Epoch 1951  \tTraining Loss: 0.0001747401213617144\tValidation Loss: 0.00017579512313535667\n",
      "Epoch 1952  \tTraining Loss: 0.0001747059470745561\tValidation Loss: 0.0001757618558708135\n",
      "Epoch 1953  \tTraining Loss: 0.00017467180432436983\tValidation Loss: 0.00017572861763074654\n",
      "Epoch 1954  \tTraining Loss: 0.00017463769524809025\tValidation Loss: 0.00017569541242779262\n",
      "Epoch 1955  \tTraining Loss: 0.00017460361980031912\tValidation Loss: 0.0001756622404962158\n",
      "Epoch 1956  \tTraining Loss: 0.00017456957793711861\tValidation Loss: 0.00017562910187894547\n",
      "Epoch 1957  \tTraining Loss: 0.00017453556961472493\tValidation Loss: 0.0001755959965457483\n",
      "Epoch 1958  \tTraining Loss: 0.00017450159478943014\tValidation Loss: 0.00017556292444731667\n",
      "Epoch 1959  \tTraining Loss: 0.00017446765341756366\tValidation Loss: 0.00017552988553278488\n",
      "Epoch 1960  \tTraining Loss: 0.00017443374545548859\tValidation Loss: 0.0001754968797535832\n",
      "Epoch 1961  \tTraining Loss: 0.00017439987085960022\tValidation Loss: 0.00017546390706334986\n",
      "Epoch 1962  \tTraining Loss: 0.0001743660295863251\tValidation Loss: 0.00017543096741715205\n",
      "Epoch 1963  \tTraining Loss: 0.00017433222159212065\tValidation Loss: 0.00017539806077085165\n",
      "Epoch 1964  \tTraining Loss: 0.00017429844683347455\tValidation Loss: 0.0001753651789885261\n",
      "Epoch 1965  \tTraining Loss: 0.00017426470526690416\tValidation Loss: 0.00017533232984208102\n",
      "Epoch 1966  \tTraining Loss: 0.00017423099684895615\tValidation Loss: 0.00017529951355365093\n",
      "Epoch 1967  \tTraining Loss: 0.0001741973215362064\tValidation Loss: 0.00017526673007995717\n",
      "Epoch 1968  \tTraining Loss: 0.00017416367928525892\tValidation Loss: 0.00017523397937773826\n",
      "Epoch 1969  \tTraining Loss: 0.00017413007001628123\tValidation Loss: 0.00017520126176974042\n",
      "Epoch 1970  \tTraining Loss: 0.00017409649364291305\tValidation Loss: 0.00017516857505845364\n",
      "Epoch 1971  \tTraining Loss: 0.00017406295020024366\tValidation Loss: 0.00017513592039918106\n",
      "Epoch 1972  \tTraining Loss: 0.0001740294396446202\tValidation Loss: 0.00017510329825517202\n",
      "Epoch 1973  \tTraining Loss: 0.00017399596193276858\tValidation Loss: 0.00017507070860588918\n",
      "Epoch 1974  \tTraining Loss: 0.00017396251702145757\tValidation Loss: 0.00017503815142617943\n",
      "Epoch 1975  \tTraining Loss: 0.000173929104867484\tValidation Loss: 0.00017500562668323018\n",
      "Epoch 1976  \tTraining Loss: 0.00017389572542767087\tValidation Loss: 0.00017497313433917242\n",
      "Epoch 1977  \tTraining Loss: 0.00017386237865886695\tValidation Loss: 0.00017494067435343144\n",
      "Epoch 1978  \tTraining Loss: 0.0001738290645179464\tValidation Loss: 0.00017490824668410626\n",
      "Epoch 1979  \tTraining Loss: 0.00017379578296180826\tValidation Loss: 0.0001748758512886773\n",
      "Epoch 1980  \tTraining Loss: 0.0001737625339473764\tValidation Loss: 0.00017484348812434904\n",
      "Epoch 1981  \tTraining Loss: 0.00017372931743159925\tValidation Loss: 0.00017481115714821006\n",
      "Epoch 1982  \tTraining Loss: 0.00017369613337144953\tValidation Loss: 0.00017477885831730693\n",
      "Epoch 1983  \tTraining Loss: 0.0001736629817239239\tValidation Loss: 0.00017474659158867793\n",
      "Epoch 1984  \tTraining Loss: 0.00017362986244604304\tValidation Loss: 0.00017471435691936832\n",
      "Epoch 1985  \tTraining Loss: 0.00017359677549485107\tValidation Loss: 0.0001746821550979714\n",
      "Epoch 1986  \tTraining Loss: 0.00017356372082741572\tValidation Loss: 0.0001746499860334063\n",
      "Epoch 1987  \tTraining Loss: 0.00017353069840082768\tValidation Loss: 0.00017461784889295627\n",
      "Epoch 1988  \tTraining Loss: 0.00017349770957263605\tValidation Loss: 0.00017458581706524087\n",
      "Epoch 1989  \tTraining Loss: 0.00017346478617059332\tValidation Loss: 0.00017455376676402947\n",
      "Epoch 1990  \tTraining Loss: 0.00017343189540328574\tValidation Loss: 0.00017452174621907242\n",
      "Epoch 1991  \tTraining Loss: 0.00017339903692408954\tValidation Loss: 0.00017448975758982746\n",
      "Epoch 1992  \tTraining Loss: 0.00017336621015205517\tValidation Loss: 0.00017445780805209677\n",
      "Epoch 1993  \tTraining Loss: 0.0001733334112212099\tValidation Loss: 0.0001744258498928235\n",
      "Epoch 1994  \tTraining Loss: 0.00017330064721905798\tValidation Loss: 0.00017439395197174772\n",
      "Epoch 1995  \tTraining Loss: 0.0001732679153896209\tValidation Loss: 0.0001743620856758933\n",
      "Epoch 1996  \tTraining Loss: 0.00017323521557843503\tValidation Loss: 0.00017433025002927356\n",
      "Epoch 1997  \tTraining Loss: 0.00017320254777739518\tValidation Loss: 0.00017429848318902897\n",
      "Epoch 1998  \tTraining Loss: 0.0001731699120779147\tValidation Loss: 0.00017426667691547768\n",
      "Epoch 1999  \tTraining Loss: 0.00017313730796134308\tValidation Loss: 0.00017423493227052545\n",
      "Epoch 2000  \tTraining Loss: 0.00017310473574145494\tValidation Loss: 0.00017420322045386875\n",
      "Epoch 2001  \tTraining Loss: 0.0001730721953141455\tValidation Loss: 0.00017417154036426623\n",
      "Epoch 2002  \tTraining Loss: 0.0001730396866312684\tValidation Loss: 0.0001741398916292784\n",
      "Epoch 2003  \tTraining Loss: 0.0001730072096490578\tValidation Loss: 0.00017410827419243963\n",
      "Epoch 2004  \tTraining Loss: 0.00017297476432440178\tValidation Loss: 0.0001740766880687743\n",
      "Epoch 2005  \tTraining Loss: 0.00017294235061433705\tValidation Loss: 0.00017404513326703698\n",
      "Epoch 2006  \tTraining Loss: 0.0001729099684759598\tValidation Loss: 0.00017401360977621665\n",
      "Epoch 2007  \tTraining Loss: 0.0001728776307460256\tValidation Loss: 0.00017398219032790653\n",
      "Epoch 2008  \tTraining Loss: 0.00017284532971071308\tValidation Loss: 0.0001739507524384878\n",
      "Epoch 2009  \tTraining Loss: 0.0001728130605801165\tValidation Loss: 0.00017391934377227185\n",
      "Epoch 2010  \tTraining Loss: 0.00017278082260099598\tValidation Loss: 0.00017388796727114546\n",
      "Epoch 2011  \tTraining Loss: 0.00017274861559925361\tValidation Loss: 0.00017385662103064776\n",
      "Epoch 2012  \tTraining Loss: 0.00017271644002902838\tValidation Loss: 0.00017382530615606004\n",
      "Epoch 2013  \tTraining Loss: 0.00017268429584416214\tValidation Loss: 0.00017379402245440827\n",
      "Epoch 2014  \tTraining Loss: 0.00017265218300056972\tValidation Loss: 0.00017376276975394954\n",
      "Epoch 2015  \tTraining Loss: 0.00017262009944507865\tValidation Loss: 0.00017373148132717627\n",
      "Epoch 2016  \tTraining Loss: 0.00017258802550390676\tValidation Loss: 0.00017370027522827333\n",
      "Epoch 2017  \tTraining Loss: 0.0001725559829133344\tValidation Loss: 0.00017366910058746497\n",
      "Epoch 2018  \tTraining Loss: 0.00017252397153607978\tValidation Loss: 0.00017363795748488408\n",
      "Epoch 2019  \tTraining Loss: 0.00017249199132408644\tValidation Loss: 0.00017360684561250906\n",
      "Epoch 2020  \tTraining Loss: 0.00017246004223337528\tValidation Loss: 0.00017357576474096305\n",
      "Epoch 2021  \tTraining Loss: 0.00017242812422049175\tValidation Loss: 0.00017354471471306784\n",
      "Epoch 2022  \tTraining Loss: 0.00017239623724212745\tValidation Loss: 0.00017351369542493875\n",
      "Epoch 2023  \tTraining Loss: 0.0001723643812550516\tValidation Loss: 0.00017348270680382747\n",
      "Epoch 2024  \tTraining Loss: 0.0001723325562160898\tValidation Loss: 0.00017345174879317368\n",
      "Epoch 2025  \tTraining Loss: 0.00017230076208211525\tValidation Loss: 0.00017342082134424097\n",
      "Epoch 2026  \tTraining Loss: 0.0001722689988100439\tValidation Loss: 0.00017338992441183826\n",
      "Epoch 2027  \tTraining Loss: 0.00017223726589472355\tValidation Loss: 0.0001733590698380244\n",
      "Epoch 2028  \tTraining Loss: 0.00017220554486111666\tValidation Loss: 0.0001733282226935844\n",
      "Epoch 2029  \tTraining Loss: 0.0001721738543645531\tValidation Loss: 0.00017329741486397669\n",
      "Epoch 2030  \tTraining Loss: 0.00017214219042799807\tValidation Loss: 0.00017326663023205203\n",
      "Epoch 2031  \tTraining Loss: 0.00017211055708803125\tValidation Loss: 0.00017323587572970223\n",
      "Epoch 2032  \tTraining Loss: 0.00017207895428960655\tValidation Loss: 0.00017320515161881202\n",
      "Epoch 2033  \tTraining Loss: 0.00017204738198778295\tValidation Loss: 0.00017317445785038668\n",
      "Epoch 2034  \tTraining Loss: 0.0001720158401391483\tValidation Loss: 0.00017314379431051472\n",
      "Epoch 2035  \tTraining Loss: 0.0001719843287006909\tValidation Loss: 0.00017311316090237283\n",
      "Epoch 2036  \tTraining Loss: 0.00017195284762956103\tValidation Loss: 0.00017308255755573917\n",
      "Epoch 2037  \tTraining Loss: 0.00017192139688299845\tValidation Loss: 0.0001730519842186646\n",
      "Epoch 2038  \tTraining Loss: 0.00017188997641830322\tValidation Loss: 0.0001730214408487892\n",
      "Epoch 2039  \tTraining Loss: 0.00017185858619282217\tValidation Loss: 0.0001729909274077525\n",
      "Epoch 2040  \tTraining Loss: 0.00017182722616394195\tValidation Loss: 0.0001729604438582293\n",
      "Epoch 2041  \tTraining Loss: 0.00017179589714654334\tValidation Loss: 0.00017292998666109842\n",
      "Epoch 2042  \tTraining Loss: 0.00017176460262179503\tValidation Loss: 0.00017289956530718492\n",
      "Epoch 2043  \tTraining Loss: 0.0001717333381646098\tValidation Loss: 0.00017286917404260937\n",
      "Epoch 2044  \tTraining Loss: 0.00017170210337682888\tValidation Loss: 0.00017283881266722255\n",
      "Epoch 2045  \tTraining Loss: 0.00017167089810193073\tValidation Loss: 0.00017280848018596875\n",
      "Epoch 2046  \tTraining Loss: 0.00017163972272941035\tValidation Loss: 0.00017277817726780474\n",
      "Epoch 2047  \tTraining Loss: 0.00017160857721681128\tValidation Loss: 0.00017274790392828317\n",
      "Epoch 2048  \tTraining Loss: 0.00017157746152175182\tValidation Loss: 0.00017271766014653902\n",
      "Epoch 2049  \tTraining Loss: 0.0001715463756018851\tValidation Loss: 0.00017268744588391527\n",
      "Epoch 2050  \tTraining Loss: 0.00017151531941489453\tValidation Loss: 0.00017265726109571246\n",
      "Epoch 2051  \tTraining Loss: 0.00017148429291849294\tValidation Loss: 0.0001726271057358352\n",
      "Epoch 2052  \tTraining Loss: 0.000171453296070422\tValidation Loss: 0.00017259697975830022\n",
      "Epoch 2053  \tTraining Loss: 0.00017142232882845195\tValidation Loss: 0.00017256688311759752\n",
      "Epoch 2054  \tTraining Loss: 0.0001713913911503816\tValidation Loss: 0.00017253681576869399\n",
      "Epoch 2055  \tTraining Loss: 0.00017136048299403776\tValidation Loss: 0.0001725067776669504\n",
      "Epoch 2056  \tTraining Loss: 0.0001713296043172757\tValidation Loss: 0.00017247676876803542\n",
      "Epoch 2057  \tTraining Loss: 0.00017129875507797864\tValidation Loss: 0.00017244678902785562\n",
      "Epoch 2058  \tTraining Loss: 0.00017126793523405766\tValidation Loss: 0.00017241683840250326\n",
      "Epoch 2059  \tTraining Loss: 0.0001712371447434519\tValidation Loss: 0.0001723869168482183\n",
      "Epoch 2060  \tTraining Loss: 0.0001712063835641282\tValidation Loss: 0.0001723570243213609\n",
      "Epoch 2061  \tTraining Loss: 0.00017117565165408123\tValidation Loss: 0.0001723271607783922\n",
      "Epoch 2062  \tTraining Loss: 0.00017114494897133337\tValidation Loss: 0.00017229732617586005\n",
      "Epoch 2063  \tTraining Loss: 0.00017111427547393459\tValidation Loss: 0.00017226752047038902\n",
      "Epoch 2064  \tTraining Loss: 0.0001710836311199626\tValidation Loss: 0.00017223774361867325\n",
      "Epoch 2065  \tTraining Loss: 0.00017105301586752263\tValidation Loss: 0.00017220799557747147\n",
      "Epoch 2066  \tTraining Loss: 0.00017102242967474774\tValidation Loss: 0.00017217827630360303\n",
      "Epoch 2067  \tTraining Loss: 0.00017099187249979835\tValidation Loss: 0.00017214858575394538\n",
      "Epoch 2068  \tTraining Loss: 0.00017096134430086263\tValidation Loss: 0.00017211892388543243\n",
      "Epoch 2069  \tTraining Loss: 0.00017093084503615644\tValidation Loss: 0.0001720892906550528\n",
      "Epoch 2070  \tTraining Loss: 0.00017090037466392316\tValidation Loss: 0.00017205968601984918\n",
      "Epoch 2071  \tTraining Loss: 0.00017086993314243407\tValidation Loss: 0.00017203010993691734\n",
      "Epoch 2072  \tTraining Loss: 0.000170839520429988\tValidation Loss: 0.00017200056236340574\n",
      "Epoch 2073  \tTraining Loss: 0.00017080913648491168\tValidation Loss: 0.00017197104325651527\n",
      "Epoch 2074  \tTraining Loss: 0.00017077878126555955\tValidation Loss: 0.00017194155257349857\n",
      "Epoch 2075  \tTraining Loss: 0.00017074845473031414\tValidation Loss: 0.00017191209027166013\n",
      "Epoch 2076  \tTraining Loss: 0.00017071815683758577\tValidation Loss: 0.00017188265630835585\n",
      "Epoch 2077  \tTraining Loss: 0.00017068788754581293\tValidation Loss: 0.00017185325064099296\n",
      "Epoch 2078  \tTraining Loss: 0.00017065764681346211\tValidation Loss: 0.00017182387127860011\n",
      "Epoch 2079  \tTraining Loss: 0.00017062743459902812\tValidation Loss: 0.0001717945170457191\n",
      "Epoch 2080  \tTraining Loss: 0.00017059725086103397\tValidation Loss: 0.00017176519094970805\n",
      "Epoch 2081  \tTraining Loss: 0.00017056709555803116\tValidation Loss: 0.00017173589294818288\n",
      "Epoch 2082  \tTraining Loss: 0.0001705369686485996\tValidation Loss: 0.00017170662299880973\n",
      "Epoch 2083  \tTraining Loss: 0.00017050687009134783\tValidation Loss: 0.00017167738105930481\n",
      "Epoch 2084  \tTraining Loss: 0.00017047679984491317\tValidation Loss: 0.00017164816708743416\n",
      "Epoch 2085  \tTraining Loss: 0.00017044675793319447\tValidation Loss: 0.00017161897902336488\n",
      "Epoch 2086  \tTraining Loss: 0.00017041674792471985\tValidation Loss: 0.0001715898126549014\n",
      "Epoch 2087  \tTraining Loss: 0.00017038676933481077\tValidation Loss: 0.0001715606854548752\n",
      "Epoch 2088  \tTraining Loss: 0.00017035681888709353\tValidation Loss: 0.00017153158683426832\n",
      "Epoch 2089  \tTraining Loss: 0.00017032689652418177\tValidation Loss: 0.00017150251618299272\n",
      "Epoch 2090  \tTraining Loss: 0.00017029700220312402\tValidation Loss: 0.00017147347328088763\n",
      "Epoch 2091  \tTraining Loss: 0.00017026713588244038\tValidation Loss: 0.00017144445805595288\n",
      "Epoch 2092  \tTraining Loss: 0.00017023729752092668\tValidation Loss: 0.00017141547046638336\n",
      "Epoch 2093  \tTraining Loss: 0.00017020748585677389\tValidation Loss: 0.00017138652090456304\n",
      "Epoch 2094  \tTraining Loss: 0.00017017769039789988\tValidation Loss: 0.0001713575853884053\n",
      "Epoch 2095  \tTraining Loss: 0.00017014792157497524\tValidation Loss: 0.00017132867242255627\n",
      "Epoch 2096  \tTraining Loss: 0.00017011818055633282\tValidation Loss: 0.000171299786744033\n",
      "Epoch 2097  \tTraining Loss: 0.00017008846729010758\tValidation Loss: 0.00017127092836214866\n",
      "Epoch 2098  \tTraining Loss: 0.00017005878173461232\tValidation Loss: 0.0001712420972643052\n",
      "Epoch 2099  \tTraining Loss: 0.00017002912384882392\tValidation Loss: 0.00017121329340720606\n",
      "Epoch 2100  \tTraining Loss: 0.0001699994935918609\tValidation Loss: 0.00017118451674089902\n",
      "Epoch 2101  \tTraining Loss: 0.00016996989092290756\tValidation Loss: 0.00017115576721857124\n",
      "Epoch 2102  \tTraining Loss: 0.00016994031580119413\tValidation Loss: 0.00017112704479783108\n",
      "Epoch 2103  \tTraining Loss: 0.00016991076818598987\tValidation Loss: 0.00017109834943934506\n",
      "Epoch 2104  \tTraining Loss: 0.00016988124803659979\tValidation Loss: 0.00017106968110535935\n",
      "Epoch 2105  \tTraining Loss: 0.00016985175531236351\tValidation Loss: 0.00017104103975871163\n",
      "Epoch 2106  \tTraining Loss: 0.00016982228997265475\tValidation Loss: 0.00017101242536229352\n",
      "Epoch 2107  \tTraining Loss: 0.00016979285197688053\tValidation Loss: 0.00017098383787880566\n",
      "Epoch 2108  \tTraining Loss: 0.00016976344128448185\tValidation Loss: 0.00017095527727067428\n",
      "Epoch 2109  \tTraining Loss: 0.0001697340578549333\tValidation Loss: 0.00017092674350004783\n",
      "Epoch 2110  \tTraining Loss: 0.00016970470164774318\tValidation Loss: 0.00017089823652882805\n",
      "Epoch 2111  \tTraining Loss: 0.0001696753726224537\tValidation Loss: 0.00017086975631871125\n",
      "Epoch 2112  \tTraining Loss: 0.00016964607073864112\tValidation Loss: 0.0001708413028312301\n",
      "Epoch 2113  \tTraining Loss: 0.00016961679595591593\tValidation Loss: 0.00017081287602779008\n",
      "Epoch 2114  \tTraining Loss: 0.00016958754823392295\tValidation Loss: 0.00017078447586970047\n",
      "Epoch 2115  \tTraining Loss: 0.0001695583275323415\tValidation Loss: 0.00017075610231819929\n",
      "Epoch 2116  \tTraining Loss: 0.00016952913381088565\tValidation Loss: 0.00017072775533447308\n",
      "Epoch 2117  \tTraining Loss: 0.00016949996702930452\tValidation Loss: 0.00017069943487967254\n",
      "Epoch 2118  \tTraining Loss: 0.00016947082714738204\tValidation Loss: 0.0001706711409149247\n",
      "Epoch 2119  \tTraining Loss: 0.00016944171006017997\tValidation Loss: 0.00017064291696646582\n",
      "Epoch 2120  \tTraining Loss: 0.00016941261425649002\tValidation Loss: 0.00017061467039648038\n",
      "Epoch 2121  \tTraining Loss: 0.0001693835453768404\tValidation Loss: 0.00017058645161375054\n",
      "Epoch 2122  \tTraining Loss: 0.0001693545032461161\tValidation Loss: 0.00017055826084239514\n",
      "Epoch 2123  \tTraining Loss: 0.00016932548781939082\tValidation Loss: 0.00017053009736733987\n",
      "Epoch 2124  \tTraining Loss: 0.00016929649905579003\tValidation Loss: 0.00017050196070316284\n",
      "Epoch 2125  \tTraining Loss: 0.00016926753691506218\tValidation Loss: 0.0001704738505739761\n",
      "Epoch 2126  \tTraining Loss: 0.00016923860135712924\tValidation Loss: 0.00017044576682667534\n",
      "Epoch 2127  \tTraining Loss: 0.00016920969199884988\tValidation Loss: 0.00017041761515195267\n",
      "Epoch 2128  \tTraining Loss: 0.0001691808039033649\tValidation Loss: 0.00017038954724515957\n",
      "Epoch 2129  \tTraining Loss: 0.00016915194235873568\tValidation Loss: 0.00017036152023483555\n",
      "Epoch 2130  \tTraining Loss: 0.00016912310725567851\tValidation Loss: 0.00017033352412641525\n",
      "Epoch 2131  \tTraining Loss: 0.00016909429855071798\tValidation Loss: 0.0001703055561434217\n",
      "Epoch 2132  \tTraining Loss: 0.0001690655162032325\tValidation Loss: 0.0001702776152687049\n",
      "Epoch 2133  \tTraining Loss: 0.00016903676017308026\tValidation Loss: 0.0001702497010526315\n",
      "Epoch 2134  \tTraining Loss: 0.0001690080304202997\tValidation Loss: 0.0001702218132605783\n",
      "Epoch 2135  \tTraining Loss: 0.00016897932302126572\tValidation Loss: 0.00017019396175411172\n",
      "Epoch 2136  \tTraining Loss: 0.00016895063616913963\tValidation Loss: 0.00017016613156147723\n",
      "Epoch 2137  \tTraining Loss: 0.00016892197557777388\tValidation Loss: 0.0001701383219539154\n",
      "Epoch 2138  \tTraining Loss: 0.00016889334112240296\tValidation Loss: 0.0001701105357726935\n",
      "Epoch 2139  \tTraining Loss: 0.0001688647339944143\tValidation Loss: 0.00017008277246684958\n",
      "Epoch 2140  \tTraining Loss: 0.0001688361583340929\tValidation Loss: 0.00017005503889214022\n",
      "Epoch 2141  \tTraining Loss: 0.00016880760869002353\tValidation Loss: 0.00017002733061518074\n",
      "Epoch 2142  \tTraining Loss: 0.00016877908501858292\tValidation Loss: 0.0001699996476573253\n",
      "Epoch 2143  \tTraining Loss: 0.0001687505872797571\tValidation Loss: 0.0001699719901024948\n",
      "Epoch 2144  \tTraining Loss: 0.00016872211543397176\tValidation Loss: 0.00016994435803043925\n",
      "Epoch 2145  \tTraining Loss: 0.00016869366802745557\tValidation Loss: 0.0001699167721916897\n",
      "Epoch 2146  \tTraining Loss: 0.0001686652421560799\tValidation Loss: 0.0001698891971305206\n",
      "Epoch 2147  \tTraining Loss: 0.00016863684214614528\tValidation Loss: 0.0001698616376150103\n",
      "Epoch 2148  \tTraining Loss: 0.00016860846788307806\tValidation Loss: 0.00016983410034480714\n",
      "Epoch 2149  \tTraining Loss: 0.00016858011972075175\tValidation Loss: 0.00016980656534241583\n",
      "Epoch 2150  \tTraining Loss: 0.00016855179935308944\tValidation Loss: 0.00016977906808280085\n",
      "Epoch 2151  \tTraining Loss: 0.0001685235049772137\tValidation Loss: 0.00016975159836415572\n",
      "Epoch 2152  \tTraining Loss: 0.00016849523617559203\tValidation Loss: 0.00016972415371509963\n",
      "Epoch 2153  \tTraining Loss: 0.00016846699285854246\tValidation Loss: 0.00016969673474817164\n",
      "Epoch 2154  \tTraining Loss: 0.0001684387745716298\tValidation Loss: 0.0001696693398410489\n",
      "Epoch 2155  \tTraining Loss: 0.00016841058172876753\tValidation Loss: 0.00016964197004173956\n",
      "Epoch 2156  \tTraining Loss: 0.00016838241429057061\tValidation Loss: 0.00016961462538752535\n",
      "Epoch 2157  \tTraining Loss: 0.00016835427221815956\tValidation Loss: 0.00016958730590093378\n",
      "Epoch 2158  \tTraining Loss: 0.00016832615547276498\tValidation Loss: 0.00016956001157283953\n",
      "Epoch 2159  \tTraining Loss: 0.00016829806276241684\tValidation Loss: 0.0001695327463920385\n",
      "Epoch 2160  \tTraining Loss: 0.00016826999378565866\tValidation Loss: 0.0001695055044141799\n",
      "Epoch 2161  \tTraining Loss: 0.00016824195003791898\tValidation Loss: 0.00016947828532120227\n",
      "Epoch 2162  \tTraining Loss: 0.000168213931466689\tValidation Loss: 0.00016945109153353497\n",
      "Epoch 2163  \tTraining Loss: 0.0001681859380325434\tValidation Loss: 0.0001694239239024859\n",
      "Epoch 2164  \tTraining Loss: 0.00016815796969678917\tValidation Loss: 0.0001693967807672906\n",
      "Epoch 2165  \tTraining Loss: 0.00016813002642091424\tValidation Loss: 0.00016936966222238129\n",
      "Epoch 2166  \tTraining Loss: 0.00016810210816650708\tValidation Loss: 0.00016934256831325817\n",
      "Epoch 2167  \tTraining Loss: 0.00016807421489523083\tValidation Loss: 0.00016931549905447674\n",
      "Epoch 2168  \tTraining Loss: 0.00016804634656881259\tValidation Loss: 0.0001692884544412379\n",
      "Epoch 2169  \tTraining Loss: 0.00016801850314903755\tValidation Loss: 0.00016926143445657163\n",
      "Epoch 2170  \tTraining Loss: 0.00016799068459774652\tValidation Loss: 0.00016923443907574051\n",
      "Epoch 2171  \tTraining Loss: 0.00016796289087683459\tValidation Loss: 0.00016920746826896692\n",
      "Epoch 2172  \tTraining Loss: 0.0001679351219482503\tValidation Loss: 0.00016918052200315696\n",
      "Epoch 2173  \tTraining Loss: 0.00016790737777399543\tValidation Loss: 0.00016915360024301375\n",
      "Epoch 2174  \tTraining Loss: 0.0001678796583161247\tValidation Loss: 0.0001691267029517706\n",
      "Epoch 2175  \tTraining Loss: 0.00016785196353674578\tValidation Loss: 0.0001690998300916812\n",
      "Epoch 2176  \tTraining Loss: 0.0001678242933980193\tValidation Loss: 0.0001690729816243519\n",
      "Epoch 2177  \tTraining Loss: 0.00016779664786215867\tValidation Loss: 0.00016904615751096836\n",
      "Epoch 2178  \tTraining Loss: 0.0001677690268914306\tValidation Loss: 0.00016901935771245174\n",
      "Epoch 2179  \tTraining Loss: 0.00016774143044815477\tValidation Loss: 0.00016899258218956687\n",
      "Epoch 2180  \tTraining Loss: 0.0001677138584947039\tValidation Loss: 0.00016896583090299718\n",
      "Epoch 2181  \tTraining Loss: 0.00016768631099350428\tValidation Loss: 0.00016893910381339725\n",
      "Epoch 2182  \tTraining Loss: 0.00016765878790703546\tValidation Loss: 0.00016891240088142925\n",
      "Epoch 2183  \tTraining Loss: 0.00016763128919783043\tValidation Loss: 0.0001688857220677886\n",
      "Epoch 2184  \tTraining Loss: 0.00016760381482847585\tValidation Loss: 0.00016885906733322235\n",
      "Epoch 2185  \tTraining Loss: 0.0001675763647616124\tValidation Loss: 0.00016883243663854135\n",
      "Epoch 2186  \tTraining Loss: 0.00016754893895993433\tValidation Loss: 0.00016880582994462958\n",
      "Epoch 2187  \tTraining Loss: 0.00016752153738619014\tValidation Loss: 0.0001687792472124503\n",
      "Epoch 2188  \tTraining Loss: 0.00016749416000318252\tValidation Loss: 0.00016875268840305045\n",
      "Epoch 2189  \tTraining Loss: 0.00016746680677376844\tValidation Loss: 0.00016872615347756373\n",
      "Epoch 2190  \tTraining Loss: 0.00016743947766085956\tValidation Loss: 0.00016869964239721284\n",
      "Epoch 2191  \tTraining Loss: 0.00016741217037964266\tValidation Loss: 0.0001686732045617094\n",
      "Epoch 2192  \tTraining Loss: 0.00016738487867852363\tValidation Loss: 0.00016864673753699485\n",
      "Epoch 2193  \tTraining Loss: 0.0001673576149870847\tValidation Loss: 0.00016862043801310706\n",
      "Epoch 2194  \tTraining Loss: 0.00016733037621752795\tValidation Loss: 0.00016859406788693798\n",
      "Epoch 2195  \tTraining Loss: 0.0001673031617268158\tValidation Loss: 0.00016856770266246597\n",
      "Epoch 2196  \tTraining Loss: 0.0001672759711382422\tValidation Loss: 0.00016854135194611502\n",
      "Epoch 2197  \tTraining Loss: 0.00016724880439350642\tValidation Loss: 0.00016851502102691816\n",
      "Epoch 2198  \tTraining Loss: 0.00016722166145193826\tValidation Loss: 0.00016848871202200185\n",
      "Epoch 2199  \tTraining Loss: 0.0001671945422756067\tValidation Loss: 0.00016846242575773213\n",
      "Epoch 2200  \tTraining Loss: 0.0001671674468273053\tValidation Loss: 0.0001684361625494414\n",
      "Epoch 2201  \tTraining Loss: 0.00016714037507011277\tValidation Loss: 0.00016840992250952475\n",
      "Epoch 2202  \tTraining Loss: 0.00016711332696726366\tValidation Loss: 0.00016838370566782005\n",
      "Epoch 2203  \tTraining Loss: 0.00016708630204446354\tValidation Loss: 0.00016835751761803917\n",
      "Epoch 2204  \tTraining Loss: 0.0001670592989982355\tValidation Loss: 0.00016833134565051513\n",
      "Epoch 2205  \tTraining Loss: 0.00016703231948982293\tValidation Loss: 0.0001683051972677063\n",
      "Epoch 2206  \tTraining Loss: 0.0001670053634794192\tValidation Loss: 0.00016827907744095834\n",
      "Epoch 2207  \tTraining Loss: 0.00016697843093052317\tValidation Loss: 0.00016825298321401484\n",
      "Epoch 2208  \tTraining Loss: 0.0001669515218068106\tValidation Loss: 0.0001682269121409877\n",
      "Epoch 2209  \tTraining Loss: 0.00016692463607204786\tValidation Loss: 0.00016820086414177715\n",
      "Epoch 2210  \tTraining Loss: 0.00016689777371926026\tValidation Loss: 0.00016817478687569692\n",
      "Epoch 2211  \tTraining Loss: 0.00016687093725275656\tValidation Loss: 0.00016814876938753303\n",
      "Epoch 2212  \tTraining Loss: 0.00016684412411410238\tValidation Loss: 0.00016812278235874218\n",
      "Epoch 2213  \tTraining Loss: 0.00016681733421594397\tValidation Loss: 0.00016809682194163933\n",
      "Epoch 2214  \tTraining Loss: 0.00016679056751935315\tValidation Loss: 0.00016807088596318274\n",
      "Epoch 2215  \tTraining Loss: 0.00016676382684339521\tValidation Loss: 0.00016804491982504017\n",
      "Epoch 2216  \tTraining Loss: 0.0001667371124628678\tValidation Loss: 0.00016801896546386616\n",
      "Epoch 2217  \tTraining Loss: 0.00016671042769563372\tValidation Loss: 0.00016799307790602556\n",
      "Epoch 2218  \tTraining Loss: 0.00016668376777470353\tValidation Loss: 0.00016796723683438773\n",
      "Epoch 2219  \tTraining Loss: 0.000166657130938422\tValidation Loss: 0.00016794142180141515\n",
      "Epoch 2220  \tTraining Loss: 0.00016663051710888148\tValidation Loss: 0.00016791563089890165\n",
      "Epoch 2221  \tTraining Loss: 0.0001666039262462453\tValidation Loss: 0.00016788986320254143\n",
      "Epoch 2222  \tTraining Loss: 0.00016657735831366875\tValidation Loss: 0.00016786411833711522\n",
      "Epoch 2223  \tTraining Loss: 0.00016655081327502427\tValidation Loss: 0.0001678383961345377\n",
      "Epoch 2224  \tTraining Loss: 0.00016652429094720476\tValidation Loss: 0.0001678127026540493\n",
      "Epoch 2225  \tTraining Loss: 0.00016649779001764274\tValidation Loss: 0.0001677870275707\n",
      "Epoch 2226  \tTraining Loss: 0.00016647131187434315\tValidation Loss: 0.0001677613749735535\n",
      "Epoch 2227  \tTraining Loss: 0.0001664448564778092\tValidation Loss: 0.00016773574488168996\n",
      "Epoch 2228  \tTraining Loss: 0.00016641842379227034\tValidation Loss: 0.00016771013723643685\n",
      "Epoch 2229  \tTraining Loss: 0.00016639201378237904\tValidation Loss: 0.00016768455195708765\n",
      "Epoch 2230  \tTraining Loss: 0.00016636562641292074\tValidation Loss: 0.0001676589889757499\n",
      "Epoch 2231  \tTraining Loss: 0.00016633926164877144\tValidation Loss: 0.00016763344824010423\n",
      "Epoch 2232  \tTraining Loss: 0.00016631291945488875\tValidation Loss: 0.00016760792970817732\n",
      "Epoch 2233  \tTraining Loss: 0.0001662865997963086\tValidation Loss: 0.00016758243334348238\n",
      "Epoch 2234  \tTraining Loss: 0.00016626030263814374\tValidation Loss: 0.00016755695911201522\n",
      "Epoch 2235  \tTraining Loss: 0.0001662340279455829\tValidation Loss: 0.0001675315069806939\n",
      "Epoch 2236  \tTraining Loss: 0.00016620777568389006\tValidation Loss: 0.00016750607691664432\n",
      "Epoch 2237  \tTraining Loss: 0.00016618154581840417\tValidation Loss: 0.00016748066888691912\n",
      "Epoch 2238  \tTraining Loss: 0.00016615533831453846\tValidation Loss: 0.000167455282858418\n",
      "Epoch 2239  \tTraining Loss: 0.00016612915313778064\tValidation Loss: 0.00016742991879789275\n",
      "Epoch 2240  \tTraining Loss: 0.00016610299025369212\tValidation Loss: 0.00016740457667198058\n",
      "Epoch 2241  \tTraining Loss: 0.0001660768496279083\tValidation Loss: 0.00016737925644724348\n",
      "Epoch 2242  \tTraining Loss: 0.00016605073122613812\tValidation Loss: 0.0001673539601082667\n",
      "Epoch 2243  \tTraining Loss: 0.000166024635014164\tValidation Loss: 0.00016732868849344109\n",
      "Epoch 2244  \tTraining Loss: 0.0001659985609578417\tValidation Loss: 0.0001673034386824108\n",
      "Epoch 2245  \tTraining Loss: 0.0001659725090231003\tValidation Loss: 0.0001672782106416967\n",
      "Epoch 2246  \tTraining Loss: 0.00016594647917594208\tValidation Loss: 0.0001672530043378581\n",
      "Epoch 2247  \tTraining Loss: 0.0001659204713824425\tValidation Loss: 0.00016722781958779397\n",
      "Epoch 2248  \tTraining Loss: 0.00016589448560875\tValidation Loss: 0.00016720265647572667\n",
      "Epoch 2249  \tTraining Loss: 0.00016586852175403438\tValidation Loss: 0.0001671775061405593\n",
      "Epoch 2250  \tTraining Loss: 0.000165842579669413\tValidation Loss: 0.00016715237910630503\n",
      "Epoch 2251  \tTraining Loss: 0.0001658166595082068\tValidation Loss: 0.0001671272748985871\n",
      "Epoch 2252  \tTraining Loss: 0.00016579076123120025\tValidation Loss: 0.00016710219311662184\n",
      "Epoch 2253  \tTraining Loss: 0.00016576488480428097\tValidation Loss: 0.0001670771333849781\n",
      "Epoch 2254  \tTraining Loss: 0.00016573903019390744\tValidation Loss: 0.0001670520954673655\n",
      "Epoch 2255  \tTraining Loss: 0.00016571319731676308\tValidation Loss: 0.00016702707750291873\n",
      "Epoch 2256  \tTraining Loss: 0.0001656873861069263\tValidation Loss: 0.00016700208216770005\n",
      "Epoch 2257  \tTraining Loss: 0.0001656615966099178\tValidation Loss: 0.0001669771085225382\n",
      "Epoch 2258  \tTraining Loss: 0.00016563582879259474\tValidation Loss: 0.00016695215640037786\n",
      "Epoch 2259  \tTraining Loss: 0.0001656100826219295\tValidation Loss: 0.00016692722568754042\n",
      "Epoch 2260  \tTraining Loss: 0.00016558435806497012\tValidation Loss: 0.00016690231631357212\n",
      "Epoch 2261  \tTraining Loss: 0.00016555865508883905\tValidation Loss: 0.0001668774282270397\n",
      "Epoch 2262  \tTraining Loss: 0.00016553297528065764\tValidation Loss: 0.00016685246552260768\n",
      "Epoch 2263  \tTraining Loss: 0.00016550731807363312\tValidation Loss: 0.0001668277338501744\n",
      "Epoch 2264  \tTraining Loss: 0.00016548168945724475\tValidation Loss: 0.00016680305418210141\n",
      "Epoch 2265  \tTraining Loss: 0.0001654560822351244\tValidation Loss: 0.0001667782044601673\n",
      "Epoch 2266  \tTraining Loss: 0.0001654304958567537\tValidation Loss: 0.00016675344134773612\n",
      "Epoch 2267  \tTraining Loss: 0.00016540493118471686\tValidation Loss: 0.00016672870635524696\n",
      "Epoch 2268  \tTraining Loss: 0.00016537938794679573\tValidation Loss: 0.00016670399655371832\n",
      "Epoch 2269  \tTraining Loss: 0.00016535386610027153\tValidation Loss: 0.0001666793091152355\n",
      "Epoch 2270  \tTraining Loss: 0.00016532836612422885\tValidation Loss: 0.00016665473675479844\n",
      "Epoch 2271  \tTraining Loss: 0.00016530288724550323\tValidation Loss: 0.00016663000775777424\n",
      "Epoch 2272  \tTraining Loss: 0.0001652774290689887\tValidation Loss: 0.00016660536683658607\n",
      "Epoch 2273  \tTraining Loss: 0.00016525199239069135\tValidation Loss: 0.00016658075467778567\n",
      "Epoch 2274  \tTraining Loss: 0.00016522657694715255\tValidation Loss: 0.00016655616798012964\n",
      "Epoch 2275  \tTraining Loss: 0.00016520118269764265\tValidation Loss: 0.00016653160375127853\n",
      "Epoch 2276  \tTraining Loss: 0.00016517580989183149\tValidation Loss: 0.00016650710485161977\n",
      "Epoch 2277  \tTraining Loss: 0.00016515045871681375\tValidation Loss: 0.00016648259609218827\n",
      "Epoch 2278  \tTraining Loss: 0.00016512512866629104\tValidation Loss: 0.00016645810151624913\n",
      "Epoch 2279  \tTraining Loss: 0.00016509981988408352\tValidation Loss: 0.00016643362307428077\n",
      "Epoch 2280  \tTraining Loss: 0.00016507453297848757\tValidation Loss: 0.00016640916580812313\n",
      "Epoch 2281  \tTraining Loss: 0.00016504926705149722\tValidation Loss: 0.00016638472757394895\n",
      "Epoch 2282  \tTraining Loss: 0.00016502402206922606\tValidation Loss: 0.00016636030907742964\n",
      "Epoch 2283  \tTraining Loss: 0.00016499879706560384\tValidation Loss: 0.00016633592774559534\n",
      "Epoch 2284  \tTraining Loss: 0.0001649735906538157\tValidation Loss: 0.0001663115479962035\n",
      "Epoch 2285  \tTraining Loss: 0.00016494840510355996\tValidation Loss: 0.00016628718870632687\n",
      "Epoch 2286  \tTraining Loss: 0.0001649232403625578\tValidation Loss: 0.000166262850011062\n",
      "Epoch 2287  \tTraining Loss: 0.00016489809639859713\tValidation Loss: 0.0001662385317218861\n",
      "Epoch 2288  \tTraining Loss: 0.00016487297318014006\tValidation Loss: 0.00016621423369230125\n",
      "Epoch 2289  \tTraining Loss: 0.00016484787067582503\tValidation Loss: 0.00016618995583434255\n",
      "Epoch 2290  \tTraining Loss: 0.00016482278885439987\tValidation Loss: 0.00016616569809575275\n",
      "Epoch 2291  \tTraining Loss: 0.00016479772768470712\tValidation Loss: 0.00016614146044224831\n",
      "Epoch 2292  \tTraining Loss: 0.00016477268713568018\tValidation Loss: 0.00016611724284746625\n",
      "Epoch 2293  \tTraining Loss: 0.00016474766497433493\tValidation Loss: 0.00016609304265070032\n",
      "Epoch 2294  \tTraining Loss: 0.00016472265889236158\tValidation Loss: 0.00016606885905715024\n",
      "Epoch 2295  \tTraining Loss: 0.00016469767334880813\tValidation Loss: 0.0001660446956561009\n",
      "Epoch 2296  \tTraining Loss: 0.000164672708308864\tValidation Loss: 0.00016602055241687072\n",
      "Epoch 2297  \tTraining Loss: 0.00016464776432578026\tValidation Loss: 0.00016599646824726435\n",
      "Epoch 2298  \tTraining Loss: 0.00016462284347029245\tValidation Loss: 0.00016597237223337608\n",
      "Epoch 2299  \tTraining Loss: 0.00016459794304887138\tValidation Loss: 0.00016594828976870785\n",
      "Epoch 2300  \tTraining Loss: 0.00016457306298131065\tValidation Loss: 0.0001659242240427658\n",
      "Epoch 2301  \tTraining Loss: 0.0001645482032343522\tValidation Loss: 0.00016590017696097256\n",
      "Epoch 2302  \tTraining Loss: 0.00016452336377741677\tValidation Loss: 0.0001658761492671644\n",
      "Epoch 2303  \tTraining Loss: 0.00016449854458043742\tValidation Loss: 0.00016585214123682448\n",
      "Epoch 2304  \tTraining Loss: 0.00016447374561354063\tValidation Loss: 0.00016582815296180208\n",
      "Epoch 2305  \tTraining Loss: 0.00016444896684697245\tValidation Loss: 0.0001658041844623667\n",
      "Epoch 2306  \tTraining Loss: 0.0001644242082510757\tValidation Loss: 0.00016578023573050597\n",
      "Epoch 2307  \tTraining Loss: 0.000164399469796282\tValidation Loss: 0.00016575630674682415\n",
      "Epoch 2308  \tTraining Loss: 0.00016437475145310843\tValidation Loss: 0.00016573239748725958\n",
      "Epoch 2309  \tTraining Loss: 0.00016435005319215574\tValidation Loss: 0.00016570850792581272\n",
      "Epoch 2310  \tTraining Loss: 0.00016432537498410764\tValidation Loss: 0.00016568463803567804\n",
      "Epoch 2311  \tTraining Loss: 0.00016430071679973023\tValidation Loss: 0.0001656607877897222\n",
      "Epoch 2312  \tTraining Loss: 0.00016427607860987155\tValidation Loss: 0.00016563695716068953\n",
      "Epoch 2313  \tTraining Loss: 0.00016425146038546163\tValidation Loss: 0.0001656131461212904\n",
      "Epoch 2314  \tTraining Loss: 0.00016422686209751185\tValidation Loss: 0.00016558935464423957\n",
      "Epoch 2315  \tTraining Loss: 0.00016420228371711535\tValidation Loss: 0.00016556558270227248\n",
      "Epoch 2316  \tTraining Loss: 0.00016417772521544648\tValidation Loss: 0.00016554183026815217\n",
      "Epoch 2317  \tTraining Loss: 0.00016415318656376086\tValidation Loss: 0.00016551809731467232\n",
      "Epoch 2318  \tTraining Loss: 0.0001641286677712906\tValidation Loss: 0.00016549434388054359\n",
      "Epoch 2319  \tTraining Loss: 0.00016410416891981917\tValidation Loss: 0.00016547067710866254\n",
      "Epoch 2320  \tTraining Loss: 0.0001640796896453727\tValidation Loss: 0.0001654469684718442\n",
      "Epoch 2321  \tTraining Loss: 0.00016405523032066852\tValidation Loss: 0.0001654233433135641\n",
      "Epoch 2322  \tTraining Loss: 0.0001640307905021513\tValidation Loss: 0.00016539967484139053\n",
      "Epoch 2323  \tTraining Loss: 0.00016400637075595262\tValidation Loss: 0.000165376099876843\n",
      "Epoch 2324  \tTraining Loss: 0.00016398197403454025\tValidation Loss: 0.0001653524704019377\n",
      "Epoch 2325  \tTraining Loss: 0.00016395759718622802\tValidation Loss: 0.0001653289239688741\n",
      "Epoch 2326  \tTraining Loss: 0.0001639332398404255\tValidation Loss: 0.000165305335146738\n",
      "Epoch 2327  \tTraining Loss: 0.00016390890366095815\tValidation Loss: 0.00016528179719725502\n",
      "Epoch 2328  \tTraining Loss: 0.00016388458980162238\tValidation Loss: 0.00016525827159546432\n",
      "Epoch 2329  \tTraining Loss: 0.0001638602969288681\tValidation Loss: 0.00016523476907794437\n",
      "Epoch 2330  \tTraining Loss: 0.00016383602354895236\tValidation Loss: 0.00016521128673195623\n",
      "Epoch 2331  \tTraining Loss: 0.00016381176962666685\tValidation Loss: 0.00016518782391376972\n",
      "Epoch 2332  \tTraining Loss: 0.00016378753513306339\tValidation Loss: 0.00016516438046726713\n",
      "Epoch 2333  \tTraining Loss: 0.00016376332004013882\tValidation Loss: 0.00016514095631240983\n",
      "Epoch 2334  \tTraining Loss: 0.0001637391269108238\tValidation Loss: 0.00016511751775977822\n",
      "Epoch 2335  \tTraining Loss: 0.0001637149562850879\tValidation Loss: 0.00016509413191562696\n",
      "Epoch 2336  \tTraining Loss: 0.0001636908050392898\tValidation Loss: 0.00016507076503713712\n",
      "Epoch 2337  \tTraining Loss: 0.00016366667307811145\tValidation Loss: 0.00016504741669161072\n",
      "Epoch 2338  \tTraining Loss: 0.00016364256037207627\tValidation Loss: 0.0001650240870581184\n",
      "Epoch 2339  \tTraining Loss: 0.00016361846689365758\tValidation Loss: 0.0001650007762841666\n",
      "Epoch 2340  \tTraining Loss: 0.00016359439261569593\tValidation Loss: 0.00016497747925707537\n",
      "Epoch 2341  \tTraining Loss: 0.00016357033751119063\tValidation Loss: 0.00016495420114708993\n",
      "Epoch 2342  \tTraining Loss: 0.00016354630155325622\tValidation Loss: 0.00016493094194454098\n",
      "Epoch 2343  \tTraining Loss: 0.0001635222847845762\tValidation Loss: 0.00016490770185866792\n",
      "Epoch 2344  \tTraining Loss: 0.0001634982882508591\tValidation Loss: 0.00016488448159449333\n",
      "Epoch 2345  \tTraining Loss: 0.00016347431076799553\tValidation Loss: 0.00016486128011136675\n",
      "Epoch 2346  \tTraining Loss: 0.0001634503523091872\tValidation Loss: 0.00016483809736244755\n",
      "Epoch 2347  \tTraining Loss: 0.00016342641284799016\tValidation Loss: 0.00016481493332048445\n",
      "Epoch 2348  \tTraining Loss: 0.00016340249235806967\tValidation Loss: 0.0001647917879638243\n",
      "Epoch 2349  \tTraining Loss: 0.00016337859081318182\tValidation Loss: 0.00016476866126817916\n",
      "Epoch 2350  \tTraining Loss: 0.00016335470818717104\tValidation Loss: 0.00016474555320625332\n",
      "Epoch 2351  \tTraining Loss: 0.0001633308444539693\tValidation Loss: 0.0001647224637491383\n",
      "Epoch 2352  \tTraining Loss: 0.00016330699958759537\tValidation Loss: 0.00016469939286744783\n",
      "Epoch 2353  \tTraining Loss: 0.00016328317356215436\tValidation Loss: 0.00016467634053192866\n",
      "Epoch 2354  \tTraining Loss: 0.00016325936635183753\tValidation Loss: 0.00016465330671371252\n",
      "Epoch 2355  \tTraining Loss: 0.00016323557793092167\tValidation Loss: 0.0001646302913843809\n",
      "Epoch 2356  \tTraining Loss: 0.00016321180827376884\tValidation Loss: 0.00016460729451594816\n",
      "Epoch 2357  \tTraining Loss: 0.0001631880573548261\tValidation Loss: 0.00016458431608081656\n",
      "Epoch 2358  \tTraining Loss: 0.00016316432514862519\tValidation Loss: 0.0001645613560517277\n",
      "Epoch 2359  \tTraining Loss: 0.00016314061339079227\tValidation Loss: 0.0001645384133460603\n",
      "Epoch 2360  \tTraining Loss: 0.00016311692234973932\tValidation Loss: 0.00016451549192520481\n",
      "Epoch 2361  \tTraining Loss: 0.00016309324992758883\tValidation Loss: 0.00016449258845637062\n",
      "Epoch 2362  \tTraining Loss: 0.0001630695960944738\tValidation Loss: 0.00016446970304703003\n",
      "Epoch 2363  \tTraining Loss: 0.00016304596082483924\tValidation Loss: 0.00016444683576798447\n",
      "Epoch 2364  \tTraining Loss: 0.00016302234409356289\tValidation Loss: 0.00016442398666858542\n",
      "Epoch 2365  \tTraining Loss: 0.0001629987458756553\tValidation Loss: 0.00016440115576707952\n",
      "Epoch 2366  \tTraining Loss: 0.00016297516614622145\tValidation Loss: 0.00016437834305865786\n",
      "Epoch 2367  \tTraining Loss: 0.00016295160488045336\tValidation Loss: 0.00016435554852547204\n",
      "Epoch 2368  \tTraining Loss: 0.00016292806205362774\tValidation Loss: 0.00016433277214347153\n",
      "Epoch 2369  \tTraining Loss: 0.00016290453764110504\tValidation Loss: 0.00016431001388614856\n",
      "Epoch 2370  \tTraining Loss: 0.0001628810316183288\tValidation Loss: 0.00016428727372633855\n",
      "Epoch 2371  \tTraining Loss: 0.0001628575439608252\tValidation Loss: 0.00016426455163698762\n",
      "Epoch 2372  \tTraining Loss: 0.00016283407397086987\tValidation Loss: 0.0001642418485243075\n",
      "Epoch 2373  \tTraining Loss: 0.00016281061631430167\tValidation Loss: 0.00016421915631066646\n",
      "Epoch 2374  \tTraining Loss: 0.00016278717696163503\tValidation Loss: 0.00016419648302951718\n",
      "Epoch 2375  \tTraining Loss: 0.00016276375585974682\tValidation Loss: 0.00016417382829774464\n",
      "Epoch 2376  \tTraining Loss: 0.00016274035298178804\tValidation Loss: 0.00016415119185598885\n",
      "Epoch 2377  \tTraining Loss: 0.00016271696830326898\tValidation Loss: 0.0001641285735025718\n",
      "Epoch 2378  \tTraining Loss: 0.00016269360180014625\tValidation Loss: 0.00016410597311197685\n",
      "Epoch 2379  \tTraining Loss: 0.00016267025344855358\tValidation Loss: 0.00016408339061386664\n",
      "Epoch 2380  \tTraining Loss: 0.00016264692322474284\tValidation Loss: 0.0001640608259685327\n",
      "Epoch 2381  \tTraining Loss: 0.00016262361110506437\tValidation Loss: 0.00016403827915047897\n",
      "Epoch 2382  \tTraining Loss: 0.00016260031697406182\tValidation Loss: 0.00016401574959047812\n",
      "Epoch 2383  \tTraining Loss: 0.00016257703946145682\tValidation Loss: 0.00016399324168488893\n",
      "Epoch 2384  \tTraining Loss: 0.00016255377999312444\tValidation Loss: 0.00016397075034785058\n",
      "Epoch 2385  \tTraining Loss: 0.00016253053853953122\tValidation Loss: 0.0001639482762783833\n",
      "Epoch 2386  \tTraining Loss: 0.0001625073150771414\tValidation Loss: 0.00016392581974470139\n",
      "Epoch 2387  \tTraining Loss: 0.0001624841095827701\tValidation Loss: 0.00016390338081426247\n",
      "Epoch 2388  \tTraining Loss: 0.00016246092203334774\tValidation Loss: 0.00016388095949322541\n",
      "Epoch 2389  \tTraining Loss: 0.00016243775240589367\tValidation Loss: 0.00016385855576957401\n",
      "Epoch 2390  \tTraining Loss: 0.00016241460067751136\tValidation Loss: 0.00016383616962581092\n",
      "Epoch 2391  \tTraining Loss: 0.0001623914668253866\tValidation Loss: 0.00016381380104254806\n",
      "Epoch 2392  \tTraining Loss: 0.00016236835082678733\tValidation Loss: 0.00016379144999952253\n",
      "Epoch 2393  \tTraining Loss: 0.0001623452526590628\tValidation Loss: 0.0001637691164759277\n",
      "Epoch 2394  \tTraining Loss: 0.00016232217229964342\tValidation Loss: 0.00016374680045057048\n",
      "Epoch 2395  \tTraining Loss: 0.00016229910972604086\tValidation Loss: 0.0001637245019019805\n",
      "Epoch 2396  \tTraining Loss: 0.0001622760649158475\tValidation Loss: 0.00016370222080849643\n",
      "Epoch 2397  \tTraining Loss: 0.00016225303784673663\tValidation Loss: 0.00016367995714833484\n",
      "Epoch 2398  \tTraining Loss: 0.00016223002849646205\tValidation Loss: 0.0001636577108996433\n",
      "Epoch 2399  \tTraining Loss: 0.00016220703684285826\tValidation Loss: 0.00016363548204054062\n",
      "Epoch 2400  \tTraining Loss: 0.00016218406286384006\tValidation Loss: 0.00016361327054914665\n",
      "Epoch 2401  \tTraining Loss: 0.0001621611065374026\tValidation Loss: 0.00016359107640360386\n",
      "Epoch 2402  \tTraining Loss: 0.00016213816784162117\tValidation Loss: 0.00016356889958209274\n",
      "Epoch 2403  \tTraining Loss: 0.00016211524638937855\tValidation Loss: 0.00016354674176616228\n",
      "Epoch 2404  \tTraining Loss: 0.00016209234181277616\tValidation Loss: 0.00016352459782682005\n",
      "Epoch 2405  \tTraining Loss: 0.00016206945481300367\tValidation Loss: 0.0001635024716850172\n",
      "Epoch 2406  \tTraining Loss: 0.00016204658536033545\tValidation Loss: 0.00016348036309738134\n",
      "Epoch 2407  \tTraining Loss: 0.00016202373343249816\tValidation Loss: 0.00016345827190379574\n",
      "Epoch 2408  \tTraining Loss: 0.000162000899007933\tValidation Loss: 0.00016343619798385727\n",
      "Epoch 2409  \tTraining Loss: 0.0001619780820652622\tValidation Loss: 0.00016341414126111583\n",
      "Epoch 2410  \tTraining Loss: 0.0001619552825832139\tValidation Loss: 0.00016339210168972256\n",
      "Epoch 2411  \tTraining Loss: 0.00016193250054060532\tValidation Loss: 0.00016337007924052777\n",
      "Epoch 2412  \tTraining Loss: 0.00016190973591633754\tValidation Loss: 0.00016334807389207031\n",
      "Epoch 2413  \tTraining Loss: 0.00016188698868939275\tValidation Loss: 0.00016332608562581334\n",
      "Epoch 2414  \tTraining Loss: 0.0001618642588388333\tValidation Loss: 0.00016330411442394145\n",
      "Epoch 2415  \tTraining Loss: 0.0001618415465061372\tValidation Loss: 0.0001632821680558504\n",
      "Epoch 2416  \tTraining Loss: 0.0001618188567811692\tValidation Loss: 0.00016326023349499721\n",
      "Epoch 2417  \tTraining Loss: 0.0001617961844090757\tValidation Loss: 0.0001632383168809535\n",
      "Epoch 2418  \tTraining Loss: 0.00016177352934785124\tValidation Loss: 0.00016321641776439991\n",
      "Epoch 2419  \tTraining Loss: 0.0001617508915748931\tValidation Loss: 0.00016319453587304632\n",
      "Epoch 2420  \tTraining Loss: 0.0001617282710692962\tValidation Loss: 0.00016317267101601915\n",
      "Epoch 2421  \tTraining Loss: 0.00016170566781048182\tValidation Loss: 0.00016315082308067272\n",
      "Epoch 2422  \tTraining Loss: 0.00016168308177801172\tValidation Loss: 0.00016312899200674478\n",
      "Epoch 2423  \tTraining Loss: 0.00016166051295154855\tValidation Loss: 0.00016310719065344288\n",
      "Epoch 2424  \tTraining Loss: 0.00016163796453064438\tValidation Loss: 0.0001630854543743576\n",
      "Epoch 2425  \tTraining Loss: 0.00016161543658183854\tValidation Loss: 0.000163063696591945\n",
      "Epoch 2426  \tTraining Loss: 0.00016159292582016923\tValidation Loss: 0.00016304195392516767\n",
      "Epoch 2427  \tTraining Loss: 0.00016157043217724892\tValidation Loss: 0.00016302022687804567\n",
      "Epoch 2428  \tTraining Loss: 0.0001615479556309764\tValidation Loss: 0.00016299852608815343\n",
      "Epoch 2429  \tTraining Loss: 0.000161525496161156\tValidation Loss: 0.00016297684605686094\n",
      "Epoch 2430  \tTraining Loss: 0.00016150305374787655\tValidation Loss: 0.00016295518253778347\n",
      "Epoch 2431  \tTraining Loss: 0.00016148062837135138\tValidation Loss: 0.00016293353557349563\n",
      "Epoch 2432  \tTraining Loss: 0.00016145822001188969\tValidation Loss: 0.00016291190517407357\n",
      "Epoch 2433  \tTraining Loss: 0.00016143582864988778\tValidation Loss: 0.00016289029133474952\n",
      "Epoch 2434  \tTraining Loss: 0.0001614134542658255\tValidation Loss: 0.00016286869404412194\n",
      "Epoch 2435  \tTraining Loss: 0.00016139109684026404\tValidation Loss: 0.00016284711328788025\n",
      "Epoch 2436  \tTraining Loss: 0.000161368756353845\tValidation Loss: 0.00016282554905047066\n",
      "Epoch 2437  \tTraining Loss: 0.00016134643278728944\tValidation Loss: 0.00016280400131583105\n",
      "Epoch 2438  \tTraining Loss: 0.00016132412612139758\tValidation Loss: 0.00016278247006770878\n",
      "Epoch 2439  \tTraining Loss: 0.00016130183633704823\tValidation Loss: 0.0001627609552897934\n",
      "Epoch 2440  \tTraining Loss: 0.00016127956341519857\tValidation Loss: 0.00016273945696576779\n",
      "Epoch 2441  \tTraining Loss: 0.00016125730733688383\tValidation Loss: 0.0001627179750793254\n",
      "Epoch 2442  \tTraining Loss: 0.00016123506808321693\tValidation Loss: 0.00016269650961417333\n",
      "Epoch 2443  \tTraining Loss: 0.0001612128456353886\tValidation Loss: 0.00016267506055403096\n",
      "Epoch 2444  \tTraining Loss: 0.00016119063997466662\tValidation Loss: 0.00016265362788262758\n",
      "Epoch 2445  \tTraining Loss: 0.00016116844915411722\tValidation Loss: 0.00016263222119349967\n",
      "Epoch 2446  \tTraining Loss: 0.00016114626663301637\tValidation Loss: 0.00016261082237833887\n",
      "Epoch 2447  \tTraining Loss: 0.0001611241011081531\tValidation Loss: 0.00016258943255189682\n",
      "Epoch 2448  \tTraining Loss: 0.00016110195235976218\tValidation Loss: 0.0001625680556708115\n",
      "Epoch 2449  \tTraining Loss: 0.0001610798203561027\tValidation Loss: 0.00016254669292698664\n",
      "Epoch 2450  \tTraining Loss: 0.00016105770507491408\tValidation Loss: 0.00016252534506035963\n",
      "Epoch 2451  \tTraining Loss: 0.00016103560649597025\tValidation Loss: 0.00016250401255150804\n",
      "Epoch 2452  \tTraining Loss: 0.0001610135245998945\tValidation Loss: 0.00016248269571021153\n",
      "Epoch 2453  \tTraining Loss: 0.00016099146252641753\tValidation Loss: 0.00016246156249125187\n",
      "Epoch 2454  \tTraining Loss: 0.00016096943382903778\tValidation Loss: 0.0001624403043424323\n",
      "Epoch 2455  \tTraining Loss: 0.00016094742250786522\tValidation Loss: 0.00016241905723938407\n",
      "Epoch 2456  \tTraining Loss: 0.00016092542786535425\tValidation Loss: 0.0001623978227069678\n",
      "Epoch 2457  \tTraining Loss: 0.00016090344985265057\tValidation Loss: 0.00016237660265339493\n",
      "Epoch 2458  \tTraining Loss: 0.00016088148844646893\tValidation Loss: 0.0001623553979387165\n",
      "Epoch 2459  \tTraining Loss: 0.00016085954362646802\tValidation Loss: 0.0001623342089718778\n",
      "Epoch 2460  \tTraining Loss: 0.00016083761537304055\tValidation Loss: 0.00016231303593340358\n",
      "Epoch 2461  \tTraining Loss: 0.00016081570366692627\tValidation Loss: 0.00016229187889658787\n",
      "Epoch 2462  \tTraining Loss: 0.0001607938084890962\tValidation Loss: 0.00016227073788592248\n",
      "Epoch 2463  \tTraining Loss: 0.00016077192982070286\tValidation Loss: 0.0001622496129040653\n",
      "Epoch 2464  \tTraining Loss: 0.0001607500676430548\tValidation Loss: 0.000162228503944023\n",
      "Epoch 2465  \tTraining Loss: 0.00016072822193760134\tValidation Loss: 0.00016220740425478868\n",
      "Epoch 2466  \tTraining Loss: 0.00016070639268592432\tValidation Loss: 0.00016218632004098786\n",
      "Epoch 2467  \tTraining Loss: 0.00016068457986973136\tValidation Loss: 0.00016216525183570821\n",
      "Epoch 2468  \tTraining Loss: 0.00016066278347085192\tValidation Loss: 0.00016214419962509846\n",
      "Epoch 2469  \tTraining Loss: 0.00016064100257362347\tValidation Loss: 0.00016212316438430306\n",
      "Epoch 2470  \tTraining Loss: 0.00016061923423120702\tValidation Loss: 0.00016210213877201478\n",
      "Epoch 2471  \tTraining Loss: 0.00016059748262291806\tValidation Loss: 0.00016208113094514322\n",
      "Epoch 2472  \tTraining Loss: 0.00016057574737490932\tValidation Loss: 0.0001620601397377134\n",
      "Epoch 2473  \tTraining Loss: 0.0001605540333665853\tValidation Loss: 0.0001620391269463829\n",
      "Epoch 2474  \tTraining Loss: 0.00016053233935448032\tValidation Loss: 0.00016201817599617338\n",
      "Epoch 2475  \tTraining Loss: 0.00016051065847784413\tValidation Loss: 0.00016199723936407897\n",
      "Epoch 2476  \tTraining Loss: 0.0001604889940059318\tValidation Loss: 0.00016197631533986054\n",
      "Epoch 2477  \tTraining Loss: 0.00016046734545968692\tValidation Loss: 0.00016195541253926962\n",
      "Epoch 2478  \tTraining Loss: 0.00016044570666285502\tValidation Loss: 0.0001619345191501607\n",
      "Epoch 2479  \tTraining Loss: 0.0001604240842098819\tValidation Loss: 0.00016191364233791465\n",
      "Epoch 2480  \tTraining Loss: 0.0001604024780089858\tValidation Loss: 0.00016189278520951832\n",
      "Epoch 2481  \tTraining Loss: 0.00016038088803793825\tValidation Loss: 0.00016187194247395737\n",
      "Epoch 2482  \tTraining Loss: 0.00016035931427810904\tValidation Loss: 0.0001618511146331716\n",
      "Epoch 2483  \tTraining Loss: 0.00016033775671166878\tValidation Loss: 0.00016183030200425473\n",
      "Epoch 2484  \tTraining Loss: 0.00016031621532116045\tValidation Loss: 0.00016180950478732935\n",
      "Epoch 2485  \tTraining Loss: 0.00016029469008936074\tValidation Loss: 0.00016178872310810131\n",
      "Epoch 2486  \tTraining Loss: 0.00016027318099921993\tValidation Loss: 0.00016176795704502106\n",
      "Epoch 2487  \tTraining Loss: 0.0001602516880338323\tValidation Loss: 0.00016174720664618197\n",
      "Epoch 2488  \tTraining Loss: 0.00016023021117642013\tValidation Loss: 0.00016172647193984588\n",
      "Epoch 2489  \tTraining Loss: 0.00016020875041032452\tValidation Loss: 0.00016170575294110302\n",
      "Epoch 2490  \tTraining Loss: 0.00016018730571899933\tValidation Loss: 0.00016168504965617024\n",
      "Epoch 2491  \tTraining Loss: 0.00016016587708600702\tValidation Loss: 0.00016166436208521603\n",
      "Epoch 2492  \tTraining Loss: 0.0001601444644950158\tValidation Loss: 0.00016164369022424592\n",
      "Epoch 2493  \tTraining Loss: 0.00016012306792979674\tValidation Loss: 0.00016162303406637663\n",
      "Epoch 2494  \tTraining Loss: 0.0001601016873742216\tValidation Loss: 0.00016160239360270563\n",
      "Epoch 2495  \tTraining Loss: 0.00016008032281226124\tValidation Loss: 0.00016158176882290974\n",
      "Epoch 2496  \tTraining Loss: 0.00016005897422798328\tValidation Loss: 0.00016156115971566008\n",
      "Epoch 2497  \tTraining Loss: 0.0001600376416055513\tValidation Loss: 0.0001615405662689121\n",
      "Epoch 2498  \tTraining Loss: 0.00016001632492922244\tValidation Loss: 0.00016151998847010917\n",
      "Epoch 2499  \tTraining Loss: 0.00015999502418334688\tValidation Loss: 0.00016149942630632675\n",
      "Epoch 2500  \tTraining Loss: 0.00015997373935236592\tValidation Loss: 0.0001614788797643752\n",
      "Epoch 2501  \tTraining Loss: 0.00015995247042081107\tValidation Loss: 0.0001614583488308737\n",
      "Epoch 2502  \tTraining Loss: 0.0001599312173733028\tValidation Loss: 0.00016143783349230372\n",
      "Epoch 2503  \tTraining Loss: 0.00015990998019454946\tValidation Loss: 0.00016141733373504838\n",
      "Epoch 2504  \tTraining Loss: 0.00015988875886934625\tValidation Loss: 0.00016139684954542125\n",
      "Epoch 2505  \tTraining Loss: 0.000159867553382574\tValidation Loss: 0.00016137638090968836\n",
      "Epoch 2506  \tTraining Loss: 0.00015984636371919864\tValidation Loss: 0.00016135592781408458\n",
      "Epoch 2507  \tTraining Loss: 0.00015982518986426978\tValidation Loss: 0.00016133549024482653\n",
      "Epoch 2508  \tTraining Loss: 0.00015980403180292029\tValidation Loss: 0.0001613150681881224\n",
      "Epoch 2509  \tTraining Loss: 0.000159782889520365\tValidation Loss: 0.00016129466163018\n",
      "Epoch 2510  \tTraining Loss: 0.00015976176300190012\tValidation Loss: 0.0001612742705572132\n",
      "Epoch 2511  \tTraining Loss: 0.0001597406522329026\tValidation Loss: 0.0001612538949554472\n",
      "Epoch 2512  \tTraining Loss: 0.000159719557198829\tValidation Loss: 0.00016123353481112284\n",
      "Epoch 2513  \tTraining Loss: 0.00015969847788521488\tValidation Loss: 0.00016121319011050048\n",
      "Epoch 2514  \tTraining Loss: 0.00015967741427767423\tValidation Loss: 0.000161192860839863\n",
      "Epoch 2515  \tTraining Loss: 0.00015965636636189863\tValidation Loss: 0.0001611725469855189\n",
      "Epoch 2516  \tTraining Loss: 0.00015963533412365654\tValidation Loss: 0.00016115224853380404\n",
      "Epoch 2517  \tTraining Loss: 0.00015961431754879274\tValidation Loss: 0.00016113196547108452\n",
      "Epoch 2518  \tTraining Loss: 0.00015959331662322753\tValidation Loss: 0.00016111169778375807\n",
      "Epoch 2519  \tTraining Loss: 0.00015957233133295643\tValidation Loss: 0.00016109144545825582\n",
      "Epoch 2520  \tTraining Loss: 0.0001595513616640492\tValidation Loss: 0.00016107120848104391\n",
      "Epoch 2521  \tTraining Loss: 0.0001595304076026494\tValidation Loss: 0.0001610509868386247\n",
      "Epoch 2522  \tTraining Loss: 0.00015950946913497399\tValidation Loss: 0.00016103078051753807\n",
      "Epoch 2523  \tTraining Loss: 0.0001594885462473124\tValidation Loss: 0.00016101058950436246\n",
      "Epoch 2524  \tTraining Loss: 0.0001594676389260263\tValidation Loss: 0.00016099041378571597\n",
      "Epoch 2525  \tTraining Loss: 0.00015944674715754898\tValidation Loss: 0.0001609702533482571\n",
      "Epoch 2526  \tTraining Loss: 0.00015942587089852494\tValidation Loss: 0.0001609501080335227\n",
      "Epoch 2527  \tTraining Loss: 0.00015940500998198556\tValidation Loss: 0.0001609299778509514\n",
      "Epoch 2528  \tTraining Loss: 0.00015938416457477304\tValidation Loss: 0.00016090985592676145\n",
      "Epoch 2529  \tTraining Loss: 0.00015936333466352103\tValidation Loss: 0.00016088973885910213\n",
      "Epoch 2530  \tTraining Loss: 0.00015934252023501352\tValidation Loss: 0.00016086963706680345\n",
      "Epoch 2531  \tTraining Loss: 0.00015932172127611083\tValidation Loss: 0.00016084955052042172\n",
      "Epoch 2532  \tTraining Loss: 0.00015930093777374301\tValidation Loss: 0.00016082947919866632\n",
      "Epoch 2533  \tTraining Loss: 0.00015928016971490812\tValidation Loss: 0.00016080942308485983\n",
      "Epoch 2534  \tTraining Loss: 0.00015925941708667165\tValidation Loss: 0.00016078938216465666\n",
      "Epoch 2535  \tTraining Loss: 0.000159238679876166\tValidation Loss: 0.00016076935642478477\n",
      "Epoch 2536  \tTraining Loss: 0.00015921795807058984\tValidation Loss: 0.00016074934585242508\n",
      "Epoch 2537  \tTraining Loss: 0.00015919725165720796\tValidation Loss: 0.00016072935043493086\n",
      "Epoch 2538  \tTraining Loss: 0.0001591765606233506\tValidation Loss: 0.00016070937015971222\n",
      "Epoch 2539  \tTraining Loss: 0.0001591558849564131\tValidation Loss: 0.00016068940501419635\n",
      "Epoch 2540  \tTraining Loss: 0.00015913522464385547\tValidation Loss: 0.00016066945498581888\n",
      "Epoch 2541  \tTraining Loss: 0.00015911457967320228\tValidation Loss: 0.00016064952006202726\n",
      "Epoch 2542  \tTraining Loss: 0.00015909395003204196\tValidation Loss: 0.0001606296002302876\n",
      "Epoch 2543  \tTraining Loss: 0.0001590733357080263\tValidation Loss: 0.00016060969547809163\n",
      "Epoch 2544  \tTraining Loss: 0.00015905273668887068\tValidation Loss: 0.00016058980579296235\n",
      "Epoch 2545  \tTraining Loss: 0.00015903215296235297\tValidation Loss: 0.00016056993116245874\n",
      "Epoch 2546  \tTraining Loss: 0.00015901158451631377\tValidation Loss: 0.00016055007157417925\n",
      "Epoch 2547  \tTraining Loss: 0.0001589910313386556\tValidation Loss: 0.00016053022701576435\n",
      "Epoch 2548  \tTraining Loss: 0.00015897049341734285\tValidation Loss: 0.00016051039747489834\n",
      "Epoch 2549  \tTraining Loss: 0.0001589499707404013\tValidation Loss: 0.0001604905829393109\n",
      "Epoch 2550  \tTraining Loss: 0.0001589294632959178\tValidation Loss: 0.00016047078339677794\n",
      "Epoch 2551  \tTraining Loss: 0.00015890897107203998\tValidation Loss: 0.0001604509988351225\n",
      "Epoch 2552  \tTraining Loss: 0.00015888849405697574\tValidation Loss: 0.0001604312292422152\n",
      "Epoch 2553  \tTraining Loss: 0.00015886803223899305\tValidation Loss: 0.0001604114746059747\n",
      "Epoch 2554  \tTraining Loss: 0.00015884758560641974\tValidation Loss: 0.00016039173491436788\n",
      "Epoch 2555  \tTraining Loss: 0.00015882715414764287\tValidation Loss: 0.00016037201015541014\n",
      "Epoch 2556  \tTraining Loss: 0.00015880673785110858\tValidation Loss: 0.0001603523003171656\n",
      "Epoch 2557  \tTraining Loss: 0.00015878633670532194\tValidation Loss: 0.00016033260538774718\n",
      "Epoch 2558  \tTraining Loss: 0.00015876595069884626\tValidation Loss: 0.0001603129253553166\n",
      "Epoch 2559  \tTraining Loss: 0.00015874557982030298\tValidation Loss: 0.00016029326020808445\n",
      "Epoch 2560  \tTraining Loss: 0.00015872522405837135\tValidation Loss: 0.0001602736099343104\n",
      "Epoch 2561  \tTraining Loss: 0.00015870488340178815\tValidation Loss: 0.00016025397452230293\n",
      "Epoch 2562  \tTraining Loss: 0.0001586845578393474\tValidation Loss: 0.0001602343539604195\n",
      "Epoch 2563  \tTraining Loss: 0.00015866424735989977\tValidation Loss: 0.00016021474823706657\n",
      "Epoch 2564  \tTraining Loss: 0.00015864395195235264\tValidation Loss: 0.0001601951573406994\n",
      "Epoch 2565  \tTraining Loss: 0.00015862367160566967\tValidation Loss: 0.00016017558125982226\n",
      "Epoch 2566  \tTraining Loss: 0.00015860340719624525\tValidation Loss: 0.00016015600723000043\n",
      "Epoch 2567  \tTraining Loss: 0.00015858316258246402\tValidation Loss: 0.00016013646351288515\n",
      "Epoch 2568  \tTraining Loss: 0.0001585629330943492\tValidation Loss: 0.00016011693219673357\n",
      "Epoch 2569  \tTraining Loss: 0.0001585427186314742\tValidation Loss: 0.00016009742379509255\n",
      "Epoch 2570  \tTraining Loss: 0.00015852251917737305\tValidation Loss: 0.00016007792943199605\n",
      "Epoch 2571  \tTraining Loss: 0.00015850233472019422\tValidation Loss: 0.00016005844958454855\n",
      "Epoch 2572  \tTraining Loss: 0.00015848216524883948\tValidation Loss: 0.00016003898445873734\n",
      "Epoch 2573  \tTraining Loss: 0.00015846201075245684\tValidation Loss: 0.00016001953413041642\n",
      "Epoch 2574  \tTraining Loss: 0.00015844187160481948\tValidation Loss: 0.00016000004343155516\n",
      "Epoch 2575  \tTraining Loss: 0.0001584217551475645\tValidation Loss: 0.00015998062525425662\n",
      "Epoch 2576  \tTraining Loss: 0.0001584016539518745\tValidation Loss: 0.00015996122111874367\n",
      "Epoch 2577  \tTraining Loss: 0.00015838156772056763\tValidation Loss: 0.00015994183050937338\n",
      "Epoch 2578  \tTraining Loss: 0.000158361494966999\tValidation Loss: 0.00015992245213858583\n",
      "Epoch 2579  \tTraining Loss: 0.00015834143523968275\tValidation Loss: 0.00015990308669782072\n",
      "Epoch 2580  \tTraining Loss: 0.0001583213904415415\tValidation Loss: 0.00015988373682256053\n",
      "Epoch 2581  \tTraining Loss: 0.00015830136055034787\tValidation Loss: 0.00015986440197605331\n",
      "Epoch 2582  \tTraining Loss: 0.00015828134555417175\tValidation Loss: 0.0001598450818834634\n",
      "Epoch 2583  \tTraining Loss: 0.00015826134544194486\tValidation Loss: 0.00015982577637792145\n",
      "Epoch 2584  \tTraining Loss: 0.00015824136020280074\tValidation Loss: 0.00015980648536614687\n",
      "Epoch 2585  \tTraining Loss: 0.00015822138982598568\tValidation Loss: 0.00015978720879984132\n",
      "Epoch 2586  \tTraining Loss: 0.0001582014343008374\tValidation Loss: 0.00015976794665460586\n",
      "Epoch 2587  \tTraining Loss: 0.00015818149361677687\tValidation Loss: 0.00015974869891757993\n",
      "Epoch 2588  \tTraining Loss: 0.00015816156776330334\tValidation Loss: 0.0001597294655810534\n",
      "Epoch 2589  \tTraining Loss: 0.0001581416567299919\tValidation Loss: 0.00015971024663940925\n",
      "Epoch 2590  \tTraining Loss: 0.00015812176050649111\tValidation Loss: 0.00015969104208774266\n",
      "Epoch 2591  \tTraining Loss: 0.000158101879082521\tValidation Loss: 0.00015967185192127447\n",
      "Epoch 2592  \tTraining Loss: 0.00015808201244787192\tValidation Loss: 0.00015965267613512443\n",
      "Epoch 2593  \tTraining Loss: 0.00015806216059240292\tValidation Loss: 0.00015963351472424007\n",
      "Epoch 2594  \tTraining Loss: 0.00015804232350604053\tValidation Loss: 0.0001596143676833884\n",
      "Epoch 2595  \tTraining Loss: 0.0001580225011787777\tValidation Loss: 0.0001595952350071705\n",
      "Epoch 2596  \tTraining Loss: 0.00015800269360067244\tValidation Loss: 0.00015957611669004075\n",
      "Epoch 2597  \tTraining Loss: 0.0001579829007618469\tValidation Loss: 0.00015955701272632678\n",
      "Epoch 2598  \tTraining Loss: 0.00015796312265248645\tValidation Loss: 0.00015953792311024491\n",
      "Epoch 2599  \tTraining Loss: 0.00015794335926283838\tValidation Loss: 0.000159518847835914\n",
      "Epoch 2600  \tTraining Loss: 0.0001579236105832113\tValidation Loss: 0.0001594997868973655\n",
      "Epoch 2601  \tTraining Loss: 0.000157903876603974\tValidation Loss: 0.00015948074028855201\n",
      "Epoch 2602  \tTraining Loss: 0.00015788415731555474\tValidation Loss: 0.00015946170800335406\n",
      "Epoch 2603  \tTraining Loss: 0.00015786445270844028\tValidation Loss: 0.00015944269003558557\n",
      "Epoch 2604  \tTraining Loss: 0.00015784476277317516\tValidation Loss: 0.00015942368637899832\n",
      "Epoch 2605  \tTraining Loss: 0.0001578250875003609\tValidation Loss: 0.00015940469702728589\n",
      "Epoch 2606  \tTraining Loss: 0.00015780542688065524\tValidation Loss: 0.000159385721974087\n",
      "Epoch 2607  \tTraining Loss: 0.0001577857809047713\tValidation Loss: 0.00015936676121298845\n",
      "Epoch 2608  \tTraining Loss: 0.00015776614956347705\tValidation Loss: 0.0001593478147375279\n",
      "Epoch 2609  \tTraining Loss: 0.00015774653347535954\tValidation Loss: 0.0001593289785189712\n",
      "Epoch 2610  \tTraining Loss: 0.00015772693486843233\tValidation Loss: 0.00015931008602110746\n",
      "Epoch 2611  \tTraining Loss: 0.00015770735095924253\tValidation Loss: 0.0001592911930435204\n",
      "Epoch 2612  \tTraining Loss: 0.00015768778165074863\tValidation Loss: 0.00015927231024512588\n",
      "Epoch 2613  \tTraining Loss: 0.0001576682269299109\tValidation Loss: 0.00015925344018539862\n",
      "Epoch 2614  \tTraining Loss: 0.00015764868678686613\tValidation Loss: 0.0001592345836926621\n",
      "Epoch 2615  \tTraining Loss: 0.00015762916121231412\tValidation Loss: 0.00015921574109262402\n",
      "Epoch 2616  \tTraining Loss: 0.00015760965019716812\tValidation Loss: 0.0001591969125343133\n",
      "Epoch 2617  \tTraining Loss: 0.00015759015373245796\tValidation Loss: 0.00015917809809236653\n",
      "Epoch 2618  \tTraining Loss: 0.00015757067180929485\tValidation Loss: 0.00015915929780573778\n",
      "Epoch 2619  \tTraining Loss: 0.00015755120441885795\tValidation Loss: 0.0001591405116946161\n",
      "Epoch 2620  \tTraining Loss: 0.00015753175155238693\tValidation Loss: 0.00015912173976865226\n",
      "Epoch 2621  \tTraining Loss: 0.00015751231320117892\tValidation Loss: 0.00015910298203127829\n",
      "Epoch 2622  \tTraining Loss: 0.00015749288935658622\tValidation Loss: 0.00015908423848210923\n",
      "Epoch 2623  \tTraining Loss: 0.00015747348001001474\tValidation Loss: 0.00015906550911834076\n",
      "Epoch 2624  \tTraining Loss: 0.0001574540851529227\tValidation Loss: 0.0001590467939355934\n",
      "Epoch 2625  \tTraining Loss: 0.00015743470477681996\tValidation Loss: 0.00015902809292844025\n",
      "Epoch 2626  \tTraining Loss: 0.00015741533887326666\tValidation Loss: 0.00015900940609074615\n",
      "Epoch 2627  \tTraining Loss: 0.00015739598743387266\tValidation Loss: 0.0001589907334158913\n",
      "Epoch 2628  \tTraining Loss: 0.00015737665045029674\tValidation Loss: 0.00015897207489692174\n",
      "Epoch 2629  \tTraining Loss: 0.00015735732791424565\tValidation Loss: 0.00015895343052665248\n",
      "Epoch 2630  \tTraining Loss: 0.00015733801981747357\tValidation Loss: 0.00015893480029773892\n",
      "Epoch 2631  \tTraining Loss: 0.00015731872615178146\tValidation Loss: 0.00015891618420272723\n",
      "Epoch 2632  \tTraining Loss: 0.0001572994469090162\tValidation Loss: 0.00015889758223408955\n",
      "Epoch 2633  \tTraining Loss: 0.00015728018208107029\tValidation Loss: 0.0001588789943842497\n",
      "Epoch 2634  \tTraining Loss: 0.00015726093165988075\tValidation Loss: 0.000158860420645601\n",
      "Epoch 2635  \tTraining Loss: 0.00015724169563742908\tValidation Loss: 0.00015884186101051937\n",
      "Epoch 2636  \tTraining Loss: 0.00015722247400574022\tValidation Loss: 0.000158823315471373\n",
      "Epoch 2637  \tTraining Loss: 0.0001572032667568824\tValidation Loss: 0.00015880478402052926\n",
      "Epoch 2638  \tTraining Loss: 0.00015718407388296622\tValidation Loss: 0.00015878626665035952\n",
      "Epoch 2639  \tTraining Loss: 0.00015716489537614446\tValidation Loss: 0.00015876776335324306\n",
      "Epoch 2640  \tTraining Loss: 0.00015714573122861133\tValidation Loss: 0.00015874927412157\n",
      "Epoch 2641  \tTraining Loss: 0.0001571265818866559\tValidation Loss: 0.0001587308476854937\n",
      "Epoch 2642  \tTraining Loss: 0.0001571074514652702\tValidation Loss: 0.0001587123950986207\n",
      "Epoch 2643  \tTraining Loss: 0.00015708833544322013\tValidation Loss: 0.0001586939547243032\n",
      "Epoch 2644  \tTraining Loss: 0.00015706923375623065\tValidation Loss: 0.0001586755271997682\n",
      "Epoch 2645  \tTraining Loss: 0.00015705014639413664\tValidation Loss: 0.00015865711314245982\n",
      "Epoch 2646  \tTraining Loss: 0.0001570310711523689\tValidation Loss: 0.0001586387227747028\n",
      "Epoch 2647  \tTraining Loss: 0.00015701200657900047\tValidation Loss: 0.00015862032389128014\n",
      "Epoch 2648  \tTraining Loss: 0.00015699295646329602\tValidation Loss: 0.00015860193868296406\n",
      "Epoch 2649  \tTraining Loss: 0.00015697392065762387\tValidation Loss: 0.00015858356640995982\n",
      "Epoch 2650  \tTraining Loss: 0.00015695489914780298\tValidation Loss: 0.00015856520751937448\n",
      "Epoch 2651  \tTraining Loss: 0.00015693589192474172\tValidation Loss: 0.00015854686234915427\n",
      "Epoch 2652  \tTraining Loss: 0.00015691689898013217\tValidation Loss: 0.00015852853106541458\n",
      "Epoch 2653  \tTraining Loss: 0.00015689792030593768\tValidation Loss: 0.00015851021373352851\n",
      "Epoch 2654  \tTraining Loss: 0.00015687895589426587\tValidation Loss: 0.00015849191037302018\n",
      "Epoch 2655  \tTraining Loss: 0.000156860005737323\tValidation Loss: 0.00015847362098511453\n",
      "Epoch 2656  \tTraining Loss: 0.000156841069827394\tValidation Loss: 0.00015845534556466494\n",
      "Epoch 2657  \tTraining Loss: 0.00015682214815683273\tValidation Loss: 0.00015843708410479863\n",
      "Epoch 2658  \tTraining Loss: 0.00015680324071805658\tValidation Loss: 0.000158418836598499\n",
      "Epoch 2659  \tTraining Loss: 0.00015678434750354247\tValidation Loss: 0.00015840060303901068\n",
      "Epoch 2660  \tTraining Loss: 0.00015676546850582448\tValidation Loss: 0.00015838238341984678\n",
      "Epoch 2661  \tTraining Loss: 0.00015674660371749148\tValidation Loss: 0.00015836417773469751\n",
      "Epoch 2662  \tTraining Loss: 0.00015672775313118535\tValidation Loss: 0.0001583459859773404\n",
      "Epoch 2663  \tTraining Loss: 0.00015670891673959955\tValidation Loss: 0.00015832780814157657\n",
      "Epoch 2664  \tTraining Loss: 0.00015669009453547701\tValidation Loss: 0.00015830964422119362\n",
      "Epoch 2665  \tTraining Loss: 0.0001566712865116093\tValidation Loss: 0.000158291494209947\n",
      "Epoch 2666  \tTraining Loss: 0.0001566524926608349\tValidation Loss: 0.00015827335810155344\n",
      "Epoch 2667  \tTraining Loss: 0.00015663371297603814\tValidation Loss: 0.0001582552358896909\n",
      "Epoch 2668  \tTraining Loss: 0.00015661494745014806\tValidation Loss: 0.00015823712756800188\n",
      "Epoch 2669  \tTraining Loss: 0.00015659619607613715\tValidation Loss: 0.00015821903313009803\n",
      "Epoch 2670  \tTraining Loss: 0.00015657745884702036\tValidation Loss: 0.00015820095256956462\n",
      "Epoch 2671  \tTraining Loss: 0.00015655873575585425\tValidation Loss: 0.00015818288587996504\n",
      "Epoch 2672  \tTraining Loss: 0.00015654002679573557\tValidation Loss: 0.00015816483305484413\n",
      "Epoch 2673  \tTraining Loss: 0.00015652133195980108\tValidation Loss: 0.00015814679408773116\n",
      "Epoch 2674  \tTraining Loss: 0.00015650265124122603\tValidation Loss: 0.0001581287689721426\n",
      "Epoch 2675  \tTraining Loss: 0.00015648398463322357\tValidation Loss: 0.00015811075770158357\n",
      "Epoch 2676  \tTraining Loss: 0.00015646533212904413\tValidation Loss: 0.00015809276026954966\n",
      "Epoch 2677  \tTraining Loss: 0.00015644669372197431\tValidation Loss: 0.00015807477666952803\n",
      "Epoch 2678  \tTraining Loss: 0.00015642806940533653\tValidation Loss: 0.00015805680689499852\n",
      "Epoch 2679  \tTraining Loss: 0.00015640945917248802\tValidation Loss: 0.00015803885093943436\n",
      "Epoch 2680  \tTraining Loss: 0.0001563908630168202\tValidation Loss: 0.0001580209087963032\n",
      "Epoch 2681  \tTraining Loss: 0.0001563722809317583\tValidation Loss: 0.00015800298045906736\n",
      "Epoch 2682  \tTraining Loss: 0.00015635371291076034\tValidation Loss: 0.00015798506592118485\n",
      "Epoch 2683  \tTraining Loss: 0.00015633515894731686\tValidation Loss: 0.00015796716594858868\n",
      "Epoch 2684  \tTraining Loss: 0.00015631661908159527\tValidation Loss: 0.00015794928158147477\n",
      "Epoch 2685  \tTraining Loss: 0.00015629809392402825\tValidation Loss: 0.00015793141083062027\n",
      "Epoch 2686  \tTraining Loss: 0.0001562795828008759\tValidation Loss: 0.00015791355452546513\n",
      "Epoch 2687  \tTraining Loss: 0.00015626108570260732\tValidation Loss: 0.00015789571225649417\n",
      "Epoch 2688  \tTraining Loss: 0.00015624260262255183\tValidation Loss: 0.00015787788384277415\n",
      "Epoch 2689  \tTraining Loss: 0.00015622413355426614\tValidation Loss: 0.00015786006919051392\n",
      "Epoch 2690  \tTraining Loss: 0.00015620567837779686\tValidation Loss: 0.00015784227736853387\n",
      "Epoch 2691  \tTraining Loss: 0.00015618723353515228\tValidation Loss: 0.00015782449379183507\n",
      "Epoch 2692  \tTraining Loss: 0.00015616880277776634\tValidation Loss: 0.0001578067199619201\n",
      "Epoch 2693  \tTraining Loss: 0.00015615038602929441\tValidation Loss: 0.0001577889579972424\n",
      "Epoch 2694  \tTraining Loss: 0.0001561319832790178\tValidation Loss: 0.00015777120854373593\n",
      "Epoch 2695  \tTraining Loss: 0.00015611359451938851\tValidation Loss: 0.00015775347200369994\n",
      "Epoch 2696  \tTraining Loss: 0.00015609521974355498\tValidation Loss: 0.00015773574863524157\n",
      "Epoch 2697  \tTraining Loss: 0.00015607685894495558\tValidation Loss: 0.00015771803860395106\n",
      "Epoch 2698  \tTraining Loss: 0.0001560585121171788\tValidation Loss: 0.00015770034201618155\n",
      "Epoch 2699  \tTraining Loss: 0.00015604017925390114\tValidation Loss: 0.00015768265894049565\n",
      "Epoch 2700  \tTraining Loss: 0.00015602186034885743\tValidation Loss: 0.0001576649894210329\n",
      "Epoch 2701  \tTraining Loss: 0.00015600355539582595\tValidation Loss: 0.00015764733348586259\n",
      "Epoch 2702  \tTraining Loss: 0.00015598526438862078\tValidation Loss: 0.00015762969115230016\n",
      "Epoch 2703  \tTraining Loss: 0.00015596698732108758\tValidation Loss: 0.00015761206243036622\n",
      "Epoch 2704  \tTraining Loss: 0.00015594872418710096\tValidation Loss: 0.00015759444732508065\n",
      "Epoch 2705  \tTraining Loss: 0.00015593047498056335\tValidation Loss: 0.00015757684583800775\n",
      "Epoch 2706  \tTraining Loss: 0.00015591223969540363\tValidation Loss: 0.0001575592579683096\n",
      "Epoch 2707  \tTraining Loss: 0.00015589401832557636\tValidation Loss: 0.0001575416837134704\n",
      "Epoch 2708  \tTraining Loss: 0.0001558758108650614\tValidation Loss: 0.00015752412306979844\n",
      "Epoch 2709  \tTraining Loss: 0.00015585761730786313\tValidation Loss: 0.0001575065760327757\n",
      "Epoch 2710  \tTraining Loss: 0.00015583943764801\tValidation Loss: 0.0001574890425973025\n",
      "Epoch 2711  \tTraining Loss: 0.0001558212718795541\tValidation Loss: 0.00015747152275786977\n",
      "Epoch 2712  \tTraining Loss: 0.00015580311999657088\tValidation Loss: 0.00015745401650868026\n",
      "Epoch 2713  \tTraining Loss: 0.00015578498199315827\tValidation Loss: 0.00015743652384373445\n",
      "Epoch 2714  \tTraining Loss: 0.00015576685786343675\tValidation Loss: 0.00015741904475689172\n",
      "Epoch 2715  \tTraining Loss: 0.0001557487488179858\tValidation Loss: 0.00015740161504198657\n",
      "Epoch 2716  \tTraining Loss: 0.0001557306564577695\tValidation Loss: 0.00015738418054806277\n",
      "Epoch 2717  \tTraining Loss: 0.0001557125779894711\tValidation Loss: 0.00015736678235073865\n",
      "Epoch 2718  \tTraining Loss: 0.00015569451337957388\tValidation Loss: 0.00015734939679741997\n",
      "Epoch 2719  \tTraining Loss: 0.0001556764626210167\tValidation Loss: 0.0001573320243320411\n",
      "Epoch 2720  \tTraining Loss: 0.00015565842570775047\tValidation Loss: 0.00015731466514694857\n",
      "Epoch 2721  \tTraining Loss: 0.00015564040263386198\tValidation Loss: 0.00015729731932803863\n",
      "Epoch 2722  \tTraining Loss: 0.00015562239339348744\tValidation Loss: 0.00015727998690967152\n",
      "Epoch 2723  \tTraining Loss: 0.00015560439798079618\tValidation Loss: 0.00015726266790307454\n",
      "Epoch 2724  \tTraining Loss: 0.00015558641638998487\tValidation Loss: 0.00015724536230943688\n",
      "Epoch 2725  \tTraining Loss: 0.00015556844861527535\tValidation Loss: 0.00015722807012574522\n",
      "Epoch 2726  \tTraining Loss: 0.0001555504946509128\tValidation Loss: 0.00015721079134732368\n",
      "Epoch 2727  \tTraining Loss: 0.000155532554491165\tValidation Loss: 0.000157193525968914\n",
      "Epoch 2728  \tTraining Loss: 0.00015551462813032107\tValidation Loss: 0.00015717627023155092\n",
      "Epoch 2729  \tTraining Loss: 0.00015549671556269151\tValidation Loss: 0.00015715902597233851\n",
      "Epoch 2730  \tTraining Loss: 0.0001554788167826072\tValidation Loss: 0.00015714179509415796\n",
      "Epoch 2731  \tTraining Loss: 0.00015546093178441894\tValidation Loss: 0.0001571245775917649\n",
      "Epoch 2732  \tTraining Loss: 0.00015544306056249727\tValidation Loss: 0.00015710737345997987\n",
      "Epoch 2733  \tTraining Loss: 0.0001554252031112318\tValidation Loss: 0.0001570901826936714\n",
      "Epoch 2734  \tTraining Loss: 0.00015540735942503077\tValidation Loss: 0.0001570730052877406\n",
      "Epoch 2735  \tTraining Loss: 0.00015538952949832084\tValidation Loss: 0.00015705584123710842\n",
      "Epoch 2736  \tTraining Loss: 0.00015537171332554657\tValidation Loss: 0.0001570386905367052\n",
      "Epoch 2737  \tTraining Loss: 0.00015535391090117006\tValidation Loss: 0.00015702155318146385\n",
      "Epoch 2738  \tTraining Loss: 0.00015533612221967053\tValidation Loss: 0.0001570044291663136\n",
      "Epoch 2739  \tTraining Loss: 0.00015531834727554406\tValidation Loss: 0.00015698731848617665\n",
      "Epoch 2740  \tTraining Loss: 0.0001553005860633032\tValidation Loss: 0.00015697022113596526\n",
      "Epoch 2741  \tTraining Loss: 0.00015528283857747642\tValidation Loss: 0.00015695313711058012\n",
      "Epoch 2742  \tTraining Loss: 0.0001552651048126081\tValidation Loss: 0.00015693606640490904\n",
      "Epoch 2743  \tTraining Loss: 0.00015524738486037047\tValidation Loss: 0.0001569190086577139\n",
      "Epoch 2744  \tTraining Loss: 0.00015522967878137543\tValidation Loss: 0.0001569019643133759\n",
      "Epoch 2745  \tTraining Loss: 0.00015521198640509025\tValidation Loss: 0.000156884932898148\n",
      "Epoch 2746  \tTraining Loss: 0.00015519430772508808\tValidation Loss: 0.00015686791463132967\n",
      "Epoch 2747  \tTraining Loss: 0.0001551766427359253\tValidation Loss: 0.00015685090960367538\n",
      "Epoch 2748  \tTraining Loss: 0.00015515899143223006\tValidation Loss: 0.0001568339178571886\n",
      "Epoch 2749  \tTraining Loss: 0.00015514135380865053\tValidation Loss: 0.00015681693940742324\n",
      "Epoch 2750  \tTraining Loss: 0.00015512372985984998\tValidation Loss: 0.00015679997425616367\n",
      "Epoch 2751  \tTraining Loss: 0.00015510611958050496\tValidation Loss: 0.0001567830223990084\n",
      "Epoch 2752  \tTraining Loss: 0.00015508852296530504\tValidation Loss: 0.00015676608382923129\n",
      "Epoch 2753  \tTraining Loss: 0.00015507094000895212\tValidation Loss: 0.00015674915853951203\n",
      "Epoch 2754  \tTraining Loss: 0.00015505337070616034\tValidation Loss: 0.00015673224652261382\n",
      "Epoch 2755  \tTraining Loss: 0.00015503581505165557\tValidation Loss: 0.0001567153477715919\n",
      "Epoch 2756  \tTraining Loss: 0.00015501827304017523\tValidation Loss: 0.00015669846227981315\n",
      "Epoch 2757  \tTraining Loss: 0.0001550007446664679\tValidation Loss: 0.00015668159004091143\n",
      "Epoch 2758  \tTraining Loss: 0.00015498322992529335\tValidation Loss: 0.0001566647310487302\n",
      "Epoch 2759  \tTraining Loss: 0.0001549657288114218\tValidation Loss: 0.00015664788529726992\n",
      "Epoch 2760  \tTraining Loss: 0.00015494824131963403\tValidation Loss: 0.00015663105278064597\n",
      "Epoch 2761  \tTraining Loss: 0.000154930767444721\tValidation Loss: 0.00015661423349305696\n",
      "Epoch 2762  \tTraining Loss: 0.00015491330718148366\tValidation Loss: 0.00015659742742876116\n",
      "Epoch 2763  \tTraining Loss: 0.0001548958605247325\tValidation Loss: 0.0001565806345820598\n",
      "Epoch 2764  \tTraining Loss: 0.00015487842746928756\tValidation Loss: 0.00015656385494728498\n",
      "Epoch 2765  \tTraining Loss: 0.00015486100800997805\tValidation Loss: 0.00015654708851879103\n",
      "Epoch 2766  \tTraining Loss: 0.00015484360214164207\tValidation Loss: 0.0001565303352909487\n",
      "Epoch 2767  \tTraining Loss: 0.0001548262098591264\tValidation Loss: 0.0001565135952581406\n",
      "Epoch 2768  \tTraining Loss: 0.0001548088311572864\tValidation Loss: 0.00015649686841475853\n",
      "Epoch 2769  \tTraining Loss: 0.0001547914660309856\tValidation Loss: 0.00015648015475520113\n",
      "Epoch 2770  \tTraining Loss: 0.0001547741144750955\tValidation Loss: 0.00015646345427387245\n",
      "Epoch 2771  \tTraining Loss: 0.00015475677648449528\tValidation Loss: 0.0001564467669651809\n",
      "Epoch 2772  \tTraining Loss: 0.0001547394520540719\tValidation Loss: 0.0001564300928235385\n",
      "Epoch 2773  \tTraining Loss: 0.0001547221411787194\tValidation Loss: 0.00015641343184336036\n",
      "Epoch 2774  \tTraining Loss: 0.00015470484385333893\tValidation Loss: 0.00015639678401906421\n",
      "Epoch 2775  \tTraining Loss: 0.00015468756007283862\tValidation Loss: 0.0001563801493450703\n",
      "Epoch 2776  \tTraining Loss: 0.00015467028983213318\tValidation Loss: 0.00015636352781580093\n",
      "Epoch 2777  \tTraining Loss: 0.00015465303312614366\tValidation Loss: 0.00015634691942568058\n",
      "Epoch 2778  \tTraining Loss: 0.00015463578994979744\tValidation Loss: 0.00015633032416913548\n",
      "Epoch 2779  \tTraining Loss: 0.00015461856029802776\tValidation Loss: 0.00015631374204059387\n",
      "Epoch 2780  \tTraining Loss: 0.0001546013441657737\tValidation Loss: 0.00015629717303448565\n",
      "Epoch 2781  \tTraining Loss: 0.00015458414154797992\tValidation Loss: 0.00015628061714524231\n",
      "Epoch 2782  \tTraining Loss: 0.00015456694959906214\tValidation Loss: 0.0001562640720301595\n",
      "Epoch 2783  \tTraining Loss: 0.00015454976487896255\tValidation Loss: 0.0001562475338957998\n",
      "Epoch 2784  \tTraining Loss: 0.0001545325937121255\tValidation Loss: 0.00015623101272228202\n",
      "Epoch 2785  \tTraining Loss: 0.00015451543603861971\tValidation Loss: 0.00015621450616771736\n",
      "Epoch 2786  \tTraining Loss: 0.00015449829184962691\tValidation Loss: 0.00015619801334649378\n",
      "Epoch 2787  \tTraining Loss: 0.00015448116113937733\tValidation Loss: 0.00015618153384036967\n",
      "Epoch 2788  \tTraining Loss: 0.0001544640439025929\tValidation Loss: 0.00015616506747028822\n",
      "Epoch 2789  \tTraining Loss: 0.00015444694013414477\tValidation Loss: 0.00015614861417032492\n",
      "Epoch 2790  \tTraining Loss: 0.00015442984982896998\tValidation Loss: 0.00015613217392250014\n",
      "Epoch 2791  \tTraining Loss: 0.00015441277298203985\tValidation Loss: 0.00015611574672627401\n",
      "Epoch 2792  \tTraining Loss: 0.00015439570958834632\tValidation Loss: 0.000156099332585591\n",
      "Epoch 2793  \tTraining Loss: 0.0001543786596428949\tValidation Loss: 0.00015608293150397526\n",
      "Epoch 2794  \tTraining Loss: 0.0001543616231407008\tValidation Loss: 0.0001560665434830519\n",
      "Epoch 2795  \tTraining Loss: 0.00015434460007678684\tValidation Loss: 0.000156050168522404\n",
      "Epoch 2796  \tTraining Loss: 0.0001543275904461824\tValidation Loss: 0.0001560338066198761\n",
      "Epoch 2797  \tTraining Loss: 0.0001543105942439223\tValidation Loss: 0.0001560174577719662\n",
      "Epoch 2798  \tTraining Loss: 0.00015429361146504622\tValidation Loss: 0.00015600112197418174\n",
      "Epoch 2799  \tTraining Loss: 0.0001542766421045981\tValidation Loss: 0.0001559847992213237\n",
      "Epoch 2800  \tTraining Loss: 0.00015425968615762593\tValidation Loss: 0.00015596848950770117\n",
      "Epoch 2801  \tTraining Loss: 0.00015424274361918117\tValidation Loss: 0.00015595219282728783\n",
      "Epoch 2802  \tTraining Loss: 0.0001542258144843184\tValidation Loss: 0.00015593590917383374\n",
      "Epoch 2803  \tTraining Loss: 0.00015420889874809504\tValidation Loss: 0.00015591963854094472\n",
      "Epoch 2804  \tTraining Loss: 0.0001541919964055712\tValidation Loss: 0.0001559033809221372\n",
      "Epoch 2805  \tTraining Loss: 0.00015417510745180914\tValidation Loss: 0.00015588713631087663\n",
      "Epoch 2806  \tTraining Loss: 0.00015415823188187307\tValidation Loss: 0.00015587090470060394\n",
      "Epoch 2807  \tTraining Loss: 0.00015414136969082906\tValidation Loss: 0.00015585468608475338\n",
      "Epoch 2808  \tTraining Loss: 0.00015412452087374456\tValidation Loss: 0.00015583848045676465\n",
      "Epoch 2809  \tTraining Loss: 0.0001541076854256882\tValidation Loss: 0.00015582228781009082\n",
      "Epoch 2810  \tTraining Loss: 0.0001540908633417296\tValidation Loss: 0.0001558061081382038\n",
      "Epoch 2811  \tTraining Loss: 0.0001540740546169393\tValidation Loss: 0.00015578994143459736\n",
      "Epoch 2812  \tTraining Loss: 0.0001540572592463882\tValidation Loss: 0.00015577378769278912\n",
      "Epoch 2813  \tTraining Loss: 0.0001540404772251474\tValidation Loss: 0.00015575764690632142\n",
      "Epoch 2814  \tTraining Loss: 0.0001540237085482884\tValidation Loss: 0.0001557415190687619\n",
      "Epoch 2815  \tTraining Loss: 0.00015400695321088223\tValidation Loss: 0.00015572540417370322\n",
      "Epoch 2816  \tTraining Loss: 0.00015399021120799983\tValidation Loss: 0.00015570930221476287\n",
      "Epoch 2817  \tTraining Loss: 0.00015397348253471153\tValidation Loss: 0.00015569321318558246\n",
      "Epoch 2818  \tTraining Loss: 0.00015395676718608678\tValidation Loss: 0.0001556771370798271\n",
      "Epoch 2819  \tTraining Loss: 0.0001539400651571943\tValidation Loss: 0.00015566107389118486\n",
      "Epoch 2820  \tTraining Loss: 0.0001539233764431015\tValidation Loss: 0.0001556450236133658\n",
      "Epoch 2821  \tTraining Loss: 0.00015390670103887448\tValidation Loss: 0.0001556289862401012\n",
      "Epoch 2822  \tTraining Loss: 0.00015389003893957794\tValidation Loss: 0.0001556129617651429\n",
      "Epoch 2823  \tTraining Loss: 0.00015387339029941316\tValidation Loss: 0.00015559693172183996\n",
      "Epoch 2824  \tTraining Loss: 0.00015385675460647386\tValidation Loss: 0.000155580904114839\n",
      "Epoch 2825  \tTraining Loss: 0.00015384011640349853\tValidation Loss: 0.00015556489523976504\n",
      "Epoch 2826  \tTraining Loss: 0.0001538234916201213\tValidation Loss: 0.00015554890608045384\n",
      "Epoch 2827  \tTraining Loss: 0.00015380688014489464\tValidation Loss: 0.000155532932426978\n",
      "Epoch 2828  \tTraining Loss: 0.00015379028240752226\tValidation Loss: 0.0001555169614722382\n",
      "Epoch 2829  \tTraining Loss: 0.00015377369974063512\tValidation Loss: 0.00015550101669598452\n",
      "Epoch 2830  \tTraining Loss: 0.00015375713041534996\tValidation Loss: 0.00015548508424808818\n",
      "Epoch 2831  \tTraining Loss: 0.0001537405743716124\tValidation Loss: 0.00015546916485555042\n",
      "Epoch 2832  \tTraining Loss: 0.0001537240316015752\tValidation Loss: 0.0001554532584070389\n",
      "Epoch 2833  \tTraining Loss: 0.00015370750209946286\tValidation Loss: 0.00015543736479461644\n",
      "Epoch 2834  \tTraining Loss: 0.00015369098585985992\tValidation Loss: 0.00015542148397430225\n",
      "Epoch 2835  \tTraining Loss: 0.00015367448287747804\tValidation Loss: 0.00015540561593739295\n",
      "Epoch 2836  \tTraining Loss: 0.00015365799314708844\tValidation Loss: 0.00015538976068702398\n",
      "Epoch 2837  \tTraining Loss: 0.00015364151666349477\tValidation Loss: 0.00015537391822769642\n",
      "Epoch 2838  \tTraining Loss: 0.0001536250534215204\tValidation Loss: 0.00015535808856182018\n",
      "Epoch 2839  \tTraining Loss: 0.00015360860341600188\tValidation Loss: 0.00015534227168915966\n",
      "Epoch 2840  \tTraining Loss: 0.00015359216664178498\tValidation Loss: 0.00015532646760724146\n",
      "Epoch 2841  \tTraining Loss: 0.0001535757430937228\tValidation Loss: 0.0001553106763119479\n",
      "Epoch 2842  \tTraining Loss: 0.00015355933154584015\tValidation Loss: 0.00015529489399598433\n",
      "Epoch 2843  \tTraining Loss: 0.0001535429289262795\tValidation Loss: 0.0001552791252842724\n",
      "Epoch 2844  \tTraining Loss: 0.00015352653954746464\tValidation Loss: 0.000155263371594313\n",
      "Epoch 2845  \tTraining Loss: 0.0001535101633756525\tValidation Loss: 0.00015524763156481356\n",
      "Epoch 2846  \tTraining Loss: 0.0001534938004038266\tValidation Loss: 0.00015523190462980098\n",
      "Epoch 2847  \tTraining Loss: 0.00015347745062656932\tValidation Loss: 0.0001552161905176065\n",
      "Epoch 2848  \tTraining Loss: 0.00015346111403868014\tValidation Loss: 0.00015520048910663622\n",
      "Epoch 2849  \tTraining Loss: 0.0001534447906350151\tValidation Loss: 0.00015518479955484624\n",
      "Epoch 2850  \tTraining Loss: 0.00015342848041045179\tValidation Loss: 0.00015516912105941014\n",
      "Epoch 2851  \tTraining Loss: 0.00015341218335987762\tValidation Loss: 0.0001551534552091252\n",
      "Epoch 2852  \tTraining Loss: 0.00015339589947818355\tValidation Loss: 0.00015513780200591032\n",
      "Epoch 2853  \tTraining Loss: 0.0001533796287602617\tValidation Loss: 0.00015512216145132531\n",
      "Epoch 2854  \tTraining Loss: 0.00015336337120100331\tValidation Loss: 0.0001551065335455051\n",
      "Epoch 2855  \tTraining Loss: 0.00015334712679529802\tValidation Loss: 0.00015509091828705982\n",
      "Epoch 2856  \tTraining Loss: 0.00015333089553803286\tValidation Loss: 0.00015507531567329915\n",
      "Epoch 2857  \tTraining Loss: 0.0001533146774240918\tValidation Loss: 0.00015505972570052155\n",
      "Epoch 2858  \tTraining Loss: 0.00015329847244835538\tValidation Loss: 0.00015504414836427563\n",
      "Epoch 2859  \tTraining Loss: 0.00015328228060570016\tValidation Loss: 0.00015502858365956906\n",
      "Epoch 2860  \tTraining Loss: 0.00015326610189099853\tValidation Loss: 0.00015501303158102652\n",
      "Epoch 2861  \tTraining Loss: 0.00015324993629911834\tValidation Loss: 0.0001549974921230051\n",
      "Epoch 2862  \tTraining Loss: 0.0001532337838249225\tValidation Loss: 0.00015498196527967688\n",
      "Epoch 2863  \tTraining Loss: 0.00015321764446326876\tValidation Loss: 0.00015496645104508836\n",
      "Epoch 2864  \tTraining Loss: 0.00015320151820900953\tValidation Loss: 0.00015495094941320138\n",
      "Epoch 2865  \tTraining Loss: 0.00015318540505699166\tValidation Loss: 0.00015493546037792267\n",
      "Epoch 2866  \tTraining Loss: 0.00015316930500205582\tValidation Loss: 0.00015491998393312386\n",
      "Epoch 2867  \tTraining Loss: 0.0001531532180390367\tValidation Loss: 0.00015490452007265582\n",
      "Epoch 2868  \tTraining Loss: 0.0001531371441627626\tValidation Loss: 0.00015488906879035822\n",
      "Epoch 2869  \tTraining Loss: 0.0001531210842544979\tValidation Loss: 0.0001548736310918594\n",
      "Epoch 2870  \tTraining Loss: 0.0001531050390243965\tValidation Loss: 0.00015485820540932216\n",
      "Epoch 2871  \tTraining Loss: 0.00015308900685229136\tValidation Loss: 0.00015484279120077352\n",
      "Epoch 2872  \tTraining Loss: 0.00015307298772691743\tValidation Loss: 0.0001548273891221152\n",
      "Epoch 2873  \tTraining Loss: 0.00015305698164274413\tValidation Loss: 0.00015481199943594636\n",
      "Epoch 2874  \tTraining Loss: 0.00015304098859455137\tValidation Loss: 0.00015479662226161082\n",
      "Epoch 2875  \tTraining Loss: 0.0001530250062626373\tValidation Loss: 0.00015478124870544133\n",
      "Epoch 2876  \tTraining Loss: 0.00015300902666624304\tValidation Loss: 0.0001547658891631405\n",
      "Epoch 2877  \tTraining Loss: 0.00015299306016359083\tValidation Loss: 0.00015475054588273696\n",
      "Epoch 2878  \tTraining Loss: 0.00015297710668019178\tValidation Loss: 0.0001547352166086494\n",
      "Epoch 2879  \tTraining Loss: 0.000152961166206251\tValidation Loss: 0.00015471990044478129\n",
      "Epoch 2880  \tTraining Loss: 0.00015294523871848593\tValidation Loss: 0.000154704599463381\n",
      "Epoch 2881  \tTraining Loss: 0.00015292932360664783\tValidation Loss: 0.00015468930580717216\n",
      "Epoch 2882  \tTraining Loss: 0.0001529134214937113\tValidation Loss: 0.000154674024663575\n",
      "Epoch 2883  \tTraining Loss: 0.00015289753236706215\tValidation Loss: 0.00015465875579310733\n",
      "Epoch 2884  \tTraining Loss: 0.00015288165622109355\tValidation Loss: 0.00015464349927184803\n",
      "Epoch 2885  \tTraining Loss: 0.00015286579308070025\tValidation Loss: 0.00015462825535944753\n",
      "Epoch 2886  \tTraining Loss: 0.00015284994335955411\tValidation Loss: 0.00015461302364521332\n",
      "Epoch 2887  \tTraining Loss: 0.00015283410660153356\tValidation Loss: 0.0001545978040278793\n",
      "Epoch 2888  \tTraining Loss: 0.00015281828280072531\tValidation Loss: 0.000154582596740242\n",
      "Epoch 2889  \tTraining Loss: 0.00015280247195176503\tValidation Loss: 0.0001545674018713264\n",
      "Epoch 2890  \tTraining Loss: 0.00015278667404931202\tValidation Loss: 0.0001545522194586806\n",
      "Epoch 2891  \tTraining Loss: 0.00015277088908802376\tValidation Loss: 0.0001545370495146619\n",
      "Epoch 2892  \tTraining Loss: 0.0001527551170625532\tValidation Loss: 0.00015452189203928661\n",
      "Epoch 2893  \tTraining Loss: 0.00015273935796754785\tValidation Loss: 0.00015450674702712744\n",
      "Epoch 2894  \tTraining Loss: 0.0001527236117976496\tValidation Loss: 0.00015449161447063711\n",
      "Epoch 2895  \tTraining Loss: 0.00015270787854749392\tValidation Loss: 0.00015447649436160798\n",
      "Epoch 2896  \tTraining Loss: 0.00015269215821170972\tValidation Loss: 0.00015446138669175427\n",
      "Epoch 2897  \tTraining Loss: 0.0001526764507849188\tValidation Loss: 0.0001544462914529126\n",
      "Epoch 2898  \tTraining Loss: 0.00015266075626173593\tValidation Loss: 0.00015443120863708768\n",
      "Epoch 2899  \tTraining Loss: 0.0001526450746367681\tValidation Loss: 0.00015441613823644372\n",
      "Epoch 2900  \tTraining Loss: 0.00015262940590461444\tValidation Loss: 0.00015440108024327963\n",
      "Epoch 2901  \tTraining Loss: 0.000152613750059866\tValidation Loss: 0.00015438603465000436\n",
      "Epoch 2902  \tTraining Loss: 0.00015259810709710532\tValidation Loss: 0.00015437100144911618\n",
      "Epoch 2903  \tTraining Loss: 0.00015258247701090637\tValidation Loss: 0.00015435598063318667\n",
      "Epoch 2904  \tTraining Loss: 0.00015256685979583411\tValidation Loss: 0.00015434097219484892\n",
      "Epoch 2905  \tTraining Loss: 0.0001525512554464445\tValidation Loss: 0.00015432597612678858\n",
      "Epoch 2906  \tTraining Loss: 0.00015253566395728404\tValidation Loss: 0.00015431099242173746\n",
      "Epoch 2907  \tTraining Loss: 0.00015252008532288958\tValidation Loss: 0.00015429602107246864\n",
      "Epoch 2908  \tTraining Loss: 0.0001525045195377883\tValidation Loss: 0.00015428106207179267\n",
      "Epoch 2909  \tTraining Loss: 0.0001524889665964973\tValidation Loss: 0.00015426611541255456\n",
      "Epoch 2910  \tTraining Loss: 0.00015247342649352352\tValidation Loss: 0.00015425118108763146\n",
      "Epoch 2911  \tTraining Loss: 0.00015245789922336352\tValidation Loss: 0.00015423625908993059\n",
      "Epoch 2912  \tTraining Loss: 0.00015244238478050328\tValidation Loss: 0.00015422134941238746\n",
      "Epoch 2913  \tTraining Loss: 0.00015242688315941793\tValidation Loss: 0.00015420645204796446\n",
      "Epoch 2914  \tTraining Loss: 0.00015241139435457184\tValidation Loss: 0.00015419156698964927\n",
      "Epoch 2915  \tTraining Loss: 0.00015239591836041811\tValidation Loss: 0.00015417669423045365\n",
      "Epoch 2916  \tTraining Loss: 0.0001523804551713987\tValidation Loss: 0.00015416183376341244\n",
      "Epoch 2917  \tTraining Loss: 0.00015236500478194414\tValidation Loss: 0.00015414698558158208\n",
      "Epoch 2918  \tTraining Loss: 0.00015234956718647327\tValidation Loss: 0.00015413214967803976\n",
      "Epoch 2919  \tTraining Loss: 0.00015233414237939333\tValidation Loss: 0.00015411732604588232\n",
      "Epoch 2920  \tTraining Loss: 0.00015231873034416628\tValidation Loss: 0.00015410251497307293\n",
      "Epoch 2921  \tTraining Loss: 0.00015230333106374517\tValidation Loss: 0.00015408771579595177\n",
      "Epoch 2922  \tTraining Loss: 0.00015228794455518925\tValidation Loss: 0.00015407292888449411\n",
      "Epoch 2923  \tTraining Loss: 0.0001522725708128178\tValidation Loss: 0.00015405815420821312\n",
      "Epoch 2924  \tTraining Loss: 0.00015225720983097474\tValidation Loss: 0.00015404339176192415\n",
      "Epoch 2925  \tTraining Loss: 0.00015224186160399243\tValidation Loss: 0.00015402864154176774\n",
      "Epoch 2926  \tTraining Loss: 0.0001522265261261908\tValidation Loss: 0.00015401390354260157\n",
      "Epoch 2927  \tTraining Loss: 0.00015221120339187704\tValidation Loss: 0.0001539991777583819\n",
      "Epoch 2928  \tTraining Loss: 0.00015219589339534554\tValidation Loss: 0.0001539844641826611\n",
      "Epoch 2929  \tTraining Loss: 0.00015218059613087743\tValidation Loss: 0.00015396976280884262\n",
      "Epoch 2930  \tTraining Loss: 0.00015216531159274087\tValidation Loss: 0.00015395507363028608\n",
      "Epoch 2931  \tTraining Loss: 0.0001521500397751909\tValidation Loss: 0.0001539403966403438\n",
      "Epoch 2932  \tTraining Loss: 0.0001521347806724689\tValidation Loss: 0.00015392573183237106\n",
      "Epoch 2933  \tTraining Loss: 0.00015211953427880296\tValidation Loss: 0.00015391107919972733\n",
      "Epoch 2934  \tTraining Loss: 0.00015210430048191544\tValidation Loss: 0.00015389643586218986\n",
      "Epoch 2935  \tTraining Loss: 0.00015208907924665113\tValidation Loss: 0.0001538818076467067\n",
      "Epoch 2936  \tTraining Loss: 0.00015207387070670597\tValidation Loss: 0.00015386719204078027\n",
      "Epoch 2937  \tTraining Loss: 0.00015205867485358702\tValidation Loss: 0.00015385258865511207\n",
      "Epoch 2938  \tTraining Loss: 0.00015204349168127754\tValidation Loss: 0.00015383799745544334\n",
      "Epoch 2939  \tTraining Loss: 0.00015202832118387847\tValidation Loss: 0.00015382341843552132\n",
      "Epoch 2940  \tTraining Loss: 0.00015201316335549978\tValidation Loss: 0.0001538088515802756\n",
      "Epoch 2941  \tTraining Loss: 0.00015199801819024478\tValidation Loss: 0.00015379429687098077\n",
      "Epoch 2942  \tTraining Loss: 0.00015198288568220615\tValidation Loss: 0.00015377975428957898\n",
      "Epoch 2943  \tTraining Loss: 0.00015196776582546378\tValidation Loss: 0.00015376522382024276\n",
      "Epoch 2944  \tTraining Loss: 0.0001519526586140844\tValidation Loss: 0.0001537507054494993\n",
      "Epoch 2945  \tTraining Loss: 0.00015193756404212067\tValidation Loss: 0.00015373619916585063\n",
      "Epoch 2946  \tTraining Loss: 0.00015192248210361137\tValidation Loss: 0.000153721704959307\n",
      "Epoch 2947  \tTraining Loss: 0.00015190741279258072\tValidation Loss: 0.0001537072228209798\n",
      "Epoch 2948  \tTraining Loss: 0.0001518923548448918\tValidation Loss: 0.00015369276351642934\n",
      "Epoch 2949  \tTraining Loss: 0.00015187730797698316\tValidation Loss: 0.00015367830429370298\n",
      "Epoch 2950  \tTraining Loss: 0.00015186227374764503\tValidation Loss: 0.00015366385719709017\n",
      "Epoch 2951  \tTraining Loss: 0.00015184725212495027\tValidation Loss: 0.00015364942249631412\n",
      "Epoch 2952  \tTraining Loss: 0.0001518322431020169\tValidation Loss: 0.0001536350000439555\n",
      "Epoch 2953  \tTraining Loss: 0.00015181724667260557\tValidation Loss: 0.0001536205897199144\n",
      "Epoch 2954  \tTraining Loss: 0.00015180226283056196\tValidation Loss: 0.00015360619146298504\n",
      "Epoch 2955  \tTraining Loss: 0.0001517872915697421\tValidation Loss: 0.00015359180524603444\n",
      "Epoch 2956  \tTraining Loss: 0.00015177233288399654\tValidation Loss: 0.0001535774310576509\n",
      "Epoch 2957  \tTraining Loss: 0.000151757386767166\tValidation Loss: 0.0001535630688924209\n",
      "Epoch 2958  \tTraining Loss: 0.00015174245321307944\tValidation Loss: 0.0001535487187465459\n",
      "Epoch 2959  \tTraining Loss: 0.00015172753221555336\tValidation Loss: 0.0001535343806161253\n",
      "Epoch 2960  \tTraining Loss: 0.00015171261921479283\tValidation Loss: 0.00015352005869881375\n",
      "Epoch 2961  \tTraining Loss: 0.00015169771109905684\tValidation Loss: 0.00015350574170780603\n",
      "Epoch 2962  \tTraining Loss: 0.0001516828158211472\tValidation Loss: 0.00015349143128086\n",
      "Epoch 2963  \tTraining Loss: 0.0001516679331375312\tValidation Loss: 0.00015347713028798646\n",
      "Epoch 2964  \tTraining Loss: 0.00015165306302712052\tValidation Loss: 0.00015346283955262587\n",
      "Epoch 2965  \tTraining Loss: 0.00015163820555524798\tValidation Loss: 0.00015344855931591432\n",
      "Epoch 2966  \tTraining Loss: 0.00015162336098071156\tValidation Loss: 0.00015343429057173847\n",
      "Epoch 2967  \tTraining Loss: 0.00015160852914565296\tValidation Loss: 0.00015342012132272607\n",
      "Epoch 2968  \tTraining Loss: 0.00015159371207465355\tValidation Loss: 0.0001534058983807865\n",
      "Epoch 2969  \tTraining Loss: 0.00015157890823687224\tValidation Loss: 0.00015339168299071917\n",
      "Epoch 2970  \tTraining Loss: 0.00015156411693554038\tValidation Loss: 0.00015337747635852658\n",
      "Epoch 2971  \tTraining Loss: 0.00015154933815419595\tValidation Loss: 0.00015336328012587744\n",
      "Epoch 2972  \tTraining Loss: 0.00015153457188544256\tValidation Loss: 0.00015334909494792363\n",
      "Epoch 2973  \tTraining Loss: 0.00015151981812225494\tValidation Loss: 0.00015333492113215613\n",
      "Epoch 2974  \tTraining Loss: 0.00015150507685770647\tValidation Loss: 0.00015332075883546333\n",
      "Epoch 2975  \tTraining Loss: 0.0001514903480849068\tValidation Loss: 0.00015330660814220342\n",
      "Epoch 2976  \tTraining Loss: 0.00015147563179698087\tValidation Loss: 0.00015329246909931234\n",
      "Epoch 2977  \tTraining Loss: 0.0001514609273105871\tValidation Loss: 0.00015327834169065664\n",
      "Epoch 2978  \tTraining Loss: 0.00015144623323078499\tValidation Loss: 0.00015326422422296465\n",
      "Epoch 2979  \tTraining Loss: 0.00015143155164568906\tValidation Loss: 0.00015325011722834656\n",
      "Epoch 2980  \tTraining Loss: 0.00015141688253314265\tValidation Loss: 0.00015323602139517213\n",
      "Epoch 2981  \tTraining Loss: 0.00015140222588516439\tValidation Loss: 0.00015322193690862904\n",
      "Epoch 2982  \tTraining Loss: 0.0001513875816944361\tValidation Loss: 0.00015320786389442614\n",
      "Epoch 2983  \tTraining Loss: 0.0001513729499537925\tValidation Loss: 0.0001531938024390263\n",
      "Epoch 2984  \tTraining Loss: 0.00015135833065613099\tValidation Loss: 0.00015317975260039973\n",
      "Epoch 2985  \tTraining Loss: 0.0001513437237943782\tValidation Loss: 0.00015316571441662804\n",
      "Epoch 2986  \tTraining Loss: 0.00015132912936147535\tValidation Loss: 0.00015315168791229158\n",
      "Epoch 2987  \tTraining Loss: 0.00015131454735037\tValidation Loss: 0.00015313767310282182\n",
      "Epoch 2988  \tTraining Loss: 0.00015129997775401222\tValidation Loss: 0.00015312366999740642\n",
      "Epoch 2989  \tTraining Loss: 0.00015128542056535194\tValidation Loss: 0.00015310967860093308\n",
      "Epoch 2990  \tTraining Loss: 0.00015127087577733719\tValidation Loss: 0.00015309569891530254\n",
      "Epoch 2991  \tTraining Loss: 0.00015125634338291314\tValidation Loss: 0.00015308173094032309\n",
      "Epoch 2992  \tTraining Loss: 0.0001512418233750209\tValidation Loss: 0.00015306777467432559\n",
      "Epoch 2993  \tTraining Loss: 0.0001512273157465971\tValidation Loss: 0.00015305383011458856\n",
      "Epoch 2994  \tTraining Loss: 0.00015121282049057274\tValidation Loss: 0.00015303989725763432\n",
      "Epoch 2995  \tTraining Loss: 0.00015119833759987298\tValidation Loss: 0.000153025976099436\n",
      "Epoch 2996  \tTraining Loss: 0.00015118386706741634\tValidation Loss: 0.00015301206453389023\n",
      "Epoch 2997  \tTraining Loss: 0.00015116940888611425\tValidation Loss: 0.00015299816101146935\n",
      "Epoch 2998  \tTraining Loss: 0.00015115496304887052\tValidation Loss: 0.00015298426917229038\n",
      "Epoch 2999  \tTraining Loss: 0.0001511405295485809\tValidation Loss: 0.00015297038901124902\n",
      "Epoch 3000  \tTraining Loss: 0.00015112610837813268\tValidation Loss: 0.00015295652052310245\n",
      "Epoch 3001  \tTraining Loss: 0.0001511116995304042\tValidation Loss: 0.0001529426637024963\n",
      "Epoch 3002  \tTraining Loss: 0.00015109730299826455\tValidation Loss: 0.00015292881854398355\n",
      "Epoch 3003  \tTraining Loss: 0.0001510829187745731\tValidation Loss: 0.00015291498504203875\n",
      "Epoch 3004  \tTraining Loss: 0.00015106854685217923\tValidation Loss: 0.00015290116319106807\n",
      "Epoch 3005  \tTraining Loss: 0.00015105418722392196\tValidation Loss: 0.00015288735298541684\n",
      "Epoch 3006  \tTraining Loss: 0.00015103983988262973\tValidation Loss: 0.00015287355441937517\n",
      "Epoch 3007  \tTraining Loss: 0.0001510255048211199\tValidation Loss: 0.00015285976748718213\n",
      "Epoch 3008  \tTraining Loss: 0.00015101118203219877\tValidation Loss: 0.00015284599218302886\n",
      "Epoch 3009  \tTraining Loss: 0.00015099687150866095\tValidation Loss: 0.00015283222850106108\n",
      "Epoch 3010  \tTraining Loss: 0.0001509825732432894\tValidation Loss: 0.00015281847643538115\n",
      "Epoch 3011  \tTraining Loss: 0.00015096828722885503\tValidation Loss: 0.00015280473598004937\n",
      "Epoch 3012  \tTraining Loss: 0.00015095401345811664\tValidation Loss: 0.00015279100712908554\n",
      "Epoch 3013  \tTraining Loss: 0.0001509397519238204\tValidation Loss: 0.00015277728987646974\n",
      "Epoch 3014  \tTraining Loss: 0.0001509255026187\tValidation Loss: 0.0001527635842161434\n",
      "Epoch 3015  \tTraining Loss: 0.00015091126553547614\tValidation Loss: 0.0001527498901420101\n",
      "Epoch 3016  \tTraining Loss: 0.00015089704066685664\tValidation Loss: 0.00015273620764793622\n",
      "Epoch 3017  \tTraining Loss: 0.00015088282800553596\tValidation Loss: 0.0001527225367277515\n",
      "Epoch 3018  \tTraining Loss: 0.00015086862755287928\tValidation Loss: 0.000152708877080201\n",
      "Epoch 3019  \tTraining Loss: 0.00015085443933467123\tValidation Loss: 0.00015269522940462269\n",
      "Epoch 3020  \tTraining Loss: 0.00015084026329541402\tValidation Loss: 0.00015268159348016255\n",
      "Epoch 3021  \tTraining Loss: 0.0001508261007784851\tValidation Loss: 0.00015266797045539532\n",
      "Epoch 3022  \tTraining Loss: 0.00015081195184683138\tValidation Loss: 0.00015265435792445367\n",
      "Epoch 3023  \tTraining Loss: 0.0001507978150625183\tValidation Loss: 0.00015264075580326985\n",
      "Epoch 3024  \tTraining Loss: 0.00015078369025723097\tValidation Loss: 0.00015262717855653424\n",
      "Epoch 3025  \tTraining Loss: 0.00015076957614637375\tValidation Loss: 0.000152613603368959\n",
      "Epoch 3026  \tTraining Loss: 0.00015075547419182893\tValidation Loss: 0.0001526000383633431\n",
      "Epoch 3027  \tTraining Loss: 0.00015074138435507804\tValidation Loss: 0.00015258648461748543\n",
      "Epoch 3028  \tTraining Loss: 0.00015072730662616396\tValidation Loss: 0.0001525729422079004\n",
      "Epoch 3029  \tTraining Loss: 0.0001507132409969459\tValidation Loss: 0.0001525594111515236\n",
      "Epoch 3030  \tTraining Loss: 0.00015069918745962051\tValidation Loss: 0.00015254589147253056\n",
      "Epoch 3031  \tTraining Loss: 0.00015068514600651375\tValidation Loss: 0.00015253238319392904\n",
      "Epoch 3032  \tTraining Loss: 0.00015067111663001007\tValidation Loss: 0.0001525188863332557\n",
      "Epoch 3033  \tTraining Loss: 0.0001506570993225194\tValidation Loss: 0.00015250540090234368\n",
      "Epoch 3034  \tTraining Loss: 0.0001506430940764594\tValidation Loss: 0.00015249192690832303\n",
      "Epoch 3035  \tTraining Loss: 0.0001506291008842467\tValidation Loss: 0.0001524784643547114\n",
      "Epoch 3036  \tTraining Loss: 0.0001506151197382912\tValidation Loss: 0.00015246501324230608\n",
      "Epoch 3037  \tTraining Loss: 0.00015060115063099343\tValidation Loss: 0.00015245157356986078\n",
      "Epoch 3038  \tTraining Loss: 0.00015058719355474264\tValidation Loss: 0.00015243814533458981\n",
      "Epoch 3039  \tTraining Loss: 0.00015057324850191583\tValidation Loss: 0.0001524247285325446\n",
      "Epoch 3040  \tTraining Loss: 0.00015055931546487675\tValidation Loss: 0.00015241132315889613\n",
      "Epoch 3041  \tTraining Loss: 0.00015054539443597577\tValidation Loss: 0.00015239792920814746\n",
      "Epoch 3042  \tTraining Loss: 0.0001505314854075492\tValidation Loss: 0.00015238454667429487\n",
      "Epoch 3043  \tTraining Loss: 0.00015051758837191927\tValidation Loss: 0.00015237117555094926\n",
      "Epoch 3044  \tTraining Loss: 0.0001505037033213938\tValidation Loss: 0.00015235781583142812\n",
      "Epoch 3045  \tTraining Loss: 0.00015048983024826612\tValidation Loss: 0.000152344467508825\n",
      "Epoch 3046  \tTraining Loss: 0.00015047596914481492\tValidation Loss: 0.00015233113057606183\n",
      "Epoch 3047  \tTraining Loss: 0.00015046212000330403\tValidation Loss: 0.00015231780502592855\n",
      "Epoch 3048  \tTraining Loss: 0.00015044828281598249\tValidation Loss: 0.0001523044908511128\n",
      "Epoch 3049  \tTraining Loss: 0.0001504344575750844\tValidation Loss: 0.0001522911880442221\n",
      "Epoch 3050  \tTraining Loss: 0.00015042064427282879\tValidation Loss: 0.00015227789659780124\n",
      "Epoch 3051  \tTraining Loss: 0.00015040684290141954\tValidation Loss: 0.00015226461650434453\n",
      "Epoch 3052  \tTraining Loss: 0.00015039305345304556\tValidation Loss: 0.00015225134775630544\n",
      "Epoch 3053  \tTraining Loss: 0.00015037927591988041\tValidation Loss: 0.00015223809034610393\n",
      "Epoch 3054  \tTraining Loss: 0.00015036551029408248\tValidation Loss: 0.00015222484426613168\n",
      "Epoch 3055  \tTraining Loss: 0.0001503517565677948\tValidation Loss: 0.00015221160950875648\n",
      "Epoch 3056  \tTraining Loss: 0.00015033801473314512\tValidation Loss: 0.00015219838606632494\n",
      "Epoch 3057  \tTraining Loss: 0.00015032428478224587\tValidation Loss: 0.00015218517393116496\n",
      "Epoch 3058  \tTraining Loss: 0.00015031056670719405\tValidation Loss: 0.00015217197309558755\n",
      "Epoch 3059  \tTraining Loss: 0.00015029686050007122\tValidation Loss: 0.000152158783551888\n",
      "Epoch 3060  \tTraining Loss: 0.0001502831661529436\tValidation Loss: 0.0001521456052923471\n",
      "Epoch 3061  \tTraining Loss: 0.00015026948365786183\tValidation Loss: 0.00015213243830923147\n",
      "Epoch 3062  \tTraining Loss: 0.00015025581300686115\tValidation Loss: 0.00015211928259479445\n",
      "Epoch 3063  \tTraining Loss: 0.00015024215419196134\tValidation Loss: 0.00015210613814127644\n",
      "Epoch 3064  \tTraining Loss: 0.00015022850720516667\tValidation Loss: 0.000152093004940905\n",
      "Epoch 3065  \tTraining Loss: 0.00015021487203846577\tValidation Loss: 0.00015207988298589526\n",
      "Epoch 3066  \tTraining Loss: 0.00015020124868383197\tValidation Loss: 0.00015206677226845\n",
      "Epoch 3067  \tTraining Loss: 0.00015018763713322287\tValidation Loss: 0.00015205367278075963\n",
      "Epoch 3068  \tTraining Loss: 0.00015017403737858066\tValidation Loss: 0.00015204058469720907\n",
      "Epoch 3069  \tTraining Loss: 0.00015016044941183198\tValidation Loss: 0.00015202750814610427\n",
      "Epoch 3070  \tTraining Loss: 0.0001501468732248879\tValidation Loss: 0.00015201444279115326\n",
      "Epoch 3071  \tTraining Loss: 0.00015013330880964397\tValidation Loss: 0.0001520013886245054\n",
      "Epoch 3072  \tTraining Loss: 0.0001501197561579802\tValidation Loss: 0.00015198834563829786\n",
      "Epoch 3073  \tTraining Loss: 0.00015010621526176116\tValidation Loss: 0.0001519753138246554\n",
      "Epoch 3074  \tTraining Loss: 0.00015009268625550186\tValidation Loss: 0.0001519622878210979\n",
      "Epoch 3075  \tTraining Loss: 0.0001500791691851927\tValidation Loss: 0.00015194927696425683\n",
      "Epoch 3076  \tTraining Loss: 0.0001500656638556032\tValidation Loss: 0.00015193627956466\n",
      "Epoch 3077  \tTraining Loss: 0.00015005217025091888\tValidation Loss: 0.0001519232943781721\n",
      "Epoch 3078  \tTraining Loss: 0.00015003868836249362\tValidation Loss: 0.00015191032070210243\n",
      "Epoch 3079  \tTraining Loss: 0.00015002521818198805\tValidation Loss: 0.00015189735827225666\n",
      "Epoch 3080  \tTraining Loss: 0.00015001175970110196\tValidation Loss: 0.0001518844069868669\n",
      "Epoch 3081  \tTraining Loss: 0.00014999831291153496\tValidation Loss: 0.00015187146680307093\n",
      "Epoch 3082  \tTraining Loss: 0.00014998487780497666\tValidation Loss: 0.00015185853769923423\n",
      "Epoch 3083  \tTraining Loss: 0.00014997145437310394\tValidation Loss: 0.0001518456196616738\n",
      "Epoch 3084  \tTraining Loss: 0.00014995804202718295\tValidation Loss: 0.00015183270226911243\n",
      "Epoch 3085  \tTraining Loss: 0.00014994463970294835\tValidation Loss: 0.00015181979775881938\n",
      "Epoch 3086  \tTraining Loss: 0.00014993124905351974\tValidation Loss: 0.00015180690629755658\n",
      "Epoch 3087  \tTraining Loss: 0.0001499178700538463\tValidation Loss: 0.00015179402666122826\n",
      "Epoch 3088  \tTraining Loss: 0.00014990450269458615\tValidation Loss: 0.00015178115839396857\n",
      "Epoch 3089  \tTraining Loss: 0.00014989114696714408\tValidation Loss: 0.00015176830128940696\n",
      "Epoch 3090  \tTraining Loss: 0.0001498778028630102\tValidation Loss: 0.00015175545525899077\n",
      "Epoch 3091  \tTraining Loss: 0.0001498644703736878\tValidation Loss: 0.00015174262026932924\n",
      "Epoch 3092  \tTraining Loss: 0.0001498511494906773\tValidation Loss: 0.0001517297963095652\n",
      "Epoch 3093  \tTraining Loss: 0.0001498378402054707\tValidation Loss: 0.0001517169833763727\n",
      "Epoch 3094  \tTraining Loss: 0.0001498245425095487\tValidation Loss: 0.00015170418146779506\n",
      "Epoch 3095  \tTraining Loss: 0.00014981125639437981\tValidation Loss: 0.00015169139058106792\n",
      "Epoch 3096  \tTraining Loss: 0.00014979798185141917\tValidation Loss: 0.00015167861071208364\n",
      "Epoch 3097  \tTraining Loss: 0.00014978471887210884\tValidation Loss: 0.00015166584185545846\n",
      "Epoch 3098  \tTraining Loss: 0.0001497714674478771\tValidation Loss: 0.00015165308400477372\n",
      "Epoch 3099  \tTraining Loss: 0.00014975822757013865\tValidation Loss: 0.00015164033565532914\n",
      "Epoch 3100  \tTraining Loss: 0.00014974499923029434\tValidation Loss: 0.00015162759596909858\n",
      "Epoch 3101  \tTraining Loss: 0.00014973178241973117\tValidation Loss: 0.00015161486727170224\n",
      "Epoch 3102  \tTraining Loss: 0.00014971857712982239\tValidation Loss: 0.00015160214955466404\n",
      "Epoch 3103  \tTraining Loss: 0.00014970538335192722\tValidation Loss: 0.00015158944280931346\n",
      "Epoch 3104  \tTraining Loss: 0.00014969220107739095\tValidation Loss: 0.00015157674702684696\n",
      "Epoch 3105  \tTraining Loss: 0.00014967903029754507\tValidation Loss: 0.00015156406219837086\n",
      "Epoch 3106  \tTraining Loss: 0.00014966587100370706\tValidation Loss: 0.00015155138831493103\n",
      "Epoch 3107  \tTraining Loss: 0.00014965272318718037\tValidation Loss: 0.00015153872536753372\n",
      "Epoch 3108  \tTraining Loss: 0.00014963958683925462\tValidation Loss: 0.00015152607334715956\n",
      "Epoch 3109  \tTraining Loss: 0.00014962646195120546\tValidation Loss: 0.00015151343224477316\n",
      "Epoch 3110  \tTraining Loss: 0.00014961334851429454\tValidation Loss: 0.0001515008020513297\n",
      "Epoch 3111  \tTraining Loss: 0.0001496002465197696\tValidation Loss: 0.00015148818275777912\n",
      "Epoch 3112  \tTraining Loss: 0.0001495871559588645\tValidation Loss: 0.000151475574355069\n",
      "Epoch 3113  \tTraining Loss: 0.00014957407682279902\tValidation Loss: 0.00015146297683414617\n",
      "Epoch 3114  \tTraining Loss: 0.00014956100910277925\tValidation Loss: 0.00015145039018595797\n",
      "Epoch 3115  \tTraining Loss: 0.0001495479527899972\tValidation Loss: 0.0001514378144014525\n",
      "Epoch 3116  \tTraining Loss: 0.00014953490787563106\tValidation Loss: 0.00015142524947157912\n",
      "Epoch 3117  \tTraining Loss: 0.00014952187435084523\tValidation Loss: 0.00015141269538728825\n",
      "Epoch 3118  \tTraining Loss: 0.00014950885220679018\tValidation Loss: 0.00015140015213953135\n",
      "Epoch 3119  \tTraining Loss: 0.0001494958414346026\tValidation Loss: 0.00015138761971926061\n",
      "Epoch 3120  \tTraining Loss: 0.0001494828420254054\tValidation Loss: 0.0001513750981174286\n",
      "Epoch 3121  \tTraining Loss: 0.00014946985397030766\tValidation Loss: 0.00015136258732498814\n",
      "Epoch 3122  \tTraining Loss: 0.00014945687726040482\tValidation Loss: 0.00015135008733289175\n",
      "Epoch 3123  \tTraining Loss: 0.0001494439118867785\tValidation Loss: 0.00015133759813209128\n",
      "Epoch 3124  \tTraining Loss: 0.00014943095784049665\tValidation Loss: 0.0001513251197135378\n",
      "Epoch 3125  \tTraining Loss: 0.00014941801511261365\tValidation Loss: 0.00015131265206818102\n",
      "Epoch 3126  \tTraining Loss: 0.00014940508369417016\tValidation Loss: 0.00015130019518696906\n",
      "Epoch 3127  \tTraining Loss: 0.00014939216357619318\tValidation Loss: 0.0001512877490608481\n",
      "Epoch 3128  \tTraining Loss: 0.00014937925474969646\tValidation Loss: 0.00015127531368076203\n",
      "Epoch 3129  \tTraining Loss: 0.00014936635720567984\tValidation Loss: 0.00015126288903765227\n",
      "Epoch 3130  \tTraining Loss: 0.0001493534709351299\tValidation Loss: 0.00015125047512245737\n",
      "Epoch 3131  \tTraining Loss: 0.00014934059592901968\tValidation Loss: 0.00015123807192611282\n",
      "Epoch 3132  \tTraining Loss: 0.00014932773217830888\tValidation Loss: 0.00015122567943955077\n",
      "Epoch 3133  \tTraining Loss: 0.00014931487967394374\tValidation Loss: 0.00015121329765369968\n",
      "Epoch 3134  \tTraining Loss: 0.00014930203840685718\tValidation Loss: 0.0001512009265594843\n",
      "Epoch 3135  \tTraining Loss: 0.0001492892083679689\tValidation Loss: 0.00015118856614782522\n",
      "Epoch 3136  \tTraining Loss: 0.00014927638954818518\tValidation Loss: 0.00015117621640963884\n",
      "Epoch 3137  \tTraining Loss: 0.00014926358193839934\tValidation Loss: 0.00015116387733583704\n",
      "Epoch 3138  \tTraining Loss: 0.00014925078552949114\tValidation Loss: 0.0001511515489173271\n",
      "Epoch 3139  \tTraining Loss: 0.00014923800031232763\tValidation Loss: 0.00015113923114501134\n",
      "Epoch 3140  \tTraining Loss: 0.00014922522627776251\tValidation Loss: 0.0001511269240097872\n",
      "Epoch 3141  \tTraining Loss: 0.00014921246341663657\tValidation Loss: 0.00015111462750254688\n",
      "Epoch 3142  \tTraining Loss: 0.00014919971171977744\tValidation Loss: 0.00015110234161417718\n",
      "Epoch 3143  \tTraining Loss: 0.000149186971178\tValidation Loss: 0.00015109006633555943\n",
      "Epoch 3144  \tTraining Loss: 0.00014917424178210605\tValidation Loss: 0.00015107780165756943\n",
      "Epoch 3145  \tTraining Loss: 0.00014916152352288463\tValidation Loss: 0.0001510655475710771\n",
      "Epoch 3146  \tTraining Loss: 0.00014914881639111206\tValidation Loss: 0.00015105330406694654\n",
      "Epoch 3147  \tTraining Loss: 0.00014913612037755164\tValidation Loss: 0.0001510410711360357\n",
      "Epoch 3148  \tTraining Loss: 0.0001491234354729542\tValidation Loss: 0.00015102884876919655\n",
      "Epoch 3149  \tTraining Loss: 0.0001491107616680577\tValidation Loss: 0.0001510166369572747\n",
      "Epoch 3150  \tTraining Loss: 0.0001490980989535878\tValidation Loss: 0.00015100443569110946\n",
      "Epoch 3151  \tTraining Loss: 0.0001490854473202572\tValidation Loss: 0.00015099224496153372\n",
      "Epoch 3152  \tTraining Loss: 0.00014907280675876633\tValidation Loss: 0.00015098006475937375\n",
      "Epoch 3153  \tTraining Loss: 0.00014906017725980318\tValidation Loss: 0.00015096789507544929\n",
      "Epoch 3154  \tTraining Loss: 0.0001490475588140432\tValidation Loss: 0.0001509557359005733\n",
      "Epoch 3155  \tTraining Loss: 0.0001490349514121496\tValidation Loss: 0.0001509435872255519\n",
      "Epoch 3156  \tTraining Loss: 0.00014902235504477313\tValidation Loss: 0.00015093144904118453\n",
      "Epoch 3157  \tTraining Loss: 0.00014900976962415742\tValidation Loss: 0.00015091932084576975\n",
      "Epoch 3158  \tTraining Loss: 0.00014899719376250915\tValidation Loss: 0.00015090720231292846\n",
      "Epoch 3159  \tTraining Loss: 0.0001489846289191683\tValidation Loss: 0.0001508950935254362\n",
      "Epoch 3160  \tTraining Loss: 0.00014897207507789603\tValidation Loss: 0.00015088299486797635\n",
      "Epoch 3161  \tTraining Loss: 0.00014895953222887457\tValidation Loss: 0.00015087090643531922\n",
      "Epoch 3162  \tTraining Loss: 0.00014894700036255178\tValidation Loss: 0.00015085882829133173\n",
      "Epoch 3163  \tTraining Loss: 0.00014893447946942427\tValidation Loss: 0.00015084676047866493\n",
      "Epoch 3164  \tTraining Loss: 0.0001489219695400007\tValidation Loss: 0.00015083470302384948\n",
      "Epoch 3165  \tTraining Loss: 0.00014890947056478952\tValidation Loss: 0.00015082265594197123\n",
      "Epoch 3166  \tTraining Loss: 0.0001488969825342928\tValidation Loss: 0.00015081061924031178\n",
      "Epoch 3167  \tTraining Loss: 0.00014888450543900443\tValidation Loss: 0.00015079859292087562\n",
      "Epoch 3168  \tTraining Loss: 0.0001488720392694085\tValidation Loss: 0.00015078657698208735\n",
      "Epoch 3169  \tTraining Loss: 0.0001488595840159787\tValidation Loss: 0.00015077457141992757\n",
      "Epoch 3170  \tTraining Loss: 0.00014884713966917818\tValidation Loss: 0.00015076257622869673\n",
      "Epoch 3171  \tTraining Loss: 0.00014883470621945927\tValidation Loss: 0.00015075059140153265\n",
      "Epoch 3172  \tTraining Loss: 0.00014882228365726362\tValidation Loss: 0.0001507386169307638\n",
      "Epoch 3173  \tTraining Loss: 0.00014880987197302183\tValidation Loss: 0.00015072665280815186\n",
      "Epoch 3174  \tTraining Loss: 0.00014879747115715384\tValidation Loss: 0.00015071469902505974\n",
      "Epoch 3175  \tTraining Loss: 0.00014878508120006872\tValidation Loss: 0.00015070275557256817\n",
      "Epoch 3176  \tTraining Loss: 0.0001487727020921649\tValidation Loss: 0.00015069082244155733\n",
      "Epoch 3177  \tTraining Loss: 0.0001487603338238298\tValidation Loss: 0.00015067889962276404\n",
      "Epoch 3178  \tTraining Loss: 0.00014874797638544037\tValidation Loss: 0.00015066698710682248\n",
      "Epoch 3179  \tTraining Loss: 0.00014873562976736287\tValidation Loss: 0.00015065508488429257\n",
      "Epoch 3180  \tTraining Loss: 0.00014872329395995283\tValidation Loss: 0.00015064319294568038\n",
      "Epoch 3181  \tTraining Loss: 0.00014871096895355534\tValidation Loss: 0.00015063131128145268\n",
      "Epoch 3182  \tTraining Loss: 0.00014869865473850493\tValidation Loss: 0.00015061943988204724\n",
      "Epoch 3183  \tTraining Loss: 0.0001486863513051255\tValidation Loss: 0.00015060757873788016\n",
      "Epoch 3184  \tTraining Loss: 0.00014867405864373077\tValidation Loss: 0.00015059572783935127\n",
      "Epoch 3185  \tTraining Loss: 0.00014866177674462396\tValidation Loss: 0.0001505838871768478\n",
      "Epoch 3186  \tTraining Loss: 0.00014864950559809797\tValidation Loss: 0.0001505720567407473\n",
      "Epoch 3187  \tTraining Loss: 0.0001486372451944354\tValidation Loss: 0.00015056023652141925\n",
      "Epoch 3188  \tTraining Loss: 0.00014862499552390865\tValidation Loss: 0.00015054842650922664\n",
      "Epoch 3189  \tTraining Loss: 0.00014861275657678005\tValidation Loss: 0.00015053662669452703\n",
      "Epoch 3190  \tTraining Loss: 0.00014860052834330165\tValidation Loss: 0.00015052483706767293\n",
      "Epoch 3191  \tTraining Loss: 0.00014858831081371554\tValidation Loss: 0.00015051305761901266\n",
      "Epoch 3192  \tTraining Loss: 0.0001485761039782538\tValidation Loss: 0.00015050128833889021\n",
      "Epoch 3193  \tTraining Loss: 0.00014856390782713862\tValidation Loss: 0.000150489529217646\n",
      "Epoch 3194  \tTraining Loss: 0.00014855172235058215\tValidation Loss: 0.0001504777802456165\n",
      "Epoch 3195  \tTraining Loss: 0.00014853954753878687\tValidation Loss: 0.00015046604141313475\n",
      "Epoch 3196  \tTraining Loss: 0.0001485273833819454\tValidation Loss: 0.00015045431271052998\n",
      "Epoch 3197  \tTraining Loss: 0.00014851522987024057\tValidation Loss: 0.00015044259412812802\n",
      "Epoch 3198  \tTraining Loss: 0.00014850308699384577\tValidation Loss: 0.00015043088565625098\n",
      "Epoch 3199  \tTraining Loss: 0.00014849095474292462\tValidation Loss: 0.00015041918728521744\n",
      "Epoch 3200  \tTraining Loss: 0.00014847883310763126\tValidation Loss: 0.0001504074990053424\n",
      "Epoch 3201  \tTraining Loss: 0.00014846672207811026\tValidation Loss: 0.00015039582080693704\n",
      "Epoch 3202  \tTraining Loss: 0.00014845462164449699\tValidation Loss: 0.00015038415268030892\n",
      "Epoch 3203  \tTraining Loss: 0.0001484425317969172\tValidation Loss: 0.0001503724946157619\n",
      "Epoch 3204  \tTraining Loss: 0.0001484304525254875\tValidation Loss: 0.000150360846603596\n",
      "Epoch 3205  \tTraining Loss: 0.00014841838382031523\tValidation Loss: 0.00015034920863410742\n",
      "Epoch 3206  \tTraining Loss: 0.00014840632567149852\tValidation Loss: 0.00015033758069758852\n",
      "Epoch 3207  \tTraining Loss: 0.00014839427806912652\tValidation Loss: 0.00015032596278432772\n",
      "Epoch 3208  \tTraining Loss: 0.00014838224100327914\tValidation Loss: 0.0001503143548846096\n",
      "Epoch 3209  \tTraining Loss: 0.0001483702144640273\tValidation Loss: 0.00015030275698871466\n",
      "Epoch 3210  \tTraining Loss: 0.00014835819844143325\tValidation Loss: 0.0001502911690869195\n",
      "Epoch 3211  \tTraining Loss: 0.00014834619292555012\tValidation Loss: 0.0001502795911694967\n",
      "Epoch 3212  \tTraining Loss: 0.00014833419790642234\tValidation Loss: 0.0001502680232267147\n",
      "Epoch 3213  \tTraining Loss: 0.00014832221337408554\tValidation Loss: 0.00015025646524883802\n",
      "Epoch 3214  \tTraining Loss: 0.00014831023931856675\tValidation Loss: 0.00015024491722612695\n",
      "Epoch 3215  \tTraining Loss: 0.00014829827572988437\tValidation Loss: 0.00015023337914883776\n",
      "Epoch 3216  \tTraining Loss: 0.0001482863225980483\tValidation Loss: 0.00015022185100722256\n",
      "Epoch 3217  \tTraining Loss: 0.00014827437991305975\tValidation Loss: 0.00015021033279152937\n",
      "Epoch 3218  \tTraining Loss: 0.00014826244766491176\tValidation Loss: 0.00015019882449200198\n",
      "Epoch 3219  \tTraining Loss: 0.00014825052585682258\tValidation Loss: 0.00015018729839657606\n",
      "Epoch 3220  \tTraining Loss: 0.00014823861706286787\tValidation Loss: 0.00015017580401083716\n",
      "Epoch 3221  \tTraining Loss: 0.0001482267188608612\tValidation Loss: 0.00015016432378197052\n",
      "Epoch 3222  \tTraining Loss: 0.00014821483108127886\tValidation Loss: 0.0001501528548014006\n",
      "Epoch 3223  \tTraining Loss: 0.0001482029537012457\tValidation Loss: 0.0001501413965233035\n",
      "Epoch 3224  \tTraining Loss: 0.0001481910867069727\tValidation Loss: 0.00015012994867357542\n",
      "Epoch 3225  \tTraining Loss: 0.00014817923008650512\tValidation Loss: 0.00015011851105720543\n",
      "Epoch 3226  \tTraining Loss: 0.00014816738382866465\tValidation Loss: 0.00015010708353378158\n",
      "Epoch 3227  \tTraining Loss: 0.00014815554792267112\tValidation Loss: 0.00015009566600241217\n",
      "Epoch 3228  \tTraining Loss: 0.00014814372235796012\tValidation Loss: 0.00015008425838950725\n",
      "Epoch 3229  \tTraining Loss: 0.0001481319071240869\tValidation Loss: 0.00015007286064007294\n",
      "Epoch 3230  \tTraining Loss: 0.00014812010221067426\tValidation Loss: 0.00015006147271191223\n",
      "Epoch 3231  \tTraining Loss: 0.00014810830760738397\tValidation Loss: 0.00015005009457180917\n",
      "Epoch 3232  \tTraining Loss: 0.00014809652330389995\tValidation Loss: 0.0001500387261929768\n",
      "Epoch 3233  \tTraining Loss: 0.00014808474899069093\tValidation Loss: 0.00015002735947349164\n",
      "Epoch 3234  \tTraining Loss: 0.0001480729810714658\tValidation Loss: 0.0001500160058326902\n",
      "Epoch 3235  \tTraining Loss: 0.00014806122346139937\tValidation Loss: 0.00015000466482974854\n",
      "Epoch 3236  \tTraining Loss: 0.00014804947612078075\tValidation Loss: 0.00014999333449181398\n",
      "Epoch 3237  \tTraining Loss: 0.00014803773903780795\tValidation Loss: 0.00014998201412974783\n",
      "Epoch 3238  \tTraining Loss: 0.00014802601220189647\tValidation Loss: 0.00014997070345015983\n",
      "Epoch 3239  \tTraining Loss: 0.0001480142956026263\tValidation Loss: 0.00014995940233719358\n",
      "Epoch 3240  \tTraining Loss: 0.0001480025892296253\tValidation Loss: 0.00014994811075358575\n",
      "Epoch 3241  \tTraining Loss: 0.00014799089307254086\tValidation Loss: 0.0001499368286925622\n",
      "Epoch 3242  \tTraining Loss: 0.00014797920712102892\tValidation Loss: 0.0001499255561567604\n",
      "Epoch 3243  \tTraining Loss: 0.00014796753136474898\tValidation Loss: 0.00014991429314994091\n",
      "Epoch 3244  \tTraining Loss: 0.0001479558657933615\tValidation Loss: 0.000149903039674232\n",
      "Epoch 3245  \tTraining Loss: 0.00014794421039652624\tValidation Loss: 0.00014989179572956842\n",
      "Epoch 3246  \tTraining Loss: 0.00014793256516390152\tValidation Loss: 0.00014988056131389786\n",
      "Epoch 3247  \tTraining Loss: 0.00014792093008514365\tValidation Loss: 0.00014986933642358258\n",
      "Epoch 3248  \tTraining Loss: 0.00014790930517804245\tValidation Loss: 0.00014985811415915156\n",
      "Epoch 3249  \tTraining Loss: 0.00014789769072713748\tValidation Loss: 0.00014984691183270263\n",
      "Epoch 3250  \tTraining Loss: 0.00014788608638549854\tValidation Loss: 0.00014983571187240113\n",
      "Epoch 3251  \tTraining Loss: 0.00014787449213860812\tValidation Loss: 0.00014982452278510425\n",
      "Epoch 3252  \tTraining Loss: 0.00014786290802645677\tValidation Loss: 0.0001498133456741706\n",
      "Epoch 3253  \tTraining Loss: 0.00014785133400802396\tValidation Loss: 0.00014980217881559974\n",
      "Epoch 3254  \tTraining Loss: 0.00014783977007164453\tValidation Loss: 0.00014979102165675175\n",
      "Epoch 3255  \tTraining Loss: 0.00014782821620668889\tValidation Loss: 0.00014977987396465824\n",
      "Epoch 3256  \tTraining Loss: 0.0001478166724026554\tValidation Loss: 0.00014976873564769534\n",
      "Epoch 3257  \tTraining Loss: 0.00014780513864907699\tValidation Loss: 0.0001497576066775877\n",
      "Epoch 3258  \tTraining Loss: 0.00014779361493550013\tValidation Loss: 0.00014974648705047384\n",
      "Epoch 3259  \tTraining Loss: 0.0001477821012514763\tValidation Loss: 0.00014973537676967298\n",
      "Epoch 3260  \tTraining Loss: 0.00014777059758655842\tValidation Loss: 0.0001497242758389351\n",
      "Epoch 3261  \tTraining Loss: 0.0001477591039302989\tValidation Loss: 0.00014971318426026172\n",
      "Epoch 3262  \tTraining Loss: 0.00014774762027224857\tValidation Loss: 0.00014970210203353194\n",
      "Epoch 3263  \tTraining Loss: 0.00014773614660195628\tValidation Loss: 0.00014969102915674722\n",
      "Epoch 3264  \tTraining Loss: 0.00014772468290896818\tValidation Loss: 0.00014967996562642054\n",
      "Epoch 3265  \tTraining Loss: 0.00014771322918282807\tValidation Loss: 0.00014966891143794263\n",
      "Epoch 3266  \tTraining Loss: 0.0001477017854130765\tValidation Loss: 0.0001496578665858773\n",
      "Epoch 3267  \tTraining Loss: 0.00014769035158925146\tValidation Loss: 0.0001496468249506385\n",
      "Epoch 3268  \tTraining Loss: 0.00014767892770088786\tValidation Loss: 0.00014963578943910834\n",
      "Epoch 3269  \tTraining Loss: 0.00014766751373751735\tValidation Loss: 0.00014962476325225388\n",
      "Epoch 3270  \tTraining Loss: 0.00014765610968866876\tValidation Loss: 0.00014961374638302322\n",
      "Epoch 3271  \tTraining Loss: 0.00014764471554386767\tValidation Loss: 0.0001496027388241795\n",
      "Epoch 3272  \tTraining Loss: 0.0001476333312926366\tValidation Loss: 0.00014959174056834083\n",
      "Epoch 3273  \tTraining Loss: 0.00014762195692449485\tValidation Loss: 0.00014958075160800903\n",
      "Epoch 3274  \tTraining Loss: 0.0001476105924289587\tValidation Loss: 0.00014956977193558924\n",
      "Epoch 3275  \tTraining Loss: 0.00014759923779554123\tValidation Loss: 0.00014955880154340394\n",
      "Epoch 3276  \tTraining Loss: 0.00014758789301375249\tValidation Loss: 0.00014954784042370286\n",
      "Epoch 3277  \tTraining Loss: 0.0001475765580730993\tValidation Loss: 0.00014953688856866995\n",
      "Epoch 3278  \tTraining Loss: 0.00014756523296308554\tValidation Loss: 0.0001495259459704284\n",
      "Epoch 3279  \tTraining Loss: 0.00014755391767321197\tValidation Loss: 0.00014951501262104445\n",
      "Epoch 3280  \tTraining Loss: 0.00014754261219297632\tValidation Loss: 0.00014950408851253003\n",
      "Epoch 3281  \tTraining Loss: 0.0001475313165118734\tValidation Loss: 0.00014949317363684514\n",
      "Epoch 3282  \tTraining Loss: 0.00014752003061939493\tValidation Loss: 0.00014948226798589947\n",
      "Epoch 3283  \tTraining Loss: 0.00014750875450502977\tValidation Loss: 0.00014947137155155377\n",
      "Epoch 3284  \tTraining Loss: 0.00014749748815826385\tValidation Loss: 0.0001494604843256212\n",
      "Epoch 3285  \tTraining Loss: 0.0001474862315685803\tValidation Loss: 0.00014944960629986842\n",
      "Epoch 3286  \tTraining Loss: 0.00014747498472545933\tValidation Loss: 0.00014943873746601659\n",
      "Epoch 3287  \tTraining Loss: 0.00014746374761837847\tValidation Loss: 0.0001494278778157423\n",
      "Epoch 3288  \tTraining Loss: 0.0001474525202368124\tValidation Loss: 0.00014941702734067868\n",
      "Epoch 3289  \tTraining Loss: 0.0001474413025702333\tValidation Loss: 0.0001494061860324161\n",
      "Epoch 3290  \tTraining Loss: 0.0001474300946081104\tValidation Loss: 0.00014939535388250314\n",
      "Epoch 3291  \tTraining Loss: 0.00014741889633991057\tValidation Loss: 0.00014938453088244745\n",
      "Epoch 3292  \tTraining Loss: 0.00014740770775509802\tValidation Loss: 0.00014937371702371672\n",
      "Epoch 3293  \tTraining Loss: 0.0001473965288431344\tValidation Loss: 0.00014936291229773944\n",
      "Epoch 3294  \tTraining Loss: 0.00014738535959347906\tValidation Loss: 0.00014935211669590574\n",
      "Epoch 3295  \tTraining Loss: 0.00014737419891303134\tValidation Loss: 0.00014934134060384094\n",
      "Epoch 3296  \tTraining Loss: 0.0001473630452011119\tValidation Loss: 0.0001493305609498697\n",
      "Epoch 3297  \tTraining Loss: 0.00014735190118668\tValidation Loss: 0.00014931978966773178\n",
      "Epoch 3298  \tTraining Loss: 0.00014734076680095144\tValidation Loss: 0.00014930902771531654\n",
      "Epoch 3299  \tTraining Loss: 0.0001473296420315615\tValidation Loss: 0.00014929827504673256\n",
      "Epoch 3300  \tTraining Loss: 0.0001473185268675032\tValidation Loss: 0.0001492875315738898\n",
      "Epoch 3301  \tTraining Loss: 0.00014730742129797408\tValidation Loss: 0.00014927679724904616\n",
      "Epoch 3302  \tTraining Loss: 0.00014729632531222724\tValidation Loss: 0.00014926607205203462\n",
      "Epoch 3303  \tTraining Loss: 0.00014728523889953883\tValidation Loss: 0.0001492553559747819\n",
      "Epoch 3304  \tTraining Loss: 0.00014727416204919806\tValidation Loss: 0.00014924464901274473\n",
      "Epoch 3305  \tTraining Loss: 0.000147263094750503\tValidation Loss: 0.00014923395116130022\n",
      "Epoch 3306  \tTraining Loss: 0.0001472520369927589\tValidation Loss: 0.00014922326241459605\n",
      "Epoch 3307  \tTraining Loss: 0.00014724098876527656\tValidation Loss: 0.00014921258276541797\n",
      "Epoch 3308  \tTraining Loss: 0.0001472299500573719\tValidation Loss: 0.0001492019122053983\n",
      "Epoch 3309  \tTraining Loss: 0.00014721892085836527\tValidation Loss: 0.00014919125072528975\n",
      "Epoch 3310  \tTraining Loss: 0.0001472079011575811\tValidation Loss: 0.00014918059831520858\n",
      "Epoch 3311  \tTraining Loss: 0.0001471968909443475\tValidation Loss: 0.000149169954964825\n",
      "Epoch 3312  \tTraining Loss: 0.0001471858902079961\tValidation Loss: 0.00014915932066350307\n",
      "Epoch 3313  \tTraining Loss: 0.00014717489893786154\tValidation Loss: 0.0001491486954004013\n",
      "Epoch 3314  \tTraining Loss: 0.00014716391712328163\tValidation Loss: 0.00014913807916454316\n",
      "Epoch 3315  \tTraining Loss: 0.00014715294475359686\tValidation Loss: 0.00014912747194486677\n",
      "Epoch 3316  \tTraining Loss: 0.00014714198181815024\tValidation Loss: 0.00014911687373025926\n",
      "Epoch 3317  \tTraining Loss: 0.0001471310283062874\tValidation Loss: 0.00014910628450958061\n",
      "Epoch 3318  \tTraining Loss: 0.00014712008420735627\tValidation Loss: 0.00014909570427168028\n",
      "Epoch 3319  \tTraining Loss: 0.00014710914951070693\tValidation Loss: 0.00014908513300540858\n",
      "Epoch 3320  \tTraining Loss: 0.00014709822420569152\tValidation Loss: 0.00014907457069962486\n",
      "Epoch 3321  \tTraining Loss: 0.00014708730828166418\tValidation Loss: 0.00014906401734320253\n",
      "Epoch 3322  \tTraining Loss: 0.00014707640172798104\tValidation Loss: 0.00014905347292503289\n",
      "Epoch 3323  \tTraining Loss: 0.00014706550453400004\tValidation Loss: 0.00014904293743402762\n",
      "Epoch 3324  \tTraining Loss: 0.0001470546166890808\tValidation Loss: 0.0001490324108591205\n",
      "Epoch 3325  \tTraining Loss: 0.0001470437381825847\tValidation Loss: 0.000149021893189268\n",
      "Epoch 3326  \tTraining Loss: 0.00014703286900387468\tValidation Loss: 0.00014901138441345034\n",
      "Epoch 3327  \tTraining Loss: 0.00014702200914231537\tValidation Loss: 0.00014900088452067138\n",
      "Epoch 3328  \tTraining Loss: 0.00014701115858727282\tValidation Loss: 0.0001489903934999589\n",
      "Epoch 3329  \tTraining Loss: 0.0001470003173281147\tValidation Loss: 0.0001489799113403645\n",
      "Epoch 3330  \tTraining Loss: 0.00014698948535421012\tValidation Loss: 0.0001489694380309632\n",
      "Epoch 3331  \tTraining Loss: 0.00014697866265492955\tValidation Loss: 0.0001489589735608534\n",
      "Epoch 3332  \tTraining Loss: 0.00014696784921964495\tValidation Loss: 0.0001489485179191564\n",
      "Epoch 3333  \tTraining Loss: 0.00014695704503772967\tValidation Loss: 0.00014893807109501618\n",
      "Epoch 3334  \tTraining Loss: 0.0001469462500985584\tValidation Loss: 0.00014892763307759868\n",
      "Epoch 3335  \tTraining Loss: 0.00014693546439150727\tValidation Loss: 0.0001489172038560917\n",
      "Epoch 3336  \tTraining Loss: 0.0001469246879059537\tValidation Loss: 0.0001489067834197045\n",
      "Epoch 3337  \tTraining Loss: 0.00014691392063127657\tValidation Loss: 0.00014889637175766695\n",
      "Epoch 3338  \tTraining Loss: 0.00014690316255685598\tValidation Loss: 0.00014888596885922947\n",
      "Epoch 3339  \tTraining Loss: 0.00014689241367207346\tValidation Loss: 0.00014887557471366254\n",
      "Epoch 3340  \tTraining Loss: 0.00014688167396631184\tValidation Loss: 0.00014886518931025612\n",
      "Epoch 3341  \tTraining Loss: 0.00014687094342895544\tValidation Loss: 0.0001488548126383192\n",
      "Epoch 3342  \tTraining Loss: 0.00014686022204938988\tValidation Loss: 0.00014884444468717966\n",
      "Epoch 3343  \tTraining Loss: 0.0001468495098170021\tValidation Loss: 0.00014883408544618334\n",
      "Epoch 3344  \tTraining Loss: 0.00014683880672118044\tValidation Loss: 0.0001488237349046942\n",
      "Epoch 3345  \tTraining Loss: 0.00014682811275131496\tValidation Loss: 0.00014881339305209348\n",
      "Epoch 3346  \tTraining Loss: 0.00014681742789679662\tValidation Loss: 0.00014880305987777942\n",
      "Epoch 3347  \tTraining Loss: 0.00014680675214701828\tValidation Loss: 0.00014879273537116692\n",
      "Epoch 3348  \tTraining Loss: 0.00014679608460922743\tValidation Loss: 0.00014878241029669438\n",
      "Epoch 3349  \tTraining Loss: 0.0001467854227337903\tValidation Loss: 0.0001487720964331484\n",
      "Epoch 3350  \tTraining Loss: 0.000146774769955537\tValidation Loss: 0.0001487617937175788\n",
      "Epoch 3351  \tTraining Loss: 0.000146764126238199\tValidation Loss: 0.00014875150044101453\n",
      "Epoch 3352  \tTraining Loss: 0.00014675349156996828\tValidation Loss: 0.00014874121603022916\n",
      "Epoch 3353  \tTraining Loss: 0.00014674286594001018\tValidation Loss: 0.00014873094023985298\n",
      "Epoch 3354  \tTraining Loss: 0.00014673224933761965\tValidation Loss: 0.00014872067297346417\n",
      "Epoch 3355  \tTraining Loss: 0.00014672164175213081\tValidation Loss: 0.00014871041419996746\n",
      "Epoch 3356  \tTraining Loss: 0.00014671104317289622\tValidation Loss: 0.0001487001639128703\n",
      "Epoch 3357  \tTraining Loss: 0.0001467004535892787\tValidation Loss: 0.00014868992211268493\n",
      "Epoch 3358  \tTraining Loss: 0.00014668987299064798\tValidation Loss: 0.00014867968880020159\n",
      "Epoch 3359  \tTraining Loss: 0.0001466793013663787\tValidation Loss: 0.0001486694639743892\n",
      "Epoch 3360  \tTraining Loss: 0.0001466687387058497\tValidation Loss: 0.0001486592476320843\n",
      "Epoch 3361  \tTraining Loss: 0.00014665818499844357\tValidation Loss: 0.0001486490397682741\n",
      "Epoch 3362  \tTraining Loss: 0.00014664764023354627\tValidation Loss: 0.0001486388403765073\n",
      "Epoch 3363  \tTraining Loss: 0.00014663710440054694\tValidation Loss: 0.00014862864944927137\n",
      "Epoch 3364  \tTraining Loss: 0.00014662657748883804\tValidation Loss: 0.00014861846697829378\n",
      "Epoch 3365  \tTraining Loss: 0.00014661605948781509\tValidation Loss: 0.00014860829295476705\n",
      "Epoch 3366  \tTraining Loss: 0.00014660555038687668\tValidation Loss: 0.00014859812736951245\n",
      "Epoch 3367  \tTraining Loss: 0.00014659505017542435\tValidation Loss: 0.00014858797021309584\n",
      "Epoch 3368  \tTraining Loss: 0.00014658455884286302\tValidation Loss: 0.00014857782147590932\n",
      "Epoch 3369  \tTraining Loss: 0.00014657407656791464\tValidation Loss: 0.00014856765517016904\n",
      "Epoch 3370  \tTraining Loss: 0.0001465636032807669\tValidation Loss: 0.00014855753720042407\n",
      "Epoch 3371  \tTraining Loss: 0.00014655313890299645\tValidation Loss: 0.00014854739054592954\n",
      "Epoch 3372  \tTraining Loss: 0.00014654268319633755\tValidation Loss: 0.00014853726430254398\n",
      "Epoch 3373  \tTraining Loss: 0.00014653223644229554\tValidation Loss: 0.00014852717479391567\n",
      "Epoch 3374  \tTraining Loss: 0.00014652179859875886\tValidation Loss: 0.00014851705455486171\n",
      "Epoch 3375  \tTraining Loss: 0.00014651136934746772\tValidation Loss: 0.00014850695420795112\n",
      "Epoch 3376  \tTraining Loss: 0.00014650094893883271\tValidation Loss: 0.00014849686442693834\n",
      "Epoch 3377  \tTraining Loss: 0.0001464905374655673\tValidation Loss: 0.00014848680940652724\n",
      "Epoch 3378  \tTraining Loss: 0.00014648013469442198\tValidation Loss: 0.00014847672329463708\n",
      "Epoch 3379  \tTraining Loss: 0.0001464697405585503\tValidation Loss: 0.00014846665680757159\n",
      "Epoch 3380  \tTraining Loss: 0.00014645935524115357\tValidation Loss: 0.0001484566265245107\n",
      "Epoch 3381  \tTraining Loss: 0.0001464489788497223\tValidation Loss: 0.00014844656533543143\n",
      "Epoch 3382  \tTraining Loss: 0.00014643861093462834\tValidation Loss: 0.00014843652375281155\n",
      "Epoch 3383  \tTraining Loss: 0.00014642825178610622\tValidation Loss: 0.00014842649254095876\n",
      "Epoch 3384  \tTraining Loss: 0.00014641790141989758\tValidation Loss: 0.00014841649584985392\n",
      "Epoch 3385  \tTraining Loss: 0.00014640755983191773\tValidation Loss: 0.0001484064680699042\n",
      "Epoch 3386  \tTraining Loss: 0.0001463972267326055\tValidation Loss: 0.00014839645975282437\n",
      "Epoch 3387  \tTraining Loss: 0.00014638690235637493\tValidation Loss: 0.00014838646171441556\n",
      "Epoch 3388  \tTraining Loss: 0.00014637658675355927\tValidation Loss: 0.00014837649807699748\n",
      "Epoch 3389  \tTraining Loss: 0.00014636627981659398\tValidation Loss: 0.00014836650336311715\n",
      "Epoch 3390  \tTraining Loss: 0.00014635598136243763\tValidation Loss: 0.00014835652803207576\n",
      "Epoch 3391  \tTraining Loss: 0.00014634569158885393\tValidation Loss: 0.00014834656292607036\n",
      "Epoch 3392  \tTraining Loss: 0.00014633541057244593\tValidation Loss: 0.0001483366321276537\n",
      "Epoch 3393  \tTraining Loss: 0.00014632513812542816\tValidation Loss: 0.00014832667028095027\n",
      "Epoch 3394  \tTraining Loss: 0.0001463148741472334\tValidation Loss: 0.00014831672774874626\n",
      "Epoch 3395  \tTraining Loss: 0.00014630461843108725\tValidation Loss: 0.00014830678543385336\n",
      "Epoch 3396  \tTraining Loss: 0.00014629436223400486\tValidation Loss: 0.00014829688376106893\n",
      "Epoch 3397  \tTraining Loss: 0.00014628411470721364\tValidation Loss: 0.00014828695773782305\n",
      "Epoch 3398  \tTraining Loss: 0.00014627387567011524\tValidation Loss: 0.000148277053004138\n",
      "Epoch 3399  \tTraining Loss: 0.00014626364525506487\tValidation Loss: 0.00014826715896837137\n",
      "Epoch 3400  \tTraining Loss: 0.00014625342354698738\tValidation Loss: 0.00014825729901248247\n",
      "Epoch 3401  \tTraining Loss: 0.0001462432103208646\tValidation Loss: 0.00014824740784689857\n",
      "Epoch 3402  \tTraining Loss: 0.00014623300552192434\tValidation Loss: 0.00014823753556252836\n",
      "Epoch 3403  \tTraining Loss: 0.00014622280941242524\tValidation Loss: 0.00014822767677723515\n",
      "Epoch 3404  \tTraining Loss: 0.0001462126222673444\tValidation Loss: 0.00014821785015659613\n",
      "Epoch 3405  \tTraining Loss: 0.00014620244359536094\tValidation Loss: 0.0001482079907164641\n",
      "Epoch 3406  \tTraining Loss: 0.0001461922729656049\tValidation Loss: 0.000148198151436143\n",
      "Epoch 3407  \tTraining Loss: 0.00014618211011337568\tValidation Loss: 0.00014818831864261422\n",
      "Epoch 3408  \tTraining Loss: 0.00014617195584444866\tValidation Loss: 0.00014817851987910299\n",
      "Epoch 3409  \tTraining Loss: 0.00014616181002113669\tValidation Loss: 0.00014816869026177553\n",
      "Epoch 3410  \tTraining Loss: 0.00014615167250084976\tValidation Loss: 0.00014815887970843654\n",
      "Epoch 3411  \tTraining Loss: 0.0001461415434621896\tValidation Loss: 0.00014814907915706568\n",
      "Epoch 3412  \tTraining Loss: 0.00014613142294083147\tValidation Loss: 0.0001481393124525936\n",
      "Epoch 3413  \tTraining Loss: 0.0001461213108599615\tValidation Loss: 0.00014812951484869869\n",
      "Epoch 3414  \tTraining Loss: 0.0001461112070218089\tValidation Loss: 0.0001481197361859667\n",
      "Epoch 3415  \tTraining Loss: 0.00014610111162288275\tValidation Loss: 0.00014810996744332985\n",
      "Epoch 3416  \tTraining Loss: 0.00014609102467714945\tValidation Loss: 0.00014810023243702175\n",
      "Epoch 3417  \tTraining Loss: 0.0001460809461725386\tValidation Loss: 0.00014809046654435718\n",
      "Epoch 3418  \tTraining Loss: 0.00014607087584768395\tValidation Loss: 0.0001480807195162995\n",
      "Epoch 3419  \tTraining Loss: 0.00014606081391983343\tValidation Loss: 0.00014807098235692257\n",
      "Epoch 3420  \tTraining Loss: 0.0001460507603800264\tValidation Loss: 0.00014806127884451416\n",
      "Epoch 3421  \tTraining Loss: 0.00014604071528488016\tValidation Loss: 0.0001480515444722996\n",
      "Epoch 3422  \tTraining Loss: 0.00014603067830516603\tValidation Loss: 0.00014804182889854013\n",
      "Epoch 3423  \tTraining Loss: 0.00014602064968040232\tValidation Loss: 0.00014803212314928884\n",
      "Epoch 3424  \tTraining Loss: 0.00014601062937848785\tValidation Loss: 0.000148022450963317\n",
      "Epoch 3425  \tTraining Loss: 0.00014600061752553713\tValidation Loss: 0.0001480127479469779\n",
      "Epoch 3426  \tTraining Loss: 0.00014599061372341405\tValidation Loss: 0.00014800306366596165\n",
      "Epoch 3427  \tTraining Loss: 0.00014598061823433342\tValidation Loss: 0.0001479933891673888\n",
      "Epoch 3428  \tTraining Loss: 0.000145970631005772\tValidation Loss: 0.00014798372288001325\n",
      "Epoch 3429  \tTraining Loss: 0.00014596065217441977\tValidation Loss: 0.00014797408984123535\n",
      "Epoch 3430  \tTraining Loss: 0.00014595068148092236\tValidation Loss: 0.00014796442593677543\n",
      "Epoch 3431  \tTraining Loss: 0.0001459407189168072\tValidation Loss: 0.000147954780648928\n",
      "Epoch 3432  \tTraining Loss: 0.00014593076461318346\tValidation Loss: 0.0001479451450648009\n",
      "Epoch 3433  \tTraining Loss: 0.0001459208188509885\tValidation Loss: 0.00014793557089476687\n",
      "Epoch 3434  \tTraining Loss: 0.00014591088127691477\tValidation Loss: 0.00014792590703981007\n",
      "Epoch 3435  \tTraining Loss: 0.00014590095139712105\tValidation Loss: 0.00014791628683634902\n",
      "Epoch 3436  \tTraining Loss: 0.0001458910298485841\tValidation Loss: 0.00014790668037755782\n",
      "Epoch 3437  \tTraining Loss: 0.0001458811168263335\tValidation Loss: 0.00014789713612877082\n",
      "Epoch 3438  \tTraining Loss: 0.00014587121191042259\tValidation Loss: 0.00014788750236233842\n",
      "Epoch 3439  \tTraining Loss: 0.00014586131466410259\tValidation Loss: 0.00014787791249176374\n",
      "Epoch 3440  \tTraining Loss: 0.00014585142570877854\tValidation Loss: 0.0001478683365248973\n",
      "Epoch 3441  \tTraining Loss: 0.00014584154527686095\tValidation Loss: 0.00014785882293530611\n",
      "Epoch 3442  \tTraining Loss: 0.00014583167283576792\tValidation Loss: 0.00014784921976947223\n",
      "Epoch 3443  \tTraining Loss: 0.00014582180805878606\tValidation Loss: 0.00014783966056741547\n",
      "Epoch 3444  \tTraining Loss: 0.00014581195154940698\tValidation Loss: 0.00014783014364488537\n",
      "Epoch 3445  \tTraining Loss: 0.0001458021036267085\tValidation Loss: 0.00014782060134278446\n",
      "Epoch 3446  \tTraining Loss: 0.00014579226327387013\tValidation Loss: 0.00014781105402873027\n",
      "Epoch 3447  \tTraining Loss: 0.00014578243091503484\tValidation Loss: 0.00014780152954632465\n",
      "Epoch 3448  \tTraining Loss: 0.00014577260675408134\tValidation Loss: 0.0001477920440939147\n",
      "Epoch 3449  \tTraining Loss: 0.00014576279095835864\tValidation Loss: 0.00014778253257482062\n",
      "Epoch 3450  \tTraining Loss: 0.00014575298280332323\tValidation Loss: 0.00014777301604532045\n",
      "Epoch 3451  \tTraining Loss: 0.00014574318258524314\tValidation Loss: 0.00014776352224470565\n",
      "Epoch 3452  \tTraining Loss: 0.00014573339059459274\tValidation Loss: 0.00014775406749010662\n",
      "Epoch 3453  \tTraining Loss: 0.00014572360677756282\tValidation Loss: 0.00014774458636956377\n",
      "Epoch 3454  \tTraining Loss: 0.00014571383064980318\tValidation Loss: 0.00014773510029297894\n",
      "Epoch 3455  \tTraining Loss: 0.00014570406240818357\tValidation Loss: 0.00014772563687506646\n",
      "Epoch 3456  \tTraining Loss: 0.00014569430242387273\tValidation Loss: 0.0001477162125522265\n",
      "Epoch 3457  \tTraining Loss: 0.00014568455042643155\tValidation Loss: 0.00014770676159372762\n",
      "Epoch 3458  \tTraining Loss: 0.00014567480615763846\tValidation Loss: 0.00014769730575534928\n",
      "Epoch 3459  \tTraining Loss: 0.00014566506972891693\tValidation Loss: 0.0001476878725223356\n",
      "Epoch 3460  \tTraining Loss: 0.0001456553415876528\tValidation Loss: 0.000147678478445117\n",
      "Epoch 3461  \tTraining Loss: 0.0001456456212511831\tValidation Loss: 0.00014766905747150622\n",
      "Epoch 3462  \tTraining Loss: 0.00014563590867331798\tValidation Loss: 0.00014765963169907843\n",
      "Epoch 3463  \tTraining Loss: 0.0001456262039190159\tValidation Loss: 0.0001476502573466074\n",
      "Epoch 3464  \tTraining Loss: 0.00014561650752094793\tValidation Loss: 0.00014764083300054098\n",
      "Epoch 3465  \tTraining Loss: 0.00014560681849008617\tValidation Loss: 0.00014763146737370008\n",
      "Epoch 3466  \tTraining Loss: 0.00014559713753906176\tValidation Loss: 0.00014762207566206928\n",
      "Epoch 3467  \tTraining Loss: 0.0001455874643632262\tValidation Loss: 0.00014761273205512836\n",
      "Epoch 3468  \tTraining Loss: 0.00014557779932297882\tValidation Loss: 0.00014760333779139203\n",
      "Epoch 3469  \tTraining Loss: 0.0001455681417102231\tValidation Loss: 0.00014759400213751068\n",
      "Epoch 3470  \tTraining Loss: 0.00014555849211592194\tValidation Loss: 0.00014758464037315682\n",
      "Epoch 3471  \tTraining Loss: 0.00014554885034003188\tValidation Loss: 0.00014757532670890247\n",
      "Epoch 3472  \tTraining Loss: 0.0001455392165095911\tValidation Loss: 0.0001475659621270905\n",
      "Epoch 3473  \tTraining Loss: 0.00014552959015743834\tValidation Loss: 0.00014755665610883298\n",
      "Epoch 3474  \tTraining Loss: 0.00014551997175126375\tValidation Loss: 0.00014754732399122248\n",
      "Epoch 3475  \tTraining Loss: 0.000145510361211556\tValidation Loss: 0.00014753804000582464\n",
      "Epoch 3476  \tTraining Loss: 0.00014550075843139452\tValidation Loss: 0.0001475287048728667\n",
      "Epoch 3477  \tTraining Loss: 0.00014549116318455396\tValidation Loss: 0.00014751942828136372\n",
      "Epoch 3478  \tTraining Loss: 0.00014548157579881\tValidation Loss: 0.0001475101256180882\n",
      "Epoch 3479  \tTraining Loss: 0.0001454719963325796\tValidation Loss: 0.00014750087113162065\n",
      "Epoch 3480  \tTraining Loss: 0.00014546242444298888\tValidation Loss: 0.00014749156527611994\n",
      "Epoch 3481  \tTraining Loss: 0.00014545286014688922\tValidation Loss: 0.00014748231794624008\n",
      "Epoch 3482  \tTraining Loss: 0.00014544330364999353\tValidation Loss: 0.00014747307393954447\n",
      "Epoch 3483  \tTraining Loss: 0.00014543375513062623\tValidation Loss: 0.00014746378755261066\n",
      "Epoch 3484  \tTraining Loss: 0.0001454242137858935\tValidation Loss: 0.00014745456135818857\n",
      "Epoch 3485  \tTraining Loss: 0.0001454146804395776\tValidation Loss: 0.0001474453094502172\n",
      "Epoch 3486  \tTraining Loss: 0.00014540515463352816\tValidation Loss: 0.00014743610596727784\n",
      "Epoch 3487  \tTraining Loss: 0.00014539563674622688\tValidation Loss: 0.00014742685079840233\n",
      "Epoch 3488  \tTraining Loss: 0.00014538612609599704\tValidation Loss: 0.00014741765420075463\n",
      "Epoch 3489  \tTraining Loss: 0.00014537662330174866\tValidation Loss: 0.0001474084316651762\n",
      "Epoch 3490  \tTraining Loss: 0.00014536712812922124\tValidation Loss: 0.00014739925748927216\n",
      "Epoch 3491  \tTraining Loss: 0.00014535764069192162\tValidation Loss: 0.00014739003133384037\n",
      "Epoch 3492  \tTraining Loss: 0.00014534816057398776\tValidation Loss: 0.00014738086368509508\n",
      "Epoch 3493  \tTraining Loss: 0.00014533868817125418\tValidation Loss: 0.00014737167009425074\n",
      "Epoch 3494  \tTraining Loss: 0.00014532922346953018\tValidation Loss: 0.00014736252488958807\n",
      "Epoch 3495  \tTraining Loss: 0.00014531976632508277\tValidation Loss: 0.00014735332747213374\n",
      "Epoch 3496  \tTraining Loss: 0.00014531031658670233\tValidation Loss: 0.00014734418853845898\n",
      "Epoch 3497  \tTraining Loss: 0.00014530087440986638\tValidation Loss: 0.00014733502368485314\n",
      "Epoch 3498  \tTraining Loss: 0.00014529144001787966\tValidation Loss: 0.00014732590726243056\n",
      "Epoch 3499  \tTraining Loss: 0.0001452820130572437\tValidation Loss: 0.00014731676300734135\n",
      "Epoch 3500  \tTraining Loss: 0.0001452725934906298\tValidation Loss: 0.0001473076149285109\n",
      "Epoch 3501  \tTraining Loss: 0.00014526318137421154\tValidation Loss: 0.00014729851892103713\n",
      "Epoch 3502  \tTraining Loss: 0.00014525377721277106\tValidation Loss: 0.00014728937084536797\n",
      "Epoch 3503  \tTraining Loss: 0.0001452443801262139\tValidation Loss: 0.00014728028140405694\n",
      "Epoch 3504  \tTraining Loss: 0.00014523499059075875\tValidation Loss: 0.00014727116620439373\n",
      "Epoch 3505  \tTraining Loss: 0.0001452256085246654\tValidation Loss: 0.00014726209962723645\n",
      "Epoch 3506  \tTraining Loss: 0.0001452162342415803\tValidation Loss: 0.0001472530048102671\n",
      "Epoch 3507  \tTraining Loss: 0.0001452068670968805\tValidation Loss: 0.00014724390635238063\n",
      "Epoch 3508  \tTraining Loss: 0.00014519750727937122\tValidation Loss: 0.00014723483006442024\n",
      "Epoch 3509  \tTraining Loss: 0.0001451881551288474\tValidation Loss: 0.00014722579385686546\n",
      "Epoch 3510  \tTraining Loss: 0.00014517881064675706\tValidation Loss: 0.00014721672761360216\n",
      "Epoch 3511  \tTraining Loss: 0.00014516947313710933\tValidation Loss: 0.00014720765757499665\n",
      "Epoch 3512  \tTraining Loss: 0.0001451601430109464\tValidation Loss: 0.0001471986095786256\n",
      "Epoch 3513  \tTraining Loss: 0.0001451508206214788\tValidation Loss: 0.00014718962616870352\n",
      "Epoch 3514  \tTraining Loss: 0.00014514150574061982\tValidation Loss: 0.00014718055044210448\n",
      "Epoch 3515  \tTraining Loss: 0.00014513219766140685\tValidation Loss: 0.0001471715177595036\n",
      "Epoch 3516  \tTraining Loss: 0.00014512289719715852\tValidation Loss: 0.00014716249883552537\n",
      "Epoch 3517  \tTraining Loss: 0.00014511360455658266\tValidation Loss: 0.0001471535433543059\n",
      "Epoch 3518  \tTraining Loss: 0.00014510431903971106\tValidation Loss: 0.00014714449533610502\n",
      "Epoch 3519  \tTraining Loss: 0.00014509504045168717\tValidation Loss: 0.00014713549143559112\n",
      "Epoch 3520  \tTraining Loss: 0.00014508576943928308\tValidation Loss: 0.00014712652541266235\n",
      "Epoch 3521  \tTraining Loss: 0.00014507650615604495\tValidation Loss: 0.0001471175606835802\n",
      "Epoch 3522  \tTraining Loss: 0.00014506724974859995\tValidation Loss: 0.00014710855085493132\n",
      "Epoch 3523  \tTraining Loss: 0.00014505800048768806\tValidation Loss: 0.00014709960101287126\n",
      "Epoch 3524  \tTraining Loss: 0.0001450487588397184\tValidation Loss: 0.00014709065614798065\n",
      "Epoch 3525  \tTraining Loss: 0.00014503952460227096\tValidation Loss: 0.0001470816665202555\n",
      "Epoch 3526  \tTraining Loss: 0.00014503029745610108\tValidation Loss: 0.0001470727219685938\n",
      "Epoch 3527  \tTraining Loss: 0.0001450210791297941\tValidation Loss: 0.00014706378659725105\n",
      "Epoch 3528  \tTraining Loss: 0.00014501186815603739\tValidation Loss: 0.00014705485789969533\n",
      "Epoch 3529  \tTraining Loss: 0.00014500266455879328\tValidation Loss: 0.00014704588552869006\n",
      "Epoch 3530  \tTraining Loss: 0.0001449934678615474\tValidation Loss: 0.00014703697396629454\n",
      "Epoch 3531  \tTraining Loss: 0.00014498427852770531\tValidation Loss: 0.00014702803747674713\n",
      "Epoch 3532  \tTraining Loss: 0.0001449750964803778\tValidation Loss: 0.00014701915032881385\n",
      "Epoch 3533  \tTraining Loss: 0.00014496592172293706\tValidation Loss: 0.00014701023346571528\n",
      "Epoch 3534  \tTraining Loss: 0.00014495675394747654\tValidation Loss: 0.00014700131367618093\n",
      "Epoch 3535  \tTraining Loss: 0.00014494759323448097\tValidation Loss: 0.00014699241593795707\n",
      "Epoch 3536  \tTraining Loss: 0.00014493844007554675\tValidation Loss: 0.00014698355889446676\n",
      "Epoch 3537  \tTraining Loss: 0.00014492929407680698\tValidation Loss: 0.00014697467021781874\n",
      "Epoch 3538  \tTraining Loss: 0.00014492015483945434\tValidation Loss: 0.0001469657783677165\n",
      "Epoch 3539  \tTraining Loss: 0.00014491102281539368\tValidation Loss: 0.00014695693924827172\n",
      "Epoch 3540  \tTraining Loss: 0.00014490189848330575\tValidation Loss: 0.0001469480702833769\n",
      "Epoch 3541  \tTraining Loss: 0.0001448927806971296\tValidation Loss: 0.00014693919853280539\n",
      "Epoch 3542  \tTraining Loss: 0.0001448836699944037\tValidation Loss: 0.00014693034876574518\n",
      "Epoch 3543  \tTraining Loss: 0.00014487456669410594\tValidation Loss: 0.00014692156408416395\n",
      "Epoch 3544  \tTraining Loss: 0.0001448654707174144\tValidation Loss: 0.00014691268594596826\n",
      "Epoch 3545  \tTraining Loss: 0.0001448563811855863\tValidation Loss: 0.00014690385210885902\n",
      "Epoch 3546  \tTraining Loss: 0.00014484729895423087\tValidation Loss: 0.00014689505590520025\n",
      "Epoch 3547  \tTraining Loss: 0.00014483822415507333\tValidation Loss: 0.00014688626192555345\n",
      "Epoch 3548  \tTraining Loss: 0.00014482915618540482\tValidation Loss: 0.00014687742135788474\n",
      "Epoch 3549  \tTraining Loss: 0.0001448200949943439\tValidation Loss: 0.00014686864080310297\n",
      "Epoch 3550  \tTraining Loss: 0.0001448110411104135\tValidation Loss: 0.00014685983502458127\n",
      "Epoch 3551  \tTraining Loss: 0.00014480199436235066\tValidation Loss: 0.00014685107858911209\n",
      "Epoch 3552  \tTraining Loss: 0.00014479295450127232\tValidation Loss: 0.00014684229105601092\n",
      "Epoch 3553  \tTraining Loss: 0.00014478392158656566\tValidation Loss: 0.00014683350079918437\n",
      "Epoch 3554  \tTraining Loss: 0.0001447748955773856\tValidation Loss: 0.0001468247635700749\n",
      "Epoch 3555  \tTraining Loss: 0.00014476587700124144\tValidation Loss: 0.00014681599557595244\n",
      "Epoch 3556  \tTraining Loss: 0.00014475686506893538\tValidation Loss: 0.00014680722507184874\n",
      "Epoch 3557  \tTraining Loss: 0.0001447478599529202\tValidation Loss: 0.0001467984764084496\n",
      "Epoch 3558  \tTraining Loss: 0.00014473886200692908\tValidation Loss: 0.00014678979298450847\n",
      "Epoch 3559  \tTraining Loss: 0.00014472987143090022\tValidation Loss: 0.00014678101544175846\n",
      "Epoch 3560  \tTraining Loss: 0.00014472088705826602\tValidation Loss: 0.0001467722822271101\n",
      "Epoch 3561  \tTraining Loss: 0.0001447119097938724\tValidation Loss: 0.00014676358643639064\n",
      "Epoch 3562  \tTraining Loss: 0.000144702939915855\tValidation Loss: 0.00014675489330688856\n",
      "Epoch 3563  \tTraining Loss: 0.00014469397669468703\tValidation Loss: 0.00014674615269823566\n",
      "Epoch 3564  \tTraining Loss: 0.0001446850200772746\tValidation Loss: 0.00014673747207072412\n",
      "Epoch 3565  \tTraining Loss: 0.00014467607067133643\tValidation Loss: 0.00014672876629402815\n",
      "Epoch 3566  \tTraining Loss: 0.00014466712821266462\tValidation Loss: 0.00014672011007329236\n",
      "Epoch 3567  \tTraining Loss: 0.0001446581925559315\tValidation Loss: 0.00014671142180751968\n",
      "Epoch 3568  \tTraining Loss: 0.0001446492636634276\tValidation Loss: 0.00014670273106710368\n",
      "Epoch 3569  \tTraining Loss: 0.00014464034151373753\tValidation Loss: 0.00014669409360190702\n",
      "Epoch 3570  \tTraining Loss: 0.000144631426759405\tValidation Loss: 0.00014668542443597722\n",
      "Epoch 3571  \tTraining Loss: 0.00014462251839556905\tValidation Loss: 0.0001466767530123385\n",
      "Epoch 3572  \tTraining Loss: 0.00014461361674624956\tValidation Loss: 0.00014666810328837907\n",
      "Epoch 3573  \tTraining Loss: 0.00014460472216914176\tValidation Loss: 0.00014665951896406415\n",
      "Epoch 3574  \tTraining Loss: 0.00014459583474613056\tValidation Loss: 0.00014665083985152552\n",
      "Epoch 3575  \tTraining Loss: 0.00014458695342004367\tValidation Loss: 0.00014664220510151978\n",
      "Epoch 3576  \tTraining Loss: 0.00014457807917343102\tValidation Loss: 0.0001466336075769356\n",
      "Epoch 3577  \tTraining Loss: 0.00014456921191768245\tValidation Loss: 0.00014662501314323563\n",
      "Epoch 3578  \tTraining Loss: 0.00014456035136951536\tValidation Loss: 0.00014661637035083162\n",
      "Epoch 3579  \tTraining Loss: 0.00014455149738377285\tValidation Loss: 0.00014660778751828655\n",
      "Epoch 3580  \tTraining Loss: 0.0001445426501944359\tValidation Loss: 0.00014659917960498979\n",
      "Epoch 3581  \tTraining Loss: 0.0001445338098962096\tValidation Loss: 0.00014659062147054762\n",
      "Epoch 3582  \tTraining Loss: 0.00014452497653967124\tValidation Loss: 0.0001465820303569267\n",
      "Epoch 3583  \tTraining Loss: 0.00014451614941524395\tValidation Loss: 0.00014657343701043928\n",
      "Epoch 3584  \tTraining Loss: 0.00014450732907535032\tValidation Loss: 0.00014656488911114217\n",
      "Epoch 3585  \tTraining Loss: 0.00014449851601688003\tValidation Loss: 0.00014655634601849603\n",
      "Epoch 3586  \tTraining Loss: 0.00014448970920696969\tValidation Loss: 0.00014654775433146005\n",
      "Epoch 3587  \tTraining Loss: 0.00014448090898881176\tValidation Loss: 0.00014653922266565808\n",
      "Epoch 3588  \tTraining Loss: 0.0001444721156200024\tValidation Loss: 0.00014653069813670355\n",
      "Epoch 3589  \tTraining Loss: 0.00014446332918024784\tValidation Loss: 0.00014652212518463355\n",
      "Epoch 3590  \tTraining Loss: 0.0001444545490587476\tValidation Loss: 0.0001465136123624011\n",
      "Epoch 3591  \tTraining Loss: 0.00014444577559443635\tValidation Loss: 0.00014650507462054356\n",
      "Epoch 3592  \tTraining Loss: 0.0001444370088093379\tValidation Loss: 0.00014649661082808023\n",
      "Epoch 3593  \tTraining Loss: 0.0001444282491857799\tValidation Loss: 0.00014648805292664732\n",
      "Epoch 3594  \tTraining Loss: 0.00014441949540525904\tValidation Loss: 0.00014647953967103638\n",
      "Epoch 3595  \tTraining Loss: 0.00014441074853872052\tValidation Loss: 0.00014647106348690824\n",
      "Epoch 3596  \tTraining Loss: 0.0001444020084783757\tValidation Loss: 0.00014646259097433176\n",
      "Epoch 3597  \tTraining Loss: 0.00014439327497536695\tValidation Loss: 0.0001464540690372568\n",
      "Epoch 3598  \tTraining Loss: 0.00014438454789687807\tValidation Loss: 0.0001464456070622666\n",
      "Epoch 3599  \tTraining Loss: 0.000144375827353372\tValidation Loss: 0.00014643712010095836\n",
      "Epoch 3600  \tTraining Loss: 0.00014436711360660091\tValidation Loss: 0.00014642870707618578\n",
      "Epoch 3601  \tTraining Loss: 0.00014435840661694959\tValidation Loss: 0.00014642019954072652\n",
      "Epoch 3602  \tTraining Loss: 0.0001443497055622293\tValidation Loss: 0.0001464117366347936\n",
      "Epoch 3603  \tTraining Loss: 0.00014434101140946474\tValidation Loss: 0.00014640331067619072\n",
      "Epoch 3604  \tTraining Loss: 0.00014433232399614344\tValidation Loss: 0.000146394888583503\n",
      "Epoch 3605  \tTraining Loss: 0.00014432364293137602\tValidation Loss: 0.00014638644043521616\n",
      "Epoch 3606  \tTraining Loss: 0.000144314968300818\tValidation Loss: 0.000146377991948804\n",
      "Epoch 3607  \tTraining Loss: 0.0001443063000288048\tValidation Loss: 0.00014636956499235602\n",
      "Epoch 3608  \tTraining Loss: 0.0001442976388441866\tValidation Loss: 0.00014636120382666373\n",
      "Epoch 3609  \tTraining Loss: 0.0001442889838570062\tValidation Loss: 0.00014635274632981483\n",
      "Epoch 3610  \tTraining Loss: 0.00014428033501395058\tValidation Loss: 0.00014634435708094266\n",
      "Epoch 3611  \tTraining Loss: 0.00014427169300948144\tValidation Loss: 0.00014633597714604237\n",
      "Epoch 3612  \tTraining Loss: 0.00014426305763824172\tValidation Loss: 0.00014632754770714773\n",
      "Epoch 3613  \tTraining Loss: 0.00014425442892395188\tValidation Loss: 0.00014631927169499127\n",
      "Epoch 3614  \tTraining Loss: 0.000144245810487534\tValidation Loss: 0.00014631091629047167\n",
      "Epoch 3615  \tTraining Loss: 0.00014423719872044035\tValidation Loss: 0.00014630262190526287\n",
      "Epoch 3616  \tTraining Loss: 0.00014422859381668863\tValidation Loss: 0.0001462942285243164\n",
      "Epoch 3617  \tTraining Loss: 0.00014421999458016444\tValidation Loss: 0.00014628587848637128\n",
      "Epoch 3618  \tTraining Loss: 0.00014421140217112663\tValidation Loss: 0.00014627756468295022\n",
      "Epoch 3619  \tTraining Loss: 0.00014420281627684927\tValidation Loss: 0.00014626925494496296\n",
      "Epoch 3620  \tTraining Loss: 0.00014419423673961768\tValidation Loss: 0.00014626091808762862\n",
      "Epoch 3621  \tTraining Loss: 0.0001441856633257098\tValidation Loss: 0.00014625258107884214\n",
      "Epoch 3622  \tTraining Loss: 0.00014417709633324102\tValidation Loss: 0.0001462443222425777\n",
      "Epoch 3623  \tTraining Loss: 0.00014416853654392468\tValidation Loss: 0.0001462359468209633\n",
      "Epoch 3624  \tTraining Loss: 0.00014415998505972402\tValidation Loss: 0.0001462276355897577\n",
      "Epoch 3625  \tTraining Loss: 0.00014415144050596837\tValidation Loss: 0.00014621936404329633\n",
      "Epoch 3626  \tTraining Loss: 0.00014414290248473827\tValidation Loss: 0.0001462110971686705\n",
      "Epoch 3627  \tTraining Loss: 0.00014413437071959817\tValidation Loss: 0.00014620280276998792\n",
      "Epoch 3628  \tTraining Loss: 0.00014412584504292075\tValidation Loss: 0.00014619450841990588\n",
      "Epoch 3629  \tTraining Loss: 0.00014411732578123587\tValidation Loss: 0.0001461862924030893\n",
      "Epoch 3630  \tTraining Loss: 0.00014410881333445095\tValidation Loss: 0.0001461779811106317\n",
      "Epoch 3631  \tTraining Loss: 0.00014410030637590976\tValidation Loss: 0.00014616971485065355\n",
      "Epoch 3632  \tTraining Loss: 0.00014409180611459717\tValidation Loss: 0.000146161485332933\n",
      "Epoch 3633  \tTraining Loss: 0.0001440833123788847\tValidation Loss: 0.00014615326053347155\n",
      "Epoch 3634  \tTraining Loss: 0.00014407482468637722\tValidation Loss: 0.00014614500782083185\n",
      "Epoch 3635  \tTraining Loss: 0.00014406634305714266\tValidation Loss: 0.00014613675522450367\n",
      "Epoch 3636  \tTraining Loss: 0.00014405786793160818\tValidation Loss: 0.00014612858099306977\n",
      "Epoch 3637  \tTraining Loss: 0.00014404939930561035\tValidation Loss: 0.00014612031111826826\n",
      "Epoch 3638  \tTraining Loss: 0.000144040936255402\tValidation Loss: 0.00014611210990989675\n",
      "Epoch 3639  \tTraining Loss: 0.00014403247991696564\tValidation Loss: 0.0001461039189789923\n",
      "Epoch 3640  \tTraining Loss: 0.00014402402989387058\tValidation Loss: 0.00014609567695797294\n",
      "Epoch 3641  \tTraining Loss: 0.00014401558578074782\tValidation Loss: 0.00014608749526420892\n",
      "Epoch 3642  \tTraining Loss: 0.00014400714779927266\tValidation Loss: 0.00014607928883869308\n",
      "Epoch 3643  \tTraining Loss: 0.00014399871643491318\tValidation Loss: 0.00014607115692512717\n",
      "Epoch 3644  \tTraining Loss: 0.00014399029112879637\tValidation Loss: 0.00014606292845162165\n",
      "Epoch 3645  \tTraining Loss: 0.00014398187161747416\tValidation Loss: 0.00014605477338436077\n",
      "Epoch 3646  \tTraining Loss: 0.00014397345856230104\tValidation Loss: 0.0001460466247825947\n",
      "Epoch 3647  \tTraining Loss: 0.0001439650517647626\tValidation Loss: 0.0001460384473645961\n",
      "Epoch 3648  \tTraining Loss: 0.000143956650795999\tValidation Loss: 0.0001460302702007162\n",
      "Epoch 3649  \tTraining Loss: 0.00014394825691980905\tValidation Loss: 0.00014602213551022331\n",
      "Epoch 3650  \tTraining Loss: 0.00014393987511196856\tValidation Loss: 0.00014601401025703326\n",
      "Epoch 3651  \tTraining Loss: 0.00014393149889840965\tValidation Loss: 0.00014600586069520263\n",
      "Epoch 3652  \tTraining Loss: 0.00014392312889034846\tValidation Loss: 0.00014599771278955758\n",
      "Epoch 3653  \tTraining Loss: 0.0001439147649443159\tValidation Loss: 0.00014598962025077187\n",
      "Epoch 3654  \tTraining Loss: 0.00014390640754232502\tValidation Loss: 0.00014598149096956096\n",
      "Epoch 3655  \tTraining Loss: 0.00014389805553867522\tValidation Loss: 0.00014597336054377578\n",
      "Epoch 3656  \tTraining Loss: 0.00014388970985243385\tValidation Loss: 0.0001459652745075593\n",
      "Epoch 3657  \tTraining Loss: 0.00014388137056601963\tValidation Loss: 0.0001459571950714435\n",
      "Epoch 3658  \tTraining Loss: 0.00014387303708173214\tValidation Loss: 0.00014594908633540688\n",
      "Epoch 3659  \tTraining Loss: 0.00014386470938313714\tValidation Loss: 0.00014594097793737393\n",
      "Epoch 3660  \tTraining Loss: 0.00014385638805458078\tValidation Loss: 0.00014593294814639593\n",
      "Epoch 3661  \tTraining Loss: 0.00014384807288224643\tValidation Loss: 0.00014592482158056218\n",
      "Epoch 3662  \tTraining Loss: 0.0001438397632385987\tValidation Loss: 0.00014591676356175666\n",
      "Epoch 3663  \tTraining Loss: 0.00014383145981091956\tValidation Loss: 0.0001459087164327423\n",
      "Epoch 3664  \tTraining Loss: 0.00014382316279754588\tValidation Loss: 0.00014590064039565157\n",
      "Epoch 3665  \tTraining Loss: 0.00014381487123251148\tValidation Loss: 0.0001458925650987413\n",
      "Epoch 3666  \tTraining Loss: 0.00014380658567312232\tValidation Loss: 0.00014588453452313223\n",
      "Epoch 3667  \tTraining Loss: 0.00014379830646821807\tValidation Loss: 0.00014587651107136068\n",
      "Epoch 3668  \tTraining Loss: 0.00014379003311673562\tValidation Loss: 0.00014586845789420235\n",
      "Epoch 3669  \tTraining Loss: 0.00014378176546015094\tValidation Loss: 0.00014586040532539935\n",
      "Epoch 3670  \tTraining Loss: 0.00014377350381030547\tValidation Loss: 0.00014585243161432428\n",
      "Epoch 3671  \tTraining Loss: 0.00014376524864548443\tValidation Loss: 0.00014584436074337355\n",
      "Epoch 3672  \tTraining Loss: 0.0001437569986810745\tValidation Loss: 0.00014583635848514756\n",
      "Epoch 3673  \tTraining Loss: 0.00014374875489280942\tValidation Loss: 0.0001458283330840293\n",
      "Epoch 3674  \tTraining Loss: 0.00014374051720158067\tValidation Loss: 0.00014582038275359738\n",
      "Epoch 3675  \tTraining Loss: 0.0001437322854855427\tValidation Loss: 0.00014581233453233485\n",
      "Epoch 3676  \tTraining Loss: 0.000143724059185827\tValidation Loss: 0.00014580435476646916\n",
      "Epoch 3677  \tTraining Loss: 0.00014371583910161766\tValidation Loss: 0.00014579638623787384\n",
      "Epoch 3678  \tTraining Loss: 0.00014370762515770338\tValidation Loss: 0.0001457883879733794\n",
      "Epoch 3679  \tTraining Loss: 0.0001436994166118916\tValidation Loss: 0.00014578039063404608\n",
      "Epoch 3680  \tTraining Loss: 0.00014369121397798186\tValidation Loss: 0.00014577243787192547\n",
      "Epoch 3681  \tTraining Loss: 0.0001436830175920444\tValidation Loss: 0.00014576449259962499\n",
      "Epoch 3682  \tTraining Loss: 0.00014367482690164454\tValidation Loss: 0.00014575651680747876\n",
      "Epoch 3683  \tTraining Loss: 0.00014366664174955264\tValidation Loss: 0.00014574854183274112\n",
      "Epoch 3684  \tTraining Loss: 0.00014365846266153374\tValidation Loss: 0.00014574064594495946\n",
      "Epoch 3685  \tTraining Loss: 0.0001436502896956117\tValidation Loss: 0.00014573265229686415\n",
      "Epoch 3686  \tTraining Loss: 0.00014364212203571753\tValidation Loss: 0.00014572472730189188\n",
      "Epoch 3687  \tTraining Loss: 0.00014363396024200584\tValidation Loss: 0.00014571677922262983\n",
      "Epoch 3688  \tTraining Loss: 0.00014362580469451828\tValidation Loss: 0.00014570890641847642\n",
      "Epoch 3689  \tTraining Loss: 0.0001436176546754863\tValidation Loss: 0.00014570093511338193\n",
      "Epoch 3690  \tTraining Loss: 0.00014360951027035237\tValidation Loss: 0.0001456930323030477\n",
      "Epoch 3691  \tTraining Loss: 0.0001436013717113672\tValidation Loss: 0.00014568514113564955\n",
      "Epoch 3692  \tTraining Loss: 0.00014359323942157793\tValidation Loss: 0.0001456772194589332\n",
      "Epoch 3693  \tTraining Loss: 0.000143585112111144\tValidation Loss: 0.00014566929891812587\n",
      "Epoch 3694  \tTraining Loss: 0.00014357699096523532\tValidation Loss: 0.00014566142284175288\n",
      "Epoch 3695  \tTraining Loss: 0.0001435688756085101\tValidation Loss: 0.00014565357802698886\n",
      "Epoch 3696  \tTraining Loss: 0.00014356076612215645\tValidation Loss: 0.00014564564268212637\n",
      "Epoch 3697  \tTraining Loss: 0.00014355266184664652\tValidation Loss: 0.0001456377774247579\n",
      "Epoch 3698  \tTraining Loss: 0.00014354456365509797\tValidation Loss: 0.00014562992426394097\n",
      "Epoch 3699  \tTraining Loss: 0.0001435364714069756\tValidation Loss: 0.00014562204029927654\n",
      "Epoch 3700  \tTraining Loss: 0.00014352838432260775\tValidation Loss: 0.00014561415886011684\n",
      "Epoch 3701  \tTraining Loss: 0.00014352030313320892\tValidation Loss: 0.00014560632646088572\n",
      "Epoch 3702  \tTraining Loss: 0.00014351222786588308\tValidation Loss: 0.0001455985021422061\n",
      "Epoch 3703  \tTraining Loss: 0.00014350415828654815\tValidation Loss: 0.00014559064622588416\n",
      "Epoch 3704  \tTraining Loss: 0.0001434960938417988\tValidation Loss: 0.00014558279130700332\n",
      "Epoch 3705  \tTraining Loss: 0.00014348803562455056\tValidation Loss: 0.00014557501583362963\n",
      "Epoch 3706  \tTraining Loss: 0.0001434799830154782\tValidation Loss: 0.00014556716515881068\n",
      "Epoch 3707  \tTraining Loss: 0.00014347193577779856\tValidation Loss: 0.00014555932393851858\n",
      "Epoch 3708  \tTraining Loss: 0.00014346389418185397\tValidation Loss: 0.00014555152864978164\n",
      "Epoch 3709  \tTraining Loss: 0.00014345585858519192\tValidation Loss: 0.00014554374185990279\n",
      "Epoch 3710  \tTraining Loss: 0.00014344782852314516\tValidation Loss: 0.00014553592316867233\n",
      "Epoch 3711  \tTraining Loss: 0.0001434398036224578\tValidation Loss: 0.0001455281056109405\n",
      "Epoch 3712  \tTraining Loss: 0.0001434317848202488\tValidation Loss: 0.00014552036764260747\n",
      "Epoch 3713  \tTraining Loss: 0.00014342377162227337\tValidation Loss: 0.0001455125541460689\n",
      "Epoch 3714  \tTraining Loss: 0.00014341576374138823\tValidation Loss: 0.00014550475019845056\n",
      "Epoch 3715  \tTraining Loss: 0.00014340776142518507\tValidation Loss: 0.00014549699213344292\n",
      "Epoch 3716  \tTraining Loss: 0.0001433997650525221\tValidation Loss: 0.00014548924274779745\n",
      "Epoch 3717  \tTraining Loss: 0.00014339177418545547\tValidation Loss: 0.00014548146107475296\n",
      "Epoch 3718  \tTraining Loss: 0.00014338378839722678\tValidation Loss: 0.00014547368062868538\n",
      "Epoch 3719  \tTraining Loss: 0.0001433758086697857\tValidation Loss: 0.00014546597989614196\n",
      "Epoch 3720  \tTraining Loss: 0.00014336783450975783\tValidation Loss: 0.00014545820329479913\n",
      "Epoch 3721  \tTraining Loss: 0.00014335986555803052\tValidation Loss: 0.00014545043632842574\n",
      "Epoch 3722  \tTraining Loss: 0.00014335190218951516\tValidation Loss: 0.00014544271519297816\n",
      "Epoch 3723  \tTraining Loss: 0.0001433439446077633\tValidation Loss: 0.00014543500291201716\n",
      "Epoch 3724  \tTraining Loss: 0.00014333599261164655\tValidation Loss: 0.0001454272579563455\n",
      "Epoch 3725  \tTraining Loss: 0.00014332804555500356\tValidation Loss: 0.00014541953759823805\n",
      "Epoch 3726  \tTraining Loss: 0.00014332010445049941\tValidation Loss: 0.00014541183813500738\n",
      "Epoch 3727  \tTraining Loss: 0.00014331216960524216\tValidation Loss: 0.0001454041392595744\n",
      "Epoch 3728  \tTraining Loss: 0.00014330424169808168\tValidation Loss: 0.0001453964163013164\n",
      "Epoch 3729  \tTraining Loss: 0.0001432963195866508\tValidation Loss: 0.00014538873567596544\n",
      "Epoch 3730  \tTraining Loss: 0.00014328840305295883\tValidation Loss: 0.00014538108614303247\n",
      "Epoch 3731  \tTraining Loss: 0.00014328049212028067\tValidation Loss: 0.00014537334408880489\n",
      "Epoch 3732  \tTraining Loss: 0.00014327258616222057\tValidation Loss: 0.00014536567191181175\n",
      "Epoch 3733  \tTraining Loss: 0.00014326468590747016\tValidation Loss: 0.0001453580127756583\n",
      "Epoch 3734  \tTraining Loss: 0.00014325679150032207\tValidation Loss: 0.0001453503209356837\n",
      "Epoch 3735  \tTraining Loss: 0.00014324890178345995\tValidation Loss: 0.00014534265397115568\n",
      "Epoch 3736  \tTraining Loss: 0.00014324101810502587\tValidation Loss: 0.0001453349745043314\n",
      "Epoch 3737  \tTraining Loss: 0.00014323314033425397\tValidation Loss: 0.00014532737250170965\n",
      "Epoch 3738  \tTraining Loss: 0.0001432252677027195\tValidation Loss: 0.000145319670528198\n",
      "Epoch 3739  \tTraining Loss: 0.00014321740034807116\tValidation Loss: 0.0001453120372745483\n",
      "Epoch 3740  \tTraining Loss: 0.0001432095385216261\tValidation Loss: 0.00014530444041182665\n",
      "Epoch 3741  \tTraining Loss: 0.0001432016824226409\tValidation Loss: 0.00014529675164922916\n",
      "Epoch 3742  \tTraining Loss: 0.00014319383112299742\tValidation Loss: 0.00014528913319260782\n",
      "Epoch 3743  \tTraining Loss: 0.00014318598535088765\tValidation Loss: 0.000145281528233267\n",
      "Epoch 3744  \tTraining Loss: 0.00014317814554135337\tValidation Loss: 0.0001452738901135265\n",
      "Epoch 3745  \tTraining Loss: 0.00014317031024800074\tValidation Loss: 0.0001452662770178244\n",
      "Epoch 3746  \tTraining Loss: 0.00014316248063382333\tValidation Loss: 0.00014525864925339104\n",
      "Epoch 3747  \tTraining Loss: 0.00014315465664739005\tValidation Loss: 0.00014525109925091418\n",
      "Epoch 3748  \tTraining Loss: 0.0001431468377600615\tValidation Loss: 0.0001452434484065524\n",
      "Epoch 3749  \tTraining Loss: 0.00014313902248232958\tValidation Loss: 0.00014523585798813544\n",
      "Epoch 3750  \tTraining Loss: 0.0001431312073015029\tValidation Loss: 0.00014522830703432425\n",
      "Epoch 3751  \tTraining Loss: 0.00014312339810508865\tValidation Loss: 0.00014522067247272835\n",
      "Epoch 3752  \tTraining Loss: 0.00014311559372891743\tValidation Loss: 0.00014521311066894985\n",
      "Epoch 3753  \tTraining Loss: 0.00014310779461379957\tValidation Loss: 0.0001452055500787838\n",
      "Epoch 3754  \tTraining Loss: 0.00014310000136402425\tValidation Loss: 0.000145198007005903\n",
      "Epoch 3755  \tTraining Loss: 0.0001430922131143051\tValidation Loss: 0.00014519043070899932\n",
      "Epoch 3756  \tTraining Loss: 0.0001430844297825411\tValidation Loss: 0.00014518285589406178\n",
      "Epoch 3757  \tTraining Loss: 0.0001430766521057423\tValidation Loss: 0.00014517536113634093\n",
      "Epoch 3758  \tTraining Loss: 0.00014306887981941954\tValidation Loss: 0.00014516778836267332\n",
      "Epoch 3759  \tTraining Loss: 0.00014306111237399205\tValidation Loss: 0.00014516022534259186\n",
      "Epoch 3760  \tTraining Loss: 0.0001430533502785383\tValidation Loss: 0.00014515270760877613\n",
      "Epoch 3761  \tTraining Loss: 0.0001430455935767376\tValidation Loss: 0.00014514522259451997\n",
      "Epoch 3762  \tTraining Loss: 0.00014303784228015008\tValidation Loss: 0.00014513764394913709\n",
      "Epoch 3763  \tTraining Loss: 0.00014303009569572564\tValidation Loss: 0.00014513013524612643\n",
      "Epoch 3764  \tTraining Loss: 0.0001430223544570387\tValidation Loss: 0.00014512264031591796\n",
      "Epoch 3765  \tTraining Loss: 0.00014301461905636567\tValidation Loss: 0.00014511511090081736\n",
      "Epoch 3766  \tTraining Loss: 0.00014300688800936463\tValidation Loss: 0.00014510760788700932\n",
      "Epoch 3767  \tTraining Loss: 0.00014299916240360645\tValidation Loss: 0.00014510009274865734\n",
      "Epoch 3768  \tTraining Loss: 0.00014299144232928887\tValidation Loss: 0.000145092655477335\n",
      "Epoch 3769  \tTraining Loss: 0.0001429837272372986\tValidation Loss: 0.0001450851395224644\n",
      "Epoch 3770  \tTraining Loss: 0.00014297601708796747\tValidation Loss: 0.00014507763348870772\n",
      "Epoch 3771  \tTraining Loss: 0.0001429683122591378\tValidation Loss: 0.00014507020946365483\n",
      "Epoch 3772  \tTraining Loss: 0.00014296061289827993\tValidation Loss: 0.0001450627072463135\n",
      "Epoch 3773  \tTraining Loss: 0.00014295291824906856\tValidation Loss: 0.00014505521519613695\n",
      "Epoch 3774  \tTraining Loss: 0.00014294522879430228\tValidation Loss: 0.00014504776842511676\n",
      "Epoch 3775  \tTraining Loss: 0.00014293754462265006\tValidation Loss: 0.00014504035480862536\n",
      "Epoch 3776  \tTraining Loss: 0.00014292986583129506\tValidation Loss: 0.00014503284704628323\n",
      "Epoch 3777  \tTraining Loss: 0.00014292219162128538\tValidation Loss: 0.00014502540935708755\n",
      "Epoch 3778  \tTraining Loss: 0.0001429145225808468\tValidation Loss: 0.00014501800895907435\n",
      "Epoch 3779  \tTraining Loss: 0.0001429068593844876\tValidation Loss: 0.00014501051489825993\n",
      "Epoch 3780  \tTraining Loss: 0.00014289920048617463\tValidation Loss: 0.00014500309112733473\n",
      "Epoch 3781  \tTraining Loss: 0.00014289154673067804\tValidation Loss: 0.00014499566775060078\n",
      "Epoch 3782  \tTraining Loss: 0.00014288389852035763\tValidation Loss: 0.00014498826254225958\n",
      "Epoch 3783  \tTraining Loss: 0.00014287625532548672\tValidation Loss: 0.00014498082288994608\n",
      "Epoch 3784  \tTraining Loss: 0.0001428686166985611\tValidation Loss: 0.00014497340850469395\n",
      "Epoch 3785  \tTraining Loss: 0.00014286098347341273\tValidation Loss: 0.00014496597926282287\n",
      "Epoch 3786  \tTraining Loss: 0.00014285335558253197\tValidation Loss: 0.0001449586281862141\n",
      "Epoch 3787  \tTraining Loss: 0.0001428457323904307\tValidation Loss: 0.00014495119757527698\n",
      "Epoch 3788  \tTraining Loss: 0.0001428381141901484\tValidation Loss: 0.0001449437771035245\n",
      "Epoch 3789  \tTraining Loss: 0.0001428305012028613\tValidation Loss: 0.0001449364389949721\n",
      "Epoch 3790  \tTraining Loss: 0.00014282289337321524\tValidation Loss: 0.00014492902186774643\n",
      "Epoch 3791  \tTraining Loss: 0.0001428152902461178\tValidation Loss: 0.00014492161514379636\n",
      "Epoch 3792  \tTraining Loss: 0.00014280769218015795\tValidation Loss: 0.00014491425362830968\n",
      "Epoch 3793  \tTraining Loss: 0.00014280009939648746\tValidation Loss: 0.00014490692566870882\n",
      "Epoch 3794  \tTraining Loss: 0.0001427925115919401\tValidation Loss: 0.00014489950277285106\n",
      "Epoch 3795  \tTraining Loss: 0.00014278492844148043\tValidation Loss: 0.00014489215007039741\n",
      "Epoch 3796  \tTraining Loss: 0.00014277735040351612\tValidation Loss: 0.0001448848351004915\n",
      "Epoch 3797  \tTraining Loss: 0.00014276977778224348\tValidation Loss: 0.00014487742568426108\n",
      "Epoch 3798  \tTraining Loss: 0.00014276220961652455\tValidation Loss: 0.00014487008668315844\n",
      "Epoch 3799  \tTraining Loss: 0.00014275464638873514\tValidation Loss: 0.00014486274807645554\n",
      "Epoch 3800  \tTraining Loss: 0.0001427470886917061\tValidation Loss: 0.000144855443093848\n",
      "Epoch 3801  \tTraining Loss: 0.00014273953630408268\tValidation Loss: 0.00014484809160583435\n",
      "Epoch 3802  \tTraining Loss: 0.00014273198834662942\tValidation Loss: 0.00014484076262649633\n",
      "Epoch 3803  \tTraining Loss: 0.00014272444554732698\tValidation Loss: 0.00014483347822580848\n",
      "Epoch 3804  \tTraining Loss: 0.00014271690825811318\tValidation Loss: 0.0001448261000332761\n",
      "Epoch 3805  \tTraining Loss: 0.00014270937519925259\tValidation Loss: 0.00014481879225201712\n",
      "Epoch 3806  \tTraining Loss: 0.00014270184709547746\tValidation Loss: 0.00014481148478269772\n",
      "Epoch 3807  \tTraining Loss: 0.00014269432426212967\tValidation Loss: 0.00014480419603352189\n",
      "Epoch 3808  \tTraining Loss: 0.00014268680645255135\tValidation Loss: 0.00014479687155165839\n",
      "Epoch 3809  \tTraining Loss: 0.00014267929296464462\tValidation Loss: 0.0001447895725558606\n",
      "Epoch 3810  \tTraining Loss: 0.00014267178452981817\tValidation Loss: 0.00014478228174980657\n",
      "Epoch 3811  \tTraining Loss: 0.00014266428154975226\tValidation Loss: 0.00014477501124492384\n",
      "Epoch 3812  \tTraining Loss: 0.00014265678306709764\tValidation Loss: 0.00014476770502943966\n",
      "Epoch 3813  \tTraining Loss: 0.00014264928913186063\tValidation Loss: 0.00014476042439672064\n",
      "Epoch 3814  \tTraining Loss: 0.00014264180043371133\tValidation Loss: 0.00014475316685037977\n",
      "Epoch 3815  \tTraining Loss: 0.0001426343169086889\tValidation Loss: 0.00014474587389751815\n",
      "Epoch 3816  \tTraining Loss: 0.00014262683756208273\tValidation Loss: 0.00014473860672217833\n",
      "Epoch 3817  \tTraining Loss: 0.00014261936322415057\tValidation Loss: 0.00014473134785545865\n",
      "Epoch 3818  \tTraining Loss: 0.0001426118941867716\tValidation Loss: 0.00014472410954880052\n",
      "Epoch 3819  \tTraining Loss: 0.00014260442981133098\tValidation Loss: 0.00014471683523280761\n",
      "Epoch 3820  \tTraining Loss: 0.00014259696983427424\tValidation Loss: 0.00014470958660988594\n",
      "Epoch 3821  \tTraining Loss: 0.00014258951493419008\tValidation Loss: 0.0001447023842815681\n",
      "Epoch 3822  \tTraining Loss: 0.000142582065338719\tValidation Loss: 0.00014469508794740006\n",
      "Epoch 3823  \tTraining Loss: 0.0001425746198972523\tValidation Loss: 0.00014468786244368687\n",
      "Epoch 3824  \tTraining Loss: 0.0001425671792973474\tValidation Loss: 0.00014468063739833266\n",
      "Epoch 3825  \tTraining Loss: 0.00014255974382563723\tValidation Loss: 0.0001446734315894889\n",
      "Epoch 3826  \tTraining Loss: 0.0001425523132883775\tValidation Loss: 0.00014466618919590295\n",
      "Epoch 3827  \tTraining Loss: 0.0001425448869473329\tValidation Loss: 0.00014465897249494068\n",
      "Epoch 3828  \tTraining Loss: 0.00014253746556912306\tValidation Loss: 0.0001446518022253791\n",
      "Epoch 3829  \tTraining Loss: 0.0001425300494753048\tValidation Loss: 0.0001446445376147355\n",
      "Epoch 3830  \tTraining Loss: 0.00014252263764103713\tValidation Loss: 0.00014463734387831946\n",
      "Epoch 3831  \tTraining Loss: 0.00014251523049623304\tValidation Loss: 0.00014463015058788692\n",
      "Epoch 3832  \tTraining Loss: 0.0001425078283604564\tValidation Loss: 0.00014462299966563145\n",
      "Epoch 3833  \tTraining Loss: 0.0001425004310993663\tValidation Loss: 0.00014461577655092205\n",
      "Epoch 3834  \tTraining Loss: 0.00014249303822389415\tValidation Loss: 0.00014460856594372727\n",
      "Epoch 3835  \tTraining Loss: 0.00014248565014543218\tValidation Loss: 0.00014460140059916065\n",
      "Epoch 3836  \tTraining Loss: 0.00014247826708986018\tValidation Loss: 0.00014459426976676747\n",
      "Epoch 3837  \tTraining Loss: 0.00014247088858595117\tValidation Loss: 0.00014458706500943626\n",
      "Epoch 3838  \tTraining Loss: 0.00014246351457047543\tValidation Loss: 0.00014457987254506868\n",
      "Epoch 3839  \tTraining Loss: 0.00014245614551980915\tValidation Loss: 0.0001445727637380647\n",
      "Epoch 3840  \tTraining Loss: 0.00014244878136610573\tValidation Loss: 0.00014456557365086716\n",
      "Epoch 3841  \tTraining Loss: 0.0001424414213826641\tValidation Loss: 0.00014455841754746287\n",
      "Epoch 3842  \tTraining Loss: 0.00014243406623462966\tValidation Loss: 0.00014455124833506763\n",
      "Epoch 3843  \tTraining Loss: 0.00014242671618424257\tValidation Loss: 0.0001445441586707678\n",
      "Epoch 3844  \tTraining Loss: 0.00014241937059877616\tValidation Loss: 0.0001445369868946792\n",
      "Epoch 3845  \tTraining Loss: 0.00014241202933927018\tValidation Loss: 0.00014452984893619888\n",
      "Epoch 3846  \tTraining Loss: 0.00014240469290054375\tValidation Loss: 0.00014452275937360147\n",
      "Epoch 3847  \tTraining Loss: 0.00014239736167258066\tValidation Loss: 0.00014451557490718204\n",
      "Epoch 3848  \tTraining Loss: 0.00014239003447515992\tValidation Loss: 0.00014450846152263088\n",
      "Epoch 3849  \tTraining Loss: 0.0001423827119227999\tValidation Loss: 0.00014450134859968547\n",
      "Epoch 3850  \tTraining Loss: 0.0001423753942329947\tValidation Loss: 0.00014449428296512597\n",
      "Epoch 3851  \tTraining Loss: 0.00014236808059253536\tValidation Loss: 0.0001444871369981007\n",
      "Epoch 3852  \tTraining Loss: 0.0001423607711803571\tValidation Loss: 0.00014448000447822045\n",
      "Epoch 3853  \tTraining Loss: 0.00014235346653044848\tValidation Loss: 0.0001444729173046161\n",
      "Epoch 3854  \tTraining Loss: 0.00014234616674704942\tValidation Loss: 0.00014446586509771832\n",
      "Epoch 3855  \tTraining Loss: 0.00014233887143956755\tValidation Loss: 0.00014445873816711178\n",
      "Epoch 3856  \tTraining Loss: 0.00014233158042212517\tValidation Loss: 0.00014445164670216405\n",
      "Epoch 3857  \tTraining Loss: 0.0001423242942477162\tValidation Loss: 0.00014444458120606686\n",
      "Epoch 3858  \tTraining Loss: 0.00014231701309918985\tValidation Loss: 0.000144437478484425\n",
      "Epoch 3859  \tTraining Loss: 0.00014230973581903252\tValidation Loss: 0.00014443040207551207\n",
      "Epoch 3860  \tTraining Loss: 0.00014230246327456703\tValidation Loss: 0.00014442333396632\n",
      "Epoch 3861  \tTraining Loss: 0.00014229519554858453\tValidation Loss: 0.00014441631034081156\n",
      "Epoch 3862  \tTraining Loss: 0.00014228793246018586\tValidation Loss: 0.0001444092133622932\n",
      "Epoch 3863  \tTraining Loss: 0.00014228067356026323\tValidation Loss: 0.0001444021292951617\n",
      "Epoch 3864  \tTraining Loss: 0.0001422734193325397\tValidation Loss: 0.00014439509040649155\n",
      "Epoch 3865  \tTraining Loss: 0.0001422661699338718\tValidation Loss: 0.00014438808663035628\n",
      "Epoch 3866  \tTraining Loss: 0.00014225892491228533\tValidation Loss: 0.0001443810075328795\n",
      "Epoch 3867  \tTraining Loss: 0.00014225168412815994\tValidation Loss: 0.00014437396394230447\n",
      "Epoch 3868  \tTraining Loss: 0.00014224444808816017\tValidation Loss: 0.0001443669694443167\n",
      "Epoch 3869  \tTraining Loss: 0.00014223721699870427\tValidation Loss: 0.00014435987910451877\n",
      "Epoch 3870  \tTraining Loss: 0.00014222998984595743\tValidation Loss: 0.00014435286002772993\n",
      "Epoch 3871  \tTraining Loss: 0.0001422227672300295\tValidation Loss: 0.00014434584137730497\n",
      "Epoch 3872  \tTraining Loss: 0.000142215549397037\tValidation Loss: 0.0001443388659273194\n",
      "Epoch 3873  \tTraining Loss: 0.00014220833613994143\tValidation Loss: 0.00014433181637552546\n",
      "Epoch 3874  \tTraining Loss: 0.0001422011269385315\tValidation Loss: 0.00014432480266110204\n",
      "Epoch 3875  \tTraining Loss: 0.0001421939223519976\tValidation Loss: 0.000144317776044433\n",
      "Epoch 3876  \tTraining Loss: 0.00014218672272608803\tValidation Loss: 0.0001443108296022439\n",
      "Epoch 3877  \tTraining Loss: 0.00014217952723562946\tValidation Loss: 0.00014430379945836746\n",
      "Epoch 3878  \tTraining Loss: 0.000142172335946819\tValidation Loss: 0.00014429680341165546\n",
      "Epoch 3879  \tTraining Loss: 0.00014216514932401464\tValidation Loss: 0.00014428985648825413\n",
      "Epoch 3880  \tTraining Loss: 0.0001421579675593486\tValidation Loss: 0.00014428283601566563\n",
      "Epoch 3881  \tTraining Loss: 0.00014215078964431132\tValidation Loss: 0.00014427585172619483\n",
      "Epoch 3882  \tTraining Loss: 0.0001421436163023625\tValidation Loss: 0.00014426887754332755\n",
      "Epoch 3883  \tTraining Loss: 0.00014213644766531357\tValidation Loss: 0.000144261925783932\n",
      "Epoch 3884  \tTraining Loss: 0.00014212928358368955\tValidation Loss: 0.00014425493488591387\n",
      "Epoch 3885  \tTraining Loss: 0.00014212212338736508\tValidation Loss: 0.0001442479703382212\n",
      "Epoch 3886  \tTraining Loss: 0.0001421149677681733\tValidation Loss: 0.00014424101394136887\n",
      "Epoch 3887  \tTraining Loss: 0.00014210781691774118\tValidation Loss: 0.00014423410248092875\n",
      "Epoch 3888  \tTraining Loss: 0.00014210067033354037\tValidation Loss: 0.0001442271163877505\n",
      "Epoch 3889  \tTraining Loss: 0.00014209352782561187\tValidation Loss: 0.0001442201663026118\n",
      "Epoch 3890  \tTraining Loss: 0.0001420863898917395\tValidation Loss: 0.0001442132659241993\n",
      "Epoch 3891  \tTraining Loss: 0.0001420792568055186\tValidation Loss: 0.00014420626876145476\n",
      "Epoch 3892  \tTraining Loss: 0.00014207212756286934\tValidation Loss: 0.00014419934305172938\n",
      "Epoch 3893  \tTraining Loss: 0.0001420650027017572\tValidation Loss: 0.00014419241774616686\n",
      "Epoch 3894  \tTraining Loss: 0.0001420578824421813\tValidation Loss: 0.00014418553610059452\n",
      "Epoch 3895  \tTraining Loss: 0.00014205076673055677\tValidation Loss: 0.000144178579307298\n",
      "Epoch 3896  \tTraining Loss: 0.0001420436548932969\tValidation Loss: 0.00014417165855623628\n",
      "Epoch 3897  \tTraining Loss: 0.00014203654754015854\tValidation Loss: 0.00014416474774616716\n",
      "Epoch 3898  \tTraining Loss: 0.00014202944485453337\tValidation Loss: 0.0001441578824255246\n",
      "Epoch 3899  \tTraining Loss: 0.00014202234644258063\tValidation Loss: 0.00014415094206364889\n",
      "Epoch 3900  \tTraining Loss: 0.00014201525200724632\tValidation Loss: 0.00014414403786234403\n",
      "Epoch 3901  \tTraining Loss: 0.00014200816204075356\tValidation Loss: 0.0001441371836464795\n",
      "Epoch 3902  \tTraining Loss: 0.00014200107696054799\tValidation Loss: 0.0001441302321610946\n",
      "Epoch 3903  \tTraining Loss: 0.00014199399556900216\tValidation Loss: 0.0001441233522191361\n",
      "Epoch 3904  \tTraining Loss: 0.0001419869185274008\tValidation Loss: 0.00014411647266694713\n",
      "Epoch 3905  \tTraining Loss: 0.00014197984597695936\tValidation Loss: 0.00014410963700355774\n",
      "Epoch 3906  \tTraining Loss: 0.00014197277800562545\tValidation Loss: 0.0001441027256692213\n",
      "Epoch 3907  \tTraining Loss: 0.0001419657137981918\tValidation Loss: 0.00014409585047931662\n",
      "Epoch 3908  \tTraining Loss: 0.00014195865401242966\tValidation Loss: 0.00014408898521730648\n",
      "Epoch 3909  \tTraining Loss: 0.00014195159877996262\tValidation Loss: 0.00014408216567344693\n",
      "Epoch 3910  \tTraining Loss: 0.0001419445478620291\tValidation Loss: 0.00014407527056364028\n",
      "Epoch 3911  \tTraining Loss: 0.00014193750080606697\tValidation Loss: 0.0001440684117145784\n",
      "Epoch 3912  \tTraining Loss: 0.00014193045814901676\tValidation Loss: 0.00014406156281434896\n",
      "Epoch 3913  \tTraining Loss: 0.00014192342013548862\tValidation Loss: 0.00014405475971397843\n",
      "Epoch 3914  \tTraining Loss: 0.00014191638618748153\tValidation Loss: 0.00014404788087002496\n",
      "Epoch 3915  \tTraining Loss: 0.00014190935619249793\tValidation Loss: 0.0001440410383205452\n",
      "Epoch 3916  \tTraining Loss: 0.0001419023306355153\tValidation Loss: 0.00014403424609334373\n",
      "Epoch 3917  \tTraining Loss: 0.0001418953097584848\tValidation Loss: 0.00014402735592815112\n",
      "Epoch 3918  \tTraining Loss: 0.00014188829255162806\tValidation Loss: 0.00014402053742230386\n",
      "Epoch 3919  \tTraining Loss: 0.0001418812796132223\tValidation Loss: 0.0001440137192789862\n",
      "Epoch 3920  \tTraining Loss: 0.00014187427198561142\tValidation Loss: 0.00014400691829358418\n",
      "Epoch 3921  \tTraining Loss: 0.00014186726998743007\tValidation Loss: 0.00014400004845320207\n",
      "Epoch 3922  \tTraining Loss: 0.00014186027185280925\tValidation Loss: 0.00014399321906584047\n",
      "Epoch 3923  \tTraining Loss: 0.00014185327807648035\tValidation Loss: 0.0001439864013524938\n",
      "Epoch 3924  \tTraining Loss: 0.0001418462887622473\tValidation Loss: 0.0001439796306240942\n",
      "Epoch 3925  \tTraining Loss: 0.00014183930368769643\tValidation Loss: 0.00014397278431517896\n",
      "Epoch 3926  \tTraining Loss: 0.00014183232240193717\tValidation Loss: 0.0001439659745617759\n",
      "Epoch 3927  \tTraining Loss: 0.00014182534543486752\tValidation Loss: 0.00014395917484079962\n",
      "Epoch 3928  \tTraining Loss: 0.00014181837289933645\tValidation Loss: 0.00014395242143263868\n",
      "Epoch 3929  \tTraining Loss: 0.0001418114045780918\tValidation Loss: 0.00014394559178135954\n",
      "Epoch 3930  \tTraining Loss: 0.0001418044400261254\tValidation Loss: 0.00014393879867539647\n",
      "Epoch 3931  \tTraining Loss: 0.0001417974797692702\tValidation Loss: 0.00014393201558721085\n",
      "Epoch 3932  \tTraining Loss: 0.00014179052390092035\tValidation Loss: 0.0001439252787274994\n",
      "Epoch 3933  \tTraining Loss: 0.00014178357226296838\tValidation Loss: 0.00014391846533502657\n",
      "Epoch 3934  \tTraining Loss: 0.00014177662435403332\tValidation Loss: 0.00014391168844859095\n",
      "Epoch 3935  \tTraining Loss: 0.00014176968071807005\tValidation Loss: 0.00014390492152551297\n",
      "Epoch 3936  \tTraining Loss: 0.00014176274141978053\tValidation Loss: 0.00014389820087632726\n",
      "Epoch 3937  \tTraining Loss: 0.00014175580638654112\tValidation Loss: 0.00014389140349019337\n",
      "Epoch 3938  \tTraining Loss: 0.00014174887503273488\tValidation Loss: 0.00014388464262981204\n",
      "Epoch 3939  \tTraining Loss: 0.00014174194793019958\tValidation Loss: 0.00014387789171749365\n",
      "Epoch 3940  \tTraining Loss: 0.00014173502510926396\tValidation Loss: 0.0001438711871509034\n",
      "Epoch 3941  \tTraining Loss: 0.00014172810659982\tValidation Loss: 0.00014386440625337095\n",
      "Epoch 3942  \tTraining Loss: 0.00014172119171443077\tValidation Loss: 0.00014385766598147017\n",
      "Epoch 3943  \tTraining Loss: 0.0001417142810588682\tValidation Loss: 0.0001438509356502007\n",
      "Epoch 3944  \tTraining Loss: 0.0001417073746248916\tValidation Loss: 0.00014384425174279958\n",
      "Epoch 3945  \tTraining Loss: 0.00014170047255761479\tValidation Loss: 0.00014383749075380244\n",
      "Epoch 3946  \tTraining Loss: 0.00014169357405478762\tValidation Loss: 0.0001438307663244286\n",
      "Epoch 3947  \tTraining Loss: 0.00014168667976054773\tValidation Loss: 0.00014382405182850308\n",
      "Epoch 3948  \tTraining Loss: 0.00014167978962474176\tValidation Loss: 0.00014381738383355188\n",
      "Epoch 3949  \tTraining Loss: 0.0001416729039178448\tValidation Loss: 0.0001438106385716719\n",
      "Epoch 3950  \tTraining Loss: 0.0001416660217124724\tValidation Loss: 0.00014380392990198974\n",
      "Epoch 3951  \tTraining Loss: 0.00014165914369463237\tValidation Loss: 0.00014379723115878404\n",
      "Epoch 3952  \tTraining Loss: 0.0001416522697694823\tValidation Loss: 0.0001437905789940097\n",
      "Epoch 3953  \tTraining Loss: 0.00014164540034127245\tValidation Loss: 0.00014378384937676502\n",
      "Epoch 3954  \tTraining Loss: 0.0001416385343489448\tValidation Loss: 0.00014377715638444447\n",
      "Epoch 3955  \tTraining Loss: 0.00014163167252326763\tValidation Loss: 0.00014377047331176555\n",
      "Epoch 3956  \tTraining Loss: 0.00014162481473822244\tValidation Loss: 0.00014376379562078568\n",
      "Epoch 3957  \tTraining Loss: 0.00014161796134070888\tValidation Loss: 0.00014375716379212773\n",
      "Epoch 3958  \tTraining Loss: 0.00014161111176034976\tValidation Loss: 0.00014375045404472957\n",
      "Epoch 3959  \tTraining Loss: 0.00014160426591846719\tValidation Loss: 0.0001437437808032394\n",
      "Epoch 3960  \tTraining Loss: 0.00014159742421653856\tValidation Loss: 0.00014373711736462795\n",
      "Epoch 3961  \tTraining Loss: 0.00014159058682489123\tValidation Loss: 0.00014373050058386292\n",
      "Epoch 3962  \tTraining Loss: 0.00014158375335236153\tValidation Loss: 0.00014372380591754944\n",
      "Epoch 3963  \tTraining Loss: 0.0001415769235349769\tValidation Loss: 0.00014371714791275073\n",
      "Epoch 3964  \tTraining Loss: 0.00014157009783771407\tValidation Loss: 0.00014371049978791843\n",
      "Epoch 3965  \tTraining Loss: 0.0001415632763737686\tValidation Loss: 0.00014370389845800698\n",
      "Epoch 3966  \tTraining Loss: 0.00014155645892187724\tValidation Loss: 0.00014369721909596596\n",
      "Epoch 3967  \tTraining Loss: 0.0001415496450475294\tValidation Loss: 0.00014369057645588636\n",
      "Epoch 3968  \tTraining Loss: 0.00014154283527301333\tValidation Loss: 0.00014368394370776582\n",
      "Epoch 3969  \tTraining Loss: 0.00014153602965426946\tValidation Loss: 0.0001436773578457511\n",
      "Epoch 3970  \tTraining Loss: 0.00014152922814169994\tValidation Loss: 0.00014367069377377797\n",
      "Epoch 3971  \tTraining Loss: 0.00014152243012879534\tValidation Loss: 0.0001436640664629186\n",
      "Epoch 3972  \tTraining Loss: 0.00014151563619553404\tValidation Loss: 0.00014365744904177877\n",
      "Epoch 3973  \tTraining Loss: 0.00014150884633960117\tValidation Loss: 0.00014365087858841835\n",
      "Epoch 3974  \tTraining Loss: 0.00014150206068614247\tValidation Loss: 0.00014364422974099365\n",
      "Epoch 3975  \tTraining Loss: 0.00014149527845361864\tValidation Loss: 0.00014363761768964795\n",
      "Epoch 3976  \tTraining Loss: 0.00014148850028068892\tValidation Loss: 0.0001436310155230309\n",
      "Epoch 3977  \tTraining Loss: 0.0001414817261057066\tValidation Loss: 0.00014362446040406559\n",
      "Epoch 3978  \tTraining Loss: 0.00014147495623181137\tValidation Loss: 0.00014361782670578374\n",
      "Epoch 3979  \tTraining Loss: 0.00014146818969918543\tValidation Loss: 0.00014361122983784228\n",
      "Epoch 3980  \tTraining Loss: 0.00014146142720624933\tValidation Loss: 0.00014360464284924446\n",
      "Epoch 3981  \tTraining Loss: 0.00014145466863096822\tValidation Loss: 0.00014359810298797033\n",
      "Epoch 3982  \tTraining Loss: 0.00014144791445765736\tValidation Loss: 0.0001435914843620686\n",
      "Epoch 3983  \tTraining Loss: 0.00014144116354503007\tValidation Loss: 0.0001435849026008051\n",
      "Epoch 3984  \tTraining Loss: 0.00014143441665233353\tValidation Loss: 0.00014357833071360984\n",
      "Epoch 3985  \tTraining Loss: 0.00014142767365297706\tValidation Loss: 0.00014357176409494308\n",
      "Epoch 3986  \tTraining Loss: 0.0001414209348523504\tValidation Loss: 0.00014356524387463342\n",
      "Epoch 3987  \tTraining Loss: 0.0001414141998058849\tValidation Loss: 0.00014355864441741498\n",
      "Epoch 3988  \tTraining Loss: 0.00014140746830859259\tValidation Loss: 0.00014355208170454745\n",
      "Epoch 3989  \tTraining Loss: 0.00014140074080590514\tValidation Loss: 0.00014354552874934937\n",
      "Epoch 3990  \tTraining Loss: 0.00014139401741274332\tValidation Loss: 0.0001435390230044366\n",
      "Epoch 3991  \tTraining Loss: 0.00014138729790314967\tValidation Loss: 0.00014353243806308774\n",
      "Epoch 3992  \tTraining Loss: 0.00014138058184716458\tValidation Loss: 0.00014352589002622045\n",
      "Epoch 3993  \tTraining Loss: 0.00014137386986962577\tValidation Loss: 0.00014351935353767523\n",
      "Epoch 3994  \tTraining Loss: 0.0001413671621944848\tValidation Loss: 0.00014351286482082318\n",
      "Epoch 3995  \tTraining Loss: 0.00014136045853393797\tValidation Loss: 0.00014350629548493676\n",
      "Epoch 3996  \tTraining Loss: 0.00014135375823165532\tValidation Loss: 0.00014349976277491731\n",
      "Epoch 3997  \tTraining Loss: 0.00014134706188614768\tValidation Loss: 0.00014349323982019307\n",
      "Epoch 3998  \tTraining Loss: 0.00014134036949357105\tValidation Loss: 0.00014348676432138452\n",
      "Epoch 3999  \tTraining Loss: 0.00014133368118365633\tValidation Loss: 0.00014348020934291483\n",
      "Epoch 4000  \tTraining Loss: 0.00014132699614772965\tValidation Loss: 0.00014347369012208318\n",
      "Epoch 4001  \tTraining Loss: 0.0001413203148671447\tValidation Loss: 0.00014346718502821515\n",
      "Epoch 4002  \tTraining Loss: 0.0001413136374661441\tValidation Loss: 0.00014346072651990184\n",
      "Epoch 4003  \tTraining Loss: 0.00014130696424314562\tValidation Loss: 0.00014345418798290948\n",
      "Epoch 4004  \tTraining Loss: 0.0001413002942381883\tValidation Loss: 0.00014344768635734395\n",
      "Epoch 4005  \tTraining Loss: 0.0001412936281515582\tValidation Loss: 0.0001434411945012894\n",
      "Epoch 4006  \tTraining Loss: 0.0001412869658562995\tValidation Loss: 0.0001434347077910114\n",
      "Epoch 4007  \tTraining Loss: 0.00014128030771539272\tValidation Loss: 0.00014342826783557763\n",
      "Epoch 4008  \tTraining Loss: 0.00014127365311387245\tValidation Loss: 0.0001434217476787329\n",
      "Epoch 4009  \tTraining Loss: 0.00014126700201530515\tValidation Loss: 0.00014341526443591518\n",
      "Epoch 4010  \tTraining Loss: 0.00014126035481072083\tValidation Loss: 0.0001434087909095448\n",
      "Epoch 4011  \tTraining Loss: 0.00014125371166840296\tValidation Loss: 0.0001434023649829928\n",
      "Epoch 4012  \tTraining Loss: 0.00014124707220292102\tValidation Loss: 0.00014339585891661824\n",
      "Epoch 4013  \tTraining Loss: 0.00014124043614179813\tValidation Loss: 0.000143389389937408\n",
      "Epoch 4014  \tTraining Loss: 0.00014123380395662037\tValidation Loss: 0.00014338293076362324\n",
      "Epoch 4015  \tTraining Loss: 0.00014122717574320233\tValidation Loss: 0.0001433765193350082\n",
      "Epoch 4016  \tTraining Loss: 0.00014122055133225644\tValidation Loss: 0.00014337002762627185\n",
      "Epoch 4017  \tTraining Loss: 0.00014121393023422074\tValidation Loss: 0.00014336357306989826\n",
      "Epoch 4018  \tTraining Loss: 0.00014120731299360414\tValidation Loss: 0.00014335712833497515\n",
      "Epoch 4019  \tTraining Loss: 0.00014120069963480201\tValidation Loss: 0.00014335073143958193\n",
      "Epoch 4020  \tTraining Loss: 0.00014119409020293407\tValidation Loss: 0.00014334425408936473\n",
      "Epoch 4021  \tTraining Loss: 0.00014118748399341853\tValidation Loss: 0.00014333781393317433\n",
      "Epoch 4022  \tTraining Loss: 0.0001411808816228807\tValidation Loss: 0.00014333138359868373\n",
      "Epoch 4023  \tTraining Loss: 0.00014117428304440648\tValidation Loss: 0.00014332500118741434\n",
      "Epoch 4024  \tTraining Loss: 0.00014116768851727063\tValidation Loss: 0.0001433185381396984\n",
      "Epoch 4025  \tTraining Loss: 0.00014116109712218082\tValidation Loss: 0.00014331211232291438\n",
      "Epoch 4026  \tTraining Loss: 0.00014115450954775232\tValidation Loss: 0.00014330569632490655\n",
      "Epoch 4027  \tTraining Loss: 0.00014114792567576477\tValidation Loss: 0.00014329932833162287\n",
      "Epoch 4028  \tTraining Loss: 0.0001411413459796561\tValidation Loss: 0.00014329287951900935\n",
      "Epoch 4029  \tTraining Loss: 0.00014113476932542333\tValidation Loss: 0.00014328646797333235\n",
      "Epoch 4030  \tTraining Loss: 0.00014112819647366754\tValidation Loss: 0.00014328006624295925\n",
      "Epoch 4031  \tTraining Loss: 0.0001411216271073695\tValidation Loss: 0.00014327366478999136\n",
      "Epoch 4032  \tTraining Loss: 0.0001411150612644625\tValidation Loss: 0.00014326731270745908\n",
      "Epoch 4033  \tTraining Loss: 0.000141108498975074\tValidation Loss: 0.00014326088047796867\n",
      "Epoch 4034  \tTraining Loss: 0.0001411019400163257\tValidation Loss: 0.00014325448586217587\n",
      "Epoch 4035  \tTraining Loss: 0.00014109538483991643\tValidation Loss: 0.0001432481012059583\n",
      "Epoch 4036  \tTraining Loss: 0.00014108883352367693\tValidation Loss: 0.0001432417648058981\n",
      "Epoch 4037  \tTraining Loss: 0.00014108228594327077\tValidation Loss: 0.00014323534728902118\n",
      "Epoch 4038  \tTraining Loss: 0.00014107574157008979\tValidation Loss: 0.00014322896717014003\n",
      "Epoch 4039  \tTraining Loss: 0.0001410692009608949\tValidation Loss: 0.00014322259689844222\n",
      "Epoch 4040  \tTraining Loss: 0.0001410626640837541\tValidation Loss: 0.00014321627489996812\n",
      "Epoch 4041  \tTraining Loss: 0.0001410561311435361\tValidation Loss: 0.00014320987155354788\n",
      "Epoch 4042  \tTraining Loss: 0.0001410496012829522\tValidation Loss: 0.0001432035056082165\n",
      "Epoch 4043  \tTraining Loss: 0.00014104307516837127\tValidation Loss: 0.00014319714948332398\n",
      "Epoch 4044  \tTraining Loss: 0.00014103655267196243\tValidation Loss: 0.00014319079843534281\n",
      "Epoch 4045  \tTraining Loss: 0.00014103003413285734\tValidation Loss: 0.00014318449488794134\n",
      "Epoch 4046  \tTraining Loss: 0.00014102351900134347\tValidation Loss: 0.00014317810949463167\n",
      "Epoch 4047  \tTraining Loss: 0.0001410170071823592\tValidation Loss: 0.00014317176137007174\n",
      "Epoch 4048  \tTraining Loss: 0.00014101049908639264\tValidation Loss: 0.0001431654229431932\n",
      "Epoch 4049  \tTraining Loss: 0.0001410039948098579\tValidation Loss: 0.00014315913285764228\n",
      "Epoch 4050  \tTraining Loss: 0.00014099749417360172\tValidation Loss: 0.00014315276097656314\n",
      "Epoch 4051  \tTraining Loss: 0.00014099099670548306\tValidation Loss: 0.00014314642653049048\n",
      "Epoch 4052  \tTraining Loss: 0.00014098450294353898\tValidation Loss: 0.00014314010186576204\n",
      "Epoch 4053  \tTraining Loss: 0.00014097801286495117\tValidation Loss: 0.0001431338256857268\n",
      "Epoch 4054  \tTraining Loss: 0.00014097152664712333\tValidation Loss: 0.0001431274675639349\n",
      "Epoch 4055  \tTraining Loss: 0.00014096504346032814\tValidation Loss: 0.00014312114694080998\n",
      "Epoch 4056  \tTraining Loss: 0.00014095856396238377\tValidation Loss: 0.0001431148361140157\n",
      "Epoch 4057  \tTraining Loss: 0.000140952088025238\tValidation Loss: 0.00014310853031311045\n",
      "Epoch 4058  \tTraining Loss: 0.0001409456159916381\tValidation Loss: 0.0001431022722490266\n",
      "Epoch 4059  \tTraining Loss: 0.00014093914730202407\tValidation Loss: 0.0001430959317625359\n",
      "Epoch 4060  \tTraining Loss: 0.0001409326818706329\tValidation Loss: 0.00014308962865512256\n",
      "Epoch 4061  \tTraining Loss: 0.00014092622010572066\tValidation Loss: 0.00014308333523016192\n",
      "Epoch 4062  \tTraining Loss: 0.00014091976210301878\tValidation Loss: 0.00014307709039312378\n",
      "Epoch 4063  \tTraining Loss: 0.00014091330768552422\tValidation Loss: 0.00014307076318941715\n",
      "Epoch 4064  \tTraining Loss: 0.0001409068563781073\tValidation Loss: 0.00014306447353427783\n",
      "Epoch 4065  \tTraining Loss: 0.00014090040872077257\tValidation Loss: 0.00014305819364739388\n",
      "Epoch 4066  \tTraining Loss: 0.00014089396468698097\tValidation Loss: 0.00014305196249331898\n",
      "Epoch 4067  \tTraining Loss: 0.00014088752446554025\tValidation Loss: 0.00014304564882746318\n",
      "Epoch 4068  \tTraining Loss: 0.00014088108721428425\tValidation Loss: 0.0001430393727747145\n",
      "Epoch 4069  \tTraining Loss: 0.0001408746535962176\tValidation Loss: 0.0001430331065060217\n",
      "Epoch 4070  \tTraining Loss: 0.0001408682234830286\tValidation Loss: 0.00014302684522048056\n",
      "Epoch 4071  \tTraining Loss: 0.00014086179721232558\tValidation Loss: 0.00014302063191416078\n",
      "Epoch 4072  \tTraining Loss: 0.0001408553742414182\tValidation Loss: 0.0001430143356136464\n",
      "Epoch 4073  \tTraining Loss: 0.00014084895446647981\tValidation Loss: 0.0001430080768060755\n",
      "Epoch 4074  \tTraining Loss: 0.00014084253830288332\tValidation Loss: 0.0001430018276688484\n",
      "Epoch 4075  \tTraining Loss: 0.0001408361258390676\tValidation Loss: 0.00014299562736859006\n",
      "Epoch 4076  \tTraining Loss: 0.00014082971692037311\tValidation Loss: 0.0001429893441334368\n",
      "Epoch 4077  \tTraining Loss: 0.00014082331138629207\tValidation Loss: 0.0001429832444224917\n",
      "Epoch 4078  \tTraining Loss: 0.00014081690955970242\tValidation Loss: 0.00014297687662340717\n",
      "Epoch 4079  \tTraining Loss: 0.00014081051399725603\tValidation Loss: 0.00014297081045373794\n",
      "Epoch 4080  \tTraining Loss: 0.00014080412293272418\tValidation Loss: 0.0001429644255124166\n",
      "Epoch 4081  \tTraining Loss: 0.00014079773409704765\tValidation Loss: 0.0001429581706507216\n",
      "Epoch 4082  \tTraining Loss: 0.0001407913493342454\tValidation Loss: 0.0001429520839181007\n",
      "Epoch 4083  \tTraining Loss: 0.0001407849679774276\tValidation Loss: 0.0001429457987027643\n",
      "Epoch 4084  \tTraining Loss: 0.00014077859022726332\tValidation Loss: 0.00014293966624752709\n",
      "Epoch 4085  \tTraining Loss: 0.00014077221585742989\tValidation Loss: 0.00014293333276593507\n",
      "Epoch 4086  \tTraining Loss: 0.0001407658443715509\tValidation Loss: 0.00014292710091805893\n",
      "Epoch 4087  \tTraining Loss: 0.00014075947689916\tValidation Loss: 0.00014292107648425918\n",
      "Epoch 4088  \tTraining Loss: 0.00014075311314321807\tValidation Loss: 0.00014291472097580025\n",
      "Epoch 4089  \tTraining Loss: 0.0001407467520212408\tValidation Loss: 0.00014290863935780378\n",
      "Epoch 4090  \tTraining Loss: 0.00014074039482006318\tValidation Loss: 0.00014290232868979588\n",
      "Epoch 4091  \tTraining Loss: 0.00014073404047062606\tValidation Loss: 0.0001428962607015983\n",
      "Epoch 4092  \tTraining Loss: 0.00014072769065421077\tValidation Loss: 0.0001428900027580656\n",
      "Epoch 4093  \tTraining Loss: 0.00014072134329846928\tValidation Loss: 0.0001428837529377396\n",
      "Epoch 4094  \tTraining Loss: 0.00014071499949918137\tValidation Loss: 0.00014287769913229567\n",
      "Epoch 4095  \tTraining Loss: 0.00014070865917356496\tValidation Loss: 0.0001428714073739754\n",
      "Epoch 4096  \tTraining Loss: 0.00014070232229766383\tValidation Loss: 0.00014286540102984447\n",
      "Epoch 4097  \tTraining Loss: 0.00014069598919294723\tValidation Loss: 0.00014285907198052515\n",
      "Epoch 4098  \tTraining Loss: 0.00014068965847858779\tValidation Loss: 0.0001428528727020619\n",
      "Epoch 4099  \tTraining Loss: 0.00014068333194712802\tValidation Loss: 0.00014284684142699817\n",
      "Epoch 4100  \tTraining Loss: 0.0001406770086556688\tValidation Loss: 0.00014284061184751854\n",
      "Epoch 4101  \tTraining Loss: 0.00014067068880307222\tValidation Loss: 0.00014283453445636195\n",
      "Epoch 4102  \tTraining Loss: 0.0001406643720569052\tValidation Loss: 0.00014282825605212065\n",
      "Epoch 4103  \tTraining Loss: 0.00014065805849726592\tValidation Loss: 0.00014282222527300004\n",
      "Epoch 4104  \tTraining Loss: 0.00014065174903328623\tValidation Loss: 0.0001428160055384783\n",
      "Epoch 4105  \tTraining Loss: 0.0001406454423196402\tValidation Loss: 0.00014280979376603766\n",
      "Epoch 4106  \tTraining Loss: 0.00014063913922196755\tValidation Loss: 0.0001428037783100479\n",
      "Epoch 4107  \tTraining Loss: 0.00014063283914774477\tValidation Loss: 0.00014279752493269704\n",
      "Epoch 4108  \tTraining Loss: 0.00014062654282075943\tValidation Loss: 0.0001427915574022274\n",
      "Epoch 4109  \tTraining Loss: 0.00014062024992659674\tValidation Loss: 0.00014278526660100875\n",
      "Epoch 4110  \tTraining Loss: 0.00014061395973184065\tValidation Loss: 0.00014277925171898596\n",
      "Epoch 4111  \tTraining Loss: 0.0001406076733605243\tValidation Loss: 0.00014277300811708237\n",
      "Epoch 4112  \tTraining Loss: 0.00014060139003909395\tValidation Loss: 0.00014276705226553874\n",
      "Epoch 4113  \tTraining Loss: 0.0001405951107927652\tValidation Loss: 0.0001427607735288232\n",
      "Epoch 4114  \tTraining Loss: 0.00014058883367693738\tValidation Loss: 0.00014275462506046593\n",
      "Epoch 4115  \tTraining Loss: 0.0001405825606553115\tValidation Loss: 0.0001427486447447513\n",
      "Epoch 4116  \tTraining Loss: 0.00014057629079378494\tValidation Loss: 0.00014274246648892064\n",
      "Epoch 4117  \tTraining Loss: 0.00014057002443822932\tValidation Loss: 0.00014273643989835586\n",
      "Epoch 4118  \tTraining Loss: 0.00014056376104413243\tValidation Loss: 0.00014273021240929082\n",
      "Epoch 4119  \tTraining Loss: 0.00014055750079461279\tValidation Loss: 0.00014272423267580107\n",
      "Epoch 4120  \tTraining Loss: 0.00014055124444808625\tValidation Loss: 0.00014271806424639833\n",
      "Epoch 4121  \tTraining Loss: 0.00014054499101336736\tValidation Loss: 0.00014271204911769778\n",
      "Epoch 4122  \tTraining Loss: 0.0001405387410788618\tValidation Loss: 0.00014270583367589188\n",
      "Epoch 4123  \tTraining Loss: 0.00014053249378883276\tValidation Loss: 0.0001426997202897444\n",
      "Epoch 4124  \tTraining Loss: 0.0001405262506137854\tValidation Loss: 0.00014269381518023858\n",
      "Epoch 4125  \tTraining Loss: 0.00014052001055057923\tValidation Loss: 0.00014268757735999376\n",
      "Epoch 4126  \tTraining Loss: 0.00014051377332158278\tValidation Loss: 0.0001426816140838682\n",
      "Epoch 4127  \tTraining Loss: 0.0001405075396035046\tValidation Loss: 0.0001426754215734816\n",
      "Epoch 4128  \tTraining Loss: 0.00014050130906548142\tValidation Loss: 0.00014266951707712328\n",
      "Epoch 4129  \tTraining Loss: 0.00014049508239837982\tValidation Loss: 0.00014266328883823457\n",
      "Epoch 4130  \tTraining Loss: 0.00014048885795295925\tValidation Loss: 0.0001426573369898606\n",
      "Epoch 4131  \tTraining Loss: 0.00014048263753174825\tValidation Loss: 0.00014265115645152978\n",
      "Epoch 4132  \tTraining Loss: 0.00014047641978411445\tValidation Loss: 0.00014264511828847662\n",
      "Epoch 4133  \tTraining Loss: 0.00014047020621493246\tValidation Loss: 0.00014263915317323656\n",
      "Epoch 4134  \tTraining Loss: 0.0001404639949263656\tValidation Loss: 0.00014263297864856934\n",
      "Epoch 4135  \tTraining Loss: 0.00014045778713077167\tValidation Loss: 0.00014262705029143913\n",
      "Epoch 4136  \tTraining Loss: 0.00014045158266927425\tValidation Loss: 0.00014262093302407525\n",
      "Epoch 4137  \tTraining Loss: 0.00014044538164216988\tValidation Loss: 0.00014261496833547674\n",
      "Epoch 4138  \tTraining Loss: 0.00014043918351883534\tValidation Loss: 0.00014260880331189212\n",
      "Epoch 4139  \tTraining Loss: 0.0001404329883876465\tValidation Loss: 0.00014260288632474092\n",
      "Epoch 4140  \tTraining Loss: 0.00014042679705645103\tValidation Loss: 0.00014259678105774085\n",
      "Epoch 4141  \tTraining Loss: 0.0001404206086868596\tValidation Loss: 0.00014259082840040454\n",
      "Epoch 4142  \tTraining Loss: 0.00014041442366342097\tValidation Loss: 0.00014258467558019955\n",
      "Epoch 4143  \tTraining Loss: 0.0001404082412015667\tValidation Loss: 0.0001425786249301265\n",
      "Epoch 4144  \tTraining Loss: 0.00014040206274338482\tValidation Loss: 0.00014257278295296908\n",
      "Epoch 4145  \tTraining Loss: 0.0001403958874037645\tValidation Loss: 0.00014256660738213407\n",
      "Epoch 4146  \tTraining Loss: 0.00014038971483159904\tValidation Loss: 0.00014256070666525603\n",
      "Epoch 4147  \tTraining Loss: 0.0001403835455873631\tValidation Loss: 0.00014255457664131651\n",
      "Epoch 4148  \tTraining Loss: 0.00014037737950053843\tValidation Loss: 0.00014254873513588312\n",
      "Epoch 4149  \tTraining Loss: 0.00014037121721065322\tValidation Loss: 0.000142542568964496\n",
      "Epoch 4150  \tTraining Loss: 0.00014036505715532836\tValidation Loss: 0.0001425366794820762\n",
      "Epoch 4151  \tTraining Loss: 0.00014035890086544405\tValidation Loss: 0.00014253056122998022\n",
      "Epoch 4152  \tTraining Loss: 0.00014035274727334024\tValidation Loss: 0.00014252473178497264\n",
      "Epoch 4153  \tTraining Loss: 0.00014034659790905296\tValidation Loss: 0.0001425185776436567\n",
      "Epoch 4154  \tTraining Loss: 0.0001403404503529558\tValidation Loss: 0.00014251255436149865\n",
      "Epoch 4155  \tTraining Loss: 0.00014033430681758345\tValidation Loss: 0.00014250669921503248\n",
      "Epoch 4156  \tTraining Loss: 0.00014032816600919592\tValidation Loss: 0.00014250064684282425\n",
      "Epoch 4157  \tTraining Loss: 0.00014032202902739192\tValidation Loss: 0.0001424947446674774\n",
      "Epoch 4158  \tTraining Loss: 0.00014031589448673535\tValidation Loss: 0.0001424886418279161\n",
      "Epoch 4159  \tTraining Loss: 0.00014030976319304674\tValidation Loss: 0.00014248278695408085\n",
      "Epoch 4160  \tTraining Loss: 0.00014030363521312604\tValidation Loss: 0.00014247674402009125\n",
      "Epoch 4161  \tTraining Loss: 0.00014029751061850462\tValidation Loss: 0.00014247085290251406\n",
      "Epoch 4162  \tTraining Loss: 0.0001402913888554365\tValidation Loss: 0.00014246476169498285\n",
      "Epoch 4163  \tTraining Loss: 0.00014028526991625789\tValidation Loss: 0.00014245891866214618\n",
      "Epoch 4164  \tTraining Loss: 0.00014027915467362814\tValidation Loss: 0.00014245288778556517\n",
      "Epoch 4165  \tTraining Loss: 0.00014027304241653174\tValidation Loss: 0.00014244700861490165\n",
      "Epoch 4166  \tTraining Loss: 0.00014026693336298158\tValidation Loss: 0.00014244092945403858\n",
      "Epoch 4167  \tTraining Loss: 0.00014026082676636784\tValidation Loss: 0.00014243495256599186\n",
      "Epoch 4168  \tTraining Loss: 0.0001402547240284233\tValidation Loss: 0.0001424291847799968\n",
      "Epoch 4169  \tTraining Loss: 0.00014024862444835762\tValidation Loss: 0.00014242308236591563\n",
      "Epoch 4170  \tTraining Loss: 0.00014024252751085117\tValidation Loss: 0.00014241725512849438\n",
      "Epoch 4171  \tTraining Loss: 0.0001402364337578316\tValidation Loss: 0.0001424111985197382\n",
      "Epoch 4172  \tTraining Loss: 0.0001402303430732839\tValidation Loss: 0.00014240543098541873\n",
      "Epoch 4173  \tTraining Loss: 0.00014022425616209286\tValidation Loss: 0.00014239933770967075\n",
      "Epoch 4174  \tTraining Loss: 0.00014021817142218754\tValidation Loss: 0.00014239352143364576\n",
      "Epoch 4175  \tTraining Loss: 0.00014021209024677825\tValidation Loss: 0.0001423874763207147\n",
      "Epoch 4176  \tTraining Loss: 0.0001402060117431555\tValidation Loss: 0.00014238167435750535\n",
      "Epoch 4177  \tTraining Loss: 0.00014019993723351929\tValidation Loss: 0.00014237568385364933\n",
      "Epoch 4178  \tTraining Loss: 0.00014019386493645185\tValidation Loss: 0.0001423698442645355\n",
      "Epoch 4179  \tTraining Loss: 0.00014018779615462446\tValidation Loss: 0.00014236380459889386\n",
      "Epoch 4180  \tTraining Loss: 0.0001401817297868999\tValidation Loss: 0.00014235786713159013\n",
      "Epoch 4181  \tTraining Loss: 0.00014017566760274967\tValidation Loss: 0.0001423521388813674\n",
      "Epoch 4182  \tTraining Loss: 0.00014016960779147098\tValidation Loss: 0.0001423460754393573\n",
      "Epoch 4183  \tTraining Loss: 0.00014016355096418154\tValidation Loss: 0.00014234028729335143\n",
      "Epoch 4184  \tTraining Loss: 0.00014015749723975964\tValidation Loss: 0.00014233426972578078\n",
      "Epoch 4185  \tTraining Loss: 0.0001401514469318016\tValidation Loss: 0.00014232854147824955\n",
      "Epoch 4186  \tTraining Loss: 0.0001401454004927588\tValidation Loss: 0.00014232247683642116\n",
      "Epoch 4187  \tTraining Loss: 0.00014013935873162343\tValidation Loss: 0.00014231671469188973\n",
      "Epoch 4188  \tTraining Loss: 0.00014013332037185805\tValidation Loss: 0.00014231071805507976\n",
      "Epoch 4189  \tTraining Loss: 0.0001401272850293844\tValidation Loss: 0.00014230500918652693\n",
      "Epoch 4190  \tTraining Loss: 0.00014012125312886492\tValidation Loss: 0.0001422989733103023\n",
      "Epoch 4191  \tTraining Loss: 0.00014011522326430424\tValidation Loss: 0.00014229321403132127\n",
      "Epoch 4192  \tTraining Loss: 0.00014010919735299294\tValidation Loss: 0.0001422872256362385\n",
      "Epoch 4193  \tTraining Loss: 0.0001401031740465972\tValidation Loss: 0.00014228138072983068\n",
      "Epoch 4194  \tTraining Loss: 0.0001400971542210657\tValidation Loss: 0.00014227560602031404\n",
      "Epoch 4195  \tTraining Loss: 0.00014009113680207941\tValidation Loss: 0.00014226962222957384\n",
      "Epoch 4196  \tTraining Loss: 0.00014008512257067008\tValidation Loss: 0.00014226388471877608\n",
      "Epoch 4197  \tTraining Loss: 0.00014007911183625462\tValidation Loss: 0.0001422579593363931\n",
      "Epoch 4198  \tTraining Loss: 0.00014007310370508017\tValidation Loss: 0.00014225218408322465\n",
      "Epoch 4199  \tTraining Loss: 0.0001400670986167283\tValidation Loss: 0.00014224620891507963\n",
      "Epoch 4200  \tTraining Loss: 0.0001400610961977425\tValidation Loss: 0.00014224048185195322\n",
      "Epoch 4201  \tTraining Loss: 0.0001400550977787199\tValidation Loss: 0.0001422345675712944\n",
      "Epoch 4202  \tTraining Loss: 0.00014004910140854188\tValidation Loss: 0.00014222865762519873\n",
      "Epoch 4203  \tTraining Loss: 0.00014004310846163658\tValidation Loss: 0.00014222294468686357\n",
      "Epoch 4204  \tTraining Loss: 0.00014003711815255114\tValidation Loss: 0.00014221704058930049\n",
      "Epoch 4205  \tTraining Loss: 0.00014003113163484283\tValidation Loss: 0.00014221128565803696\n",
      "Epoch 4206  \tTraining Loss: 0.00014002514730073652\tValidation Loss: 0.00014220533075635445\n",
      "Epoch 4207  \tTraining Loss: 0.0001400191660719526\tValidation Loss: 0.00014219962401765753\n",
      "Epoch 4208  \tTraining Loss: 0.00014001318800708354\tValidation Loss: 0.00014219373020954592\n",
      "Epoch 4209  \tTraining Loss: 0.00014000721313124618\tValidation Loss: 0.00014218798634902435\n",
      "Epoch 4210  \tTraining Loss: 0.0001400012409465478\tValidation Loss: 0.0001421820428266974\n",
      "Epoch 4211  \tTraining Loss: 0.00013999527138700615\tValidation Loss: 0.00014217634751854653\n",
      "Epoch 4212  \tTraining Loss: 0.00013998930548060796\tValidation Loss: 0.0001421704652708827\n",
      "Epoch 4213  \tTraining Loss: 0.0001399833422076669\tValidation Loss: 0.00014216473279582631\n",
      "Epoch 4214  \tTraining Loss: 0.00013997738211041997\tValidation Loss: 0.00014215880072756032\n",
      "Epoch 4215  \tTraining Loss: 0.00013997142426951307\tValidation Loss: 0.0001421529710312772\n",
      "Epoch 4216  \tTraining Loss: 0.00013996547016522433\tValidation Loss: 0.00014214735112239954\n",
      "Epoch 4217  \tTraining Loss: 0.00013995951898325448\tValidation Loss: 0.0001421413947336299\n",
      "Epoch 4218  \tTraining Loss: 0.00013995357022096664\tValidation Loss: 0.00014213571391412274\n",
      "Epoch 4219  \tTraining Loss: 0.00013994762465600422\tValidation Loss: 0.00014212980373339156\n",
      "Epoch 4220  \tTraining Loss: 0.00013994168195903965\tValidation Loss: 0.00014212418348754112\n",
      "Epoch 4221  \tTraining Loss: 0.0001399357427917458\tValidation Loss: 0.00014211823559158473\n",
      "Epoch 4222  \tTraining Loss: 0.00013992980553875032\tValidation Loss: 0.00014211256505331567\n",
      "Epoch 4223  \tTraining Loss: 0.00013992387194322088\tValidation Loss: 0.0001421066656986519\n",
      "Epoch 4224  \tTraining Loss: 0.00013991794077628227\tValidation Loss: 0.00014210105654105358\n",
      "Epoch 4225  \tTraining Loss: 0.00013991201351241808\tValidation Loss: 0.00014209511973168075\n",
      "Epoch 4226  \tTraining Loss: 0.00013990608783808468\tValidation Loss: 0.00014208931456769562\n",
      "Epoch 4227  \tTraining Loss: 0.00013990016585076418\tValidation Loss: 0.00014208367724705218\n",
      "Epoch 4228  \tTraining Loss: 0.00013989424668602003\tValidation Loss: 0.00014207784414058981\n",
      "Epoch 4229  \tTraining Loss: 0.00013988833060165487\tValidation Loss: 0.00014207215836724475\n",
      "Epoch 4230  \tTraining Loss: 0.00013988241705546804\tValidation Loss: 0.00014206627253314912\n",
      "Epoch 4231  \tTraining Loss: 0.000139876506304397\tValidation Loss: 0.0001420606346735927\n",
      "Epoch 4232  \tTraining Loss: 0.0001398705990484945\tValidation Loss: 0.00014205481008242496\n",
      "Epoch 4233  \tTraining Loss: 0.00013986469430624736\tValidation Loss: 0.00014204913441547497\n",
      "Epoch 4234  \tTraining Loss: 0.00013985879258271913\tValidation Loss: 0.00014204325927120415\n",
      "Epoch 4235  \tTraining Loss: 0.00013985289318305658\tValidation Loss: 0.00014203763229161584\n",
      "Epoch 4236  \tTraining Loss: 0.00013984699775465432\tValidation Loss: 0.00014203181880877326\n",
      "Epoch 4237  \tTraining Loss: 0.00013984110441462135\tValidation Loss: 0.0001420260083702085\n",
      "Epoch 4238  \tTraining Loss: 0.00013983521413914497\tValidation Loss: 0.0001420203951039638\n",
      "Epoch 4239  \tTraining Loss: 0.00013982932658543652\tValidation Loss: 0.0001420145439712586\n",
      "Epoch 4240  \tTraining Loss: 0.00013982344225313948\tValidation Loss: 0.00014200898153230156\n",
      "Epoch 4241  \tTraining Loss: 0.0001398175608548887\tValidation Loss: 0.00014200309024682382\n",
      "Epoch 4242  \tTraining Loss: 0.00013981168169283412\tValidation Loss: 0.0001419974763304352\n",
      "Epoch 4243  \tTraining Loss: 0.00013980580587533907\tValidation Loss: 0.00014199163351352213\n",
      "Epoch 4244  \tTraining Loss: 0.00013979993280213303\tValidation Loss: 0.00014198608120590995\n",
      "Epoch 4245  \tTraining Loss: 0.00013979406305620785\tValidation Loss: 0.00014198020044486643\n",
      "Epoch 4246  \tTraining Loss: 0.0001397881951060784\tValidation Loss: 0.00014197459727571866\n",
      "Epoch 4247  \tTraining Loss: 0.0001397823309404381\tValidation Loss: 0.0001419687653652221\n",
      "Epoch 4248  \tTraining Loss: 0.0001397764691821258\tValidation Loss: 0.0001419630783398799\n",
      "Epoch 4249  \tTraining Loss: 0.000139770610735832\tValidation Loss: 0.00014195745925779595\n",
      "Epoch 4250  \tTraining Loss: 0.00013976475449356518\tValidation Loss: 0.00014195163180789216\n",
      "Epoch 4251  \tTraining Loss: 0.00013975890126964436\tValidation Loss: 0.00014194605072927175\n",
      "Epoch 4252  \tTraining Loss: 0.0001397530513381384\tValidation Loss: 0.00014194028286862865\n",
      "Epoch 4253  \tTraining Loss: 0.0001397472038605664\tValidation Loss: 0.00014193466298515023\n",
      "Epoch 4254  \tTraining Loss: 0.00013974135920562914\tValidation Loss: 0.00014192884372539604\n",
      "Epoch 4255  \tTraining Loss: 0.00013973551707309486\tValidation Loss: 0.00014192327257514344\n",
      "Epoch 4256  \tTraining Loss: 0.00013972967871705159\tValidation Loss: 0.0001419175152646798\n",
      "Epoch 4257  \tTraining Loss: 0.00013972384228463233\tValidation Loss: 0.00014191190596214142\n",
      "Epoch 4258  \tTraining Loss: 0.00013971800914335203\tValidation Loss: 0.00014190609747924594\n",
      "Epoch 4259  \tTraining Loss: 0.0001397121781519564\tValidation Loss: 0.00014190043938745416\n",
      "Epoch 4260  \tTraining Loss: 0.000139706351173899\tValidation Loss: 0.00014189484965060166\n",
      "Epoch 4261  \tTraining Loss: 0.00013970052597310403\tValidation Loss: 0.00014188905202656656\n",
      "Epoch 4262  \tTraining Loss: 0.00013969470380074497\tValidation Loss: 0.00014188350095109678\n",
      "Epoch 4263  \tTraining Loss: 0.00013968888451716753\tValidation Loss: 0.00014187776345785683\n",
      "Epoch 4264  \tTraining Loss: 0.00013968306835486187\tValidation Loss: 0.0001418721735448924\n",
      "Epoch 4265  \tTraining Loss: 0.00013967725460476237\tValidation Loss: 0.00014186638443351322\n",
      "Epoch 4266  \tTraining Loss: 0.0001396714433925121\tValidation Loss: 0.00014186084345383938\n",
      "Epoch 4267  \tTraining Loss: 0.00013966563555517812\tValidation Loss: 0.00014185511658167574\n",
      "Epoch 4268  \tTraining Loss: 0.00013965983029844148\tValidation Loss: 0.0001418495372550113\n",
      "Epoch 4269  \tTraining Loss: 0.00013965402792686914\tValidation Loss: 0.00014184375888431233\n",
      "Epoch 4270  \tTraining Loss: 0.00013964822768537216\tValidation Loss: 0.00014183808300097502\n",
      "Epoch 4271  \tTraining Loss: 0.00013964243105005968\tValidation Loss: 0.00014183261763727368\n",
      "Epoch 4272  \tTraining Loss: 0.00013963663708444396\tValidation Loss: 0.00014182681380665155\n",
      "Epoch 4273  \tTraining Loss: 0.00013963084545597532\tValidation Loss: 0.00014182128599316484\n",
      "Epoch 4274  \tTraining Loss: 0.0001396250567406159\tValidation Loss: 0.00014181552891630295\n",
      "Epoch 4275  \tTraining Loss: 0.0001396192708275078\tValidation Loss: 0.00014181006272085282\n",
      "Epoch 4276  \tTraining Loss: 0.0001396134881771592\tValidation Loss: 0.00014180426680756623\n",
      "Epoch 4277  \tTraining Loss: 0.00013960770737360474\tValidation Loss: 0.00014179874867283198\n",
      "Epoch 4278  \tTraining Loss: 0.00013960192993351956\tValidation Loss: 0.00014179300182672034\n",
      "Epoch 4279  \tTraining Loss: 0.00013959615487253474\tValidation Loss: 0.00014178754611326853\n",
      "Epoch 4280  \tTraining Loss: 0.00013959038343128573\tValidation Loss: 0.00014178176069411623\n",
      "Epoch 4281  \tTraining Loss: 0.0001395846134594243\tValidation Loss: 0.00014177610751256342\n",
      "Epoch 4282  \tTraining Loss: 0.00013957884707590373\tValidation Loss: 0.00014177062189445563\n",
      "Epoch 4283  \tTraining Loss: 0.00013957308320731678\tValidation Loss: 0.0001417649417040933\n",
      "Epoch 4284  \tTraining Loss: 0.00013956732239723323\tValidation Loss: 0.00014175940662778896\n",
      "Epoch 4285  \tTraining Loss: 0.00013956156380495733\tValidation Loss: 0.00014175367209879935\n",
      "Epoch 4286  \tTraining Loss: 0.0001395558079854535\tValidation Loss: 0.0001417481854643767\n",
      "Epoch 4287  \tTraining Loss: 0.00013955005535128183\tValidation Loss: 0.0001417425132240332\n",
      "Epoch 4288  \tTraining Loss: 0.00013954430520800358\tValidation Loss: 0.0001417369876630274\n",
      "Epoch 4289  \tTraining Loss: 0.0001395385577610343\tValidation Loss: 0.00014173126323832708\n",
      "Epoch 4290  \tTraining Loss: 0.00013953281262585058\tValidation Loss: 0.0001417257868877934\n",
      "Epoch 4291  \tTraining Loss: 0.00013952707115175465\tValidation Loss: 0.00014172012516889805\n",
      "Epoch 4292  \tTraining Loss: 0.00013952133165857298\tValidation Loss: 0.0001417144644399445\n",
      "Epoch 4293  \tTraining Loss: 0.00013951559514935972\tValidation Loss: 0.0001417090010660159\n",
      "Epoch 4294  \tTraining Loss: 0.00013950986075886515\tValidation Loss: 0.00014170329954233386\n",
      "Epoch 4295  \tTraining Loss: 0.0001395041290902014\tValidation Loss: 0.00014169788623901697\n",
      "Epoch 4296  \tTraining Loss: 0.00013949839999133403\tValidation Loss: 0.0001416921436404052\n",
      "Epoch 4297  \tTraining Loss: 0.00013949267315382276\tValidation Loss: 0.00014168667921474383\n",
      "Epoch 4298  \tTraining Loss: 0.00013948694930527634\tValidation Loss: 0.00014168098616087202\n",
      "Epoch 4299  \tTraining Loss: 0.00013948122824369722\tValidation Loss: 0.0001416755845114989\n",
      "Epoch 4300  \tTraining Loss: 0.00013947551011516532\tValidation Loss: 0.00014166985236985566\n",
      "Epoch 4301  \tTraining Loss: 0.00013946979385115048\tValidation Loss: 0.000141664398141459\n",
      "Epoch 4302  \tTraining Loss: 0.0001394640809564985\tValidation Loss: 0.00014165871527603896\n",
      "Epoch 4303  \tTraining Loss: 0.00013945837048172512\tValidation Loss: 0.000141653323878831\n",
      "Epoch 4304  \tTraining Loss: 0.00013945266325304478\tValidation Loss: 0.0001416476019358213\n",
      "Epoch 4305  \tTraining Loss: 0.00013944695756847318\tValidation Loss: 0.00014164201244573656\n",
      "Epoch 4306  \tTraining Loss: 0.0001394412554065441\tValidation Loss: 0.00014163659035914175\n",
      "Epoch 4307  \tTraining Loss: 0.00013943555585191106\tValidation Loss: 0.00014163097423771382\n",
      "Epoch 4308  \tTraining Loss: 0.00013942986142506143\tValidation Loss: 0.00014162548477711804\n",
      "Epoch 4309  \tTraining Loss: 0.00013942416782268386\tValidation Loss: 0.00014161979889228344\n",
      "Epoch 4310  \tTraining Loss: 0.00013941847606662857\tValidation Loss: 0.000141614371785945\n",
      "Epoch 4311  \tTraining Loss: 0.0001394127900031798\tValidation Loss: 0.0001416087525319707\n",
      "Epoch 4312  \tTraining Loss: 0.00013940710298551107\tValidation Loss: 0.00014160330482786788\n",
      "Epoch 4313  \tTraining Loss: 0.0001394014211950438\tValidation Loss: 0.00014159763033744137\n",
      "Epoch 4314  \tTraining Loss: 0.00013939574121867276\tValidation Loss: 0.00014159220353823302\n",
      "Epoch 4315  \tTraining Loss: 0.00013939006319457515\tValidation Loss: 0.00014158660176650004\n",
      "Epoch 4316  \tTraining Loss: 0.0001393843901407362\tValidation Loss: 0.0001415811381135288\n",
      "Epoch 4317  \tTraining Loss: 0.00013937871633999072\tValidation Loss: 0.0001415755019310894\n",
      "Epoch 4318  \tTraining Loss: 0.00013937304793837935\tValidation Loss: 0.00014157008583461894\n",
      "Epoch 4319  \tTraining Loss: 0.00013936738174384337\tValidation Loss: 0.00014156448434289216\n",
      "Epoch 4320  \tTraining Loss: 0.00013936171684330342\tValidation Loss: 0.0001415590387399946\n",
      "Epoch 4321  \tTraining Loss: 0.00013935605680956686\tValidation Loss: 0.00014155338670002984\n",
      "Epoch 4322  \tTraining Loss: 0.00013935039574586484\tValidation Loss: 0.00014154800902495604\n",
      "Epoch 4323  \tTraining Loss: 0.00013934474170738037\tValidation Loss: 0.00014154241878089685\n",
      "Epoch 4324  \tTraining Loss: 0.0001393390875638159\tValidation Loss: 0.00014153697306274873\n",
      "Epoch 4325  \tTraining Loss: 0.00013933343650553246\tValidation Loss: 0.00014153134099168776\n",
      "Epoch 4326  \tTraining Loss: 0.00013932778787975733\tValidation Loss: 0.0001415259467480125\n",
      "Epoch 4327  \tTraining Loss: 0.0001393221405594407\tValidation Loss: 0.00014152037672089072\n",
      "Epoch 4328  \tTraining Loss: 0.00013931649846817104\tValidation Loss: 0.0001415149428755916\n",
      "Epoch 4329  \tTraining Loss: 0.00013931085556608854\tValidation Loss: 0.0001415093369904113\n",
      "Epoch 4330  \tTraining Loss: 0.00013930521786160324\tValidation Loss: 0.0001415039513731547\n",
      "Epoch 4331  \tTraining Loss: 0.00013929958239722887\tValidation Loss: 0.00014149837994572348\n",
      "Epoch 4332  \tTraining Loss: 0.00013929394844359322\tValidation Loss: 0.00014149296468700582\n",
      "Epoch 4333  \tTraining Loss: 0.00013928831883600013\tValidation Loss: 0.0001414873424443482\n",
      "Epoch 4334  \tTraining Loss: 0.0001392826887433616\tValidation Loss: 0.0001414819782248942\n",
      "Epoch 4335  \tTraining Loss: 0.00013927706571345493\tValidation Loss: 0.0001414764200751089\n",
      "Epoch 4336  \tTraining Loss: 0.0001392714405914075\tValidation Loss: 0.0001414710331826932\n",
      "Epoch 4337  \tTraining Loss: 0.0001392658219206894\tValidation Loss: 0.00014146542058660118\n",
      "Epoch 4338  \tTraining Loss: 0.0001392602033295744\tValidation Loss: 0.00014145990881007654\n",
      "Epoch 4339  \tTraining Loss: 0.00013925458857692332\tValidation Loss: 0.00014145462000770444\n",
      "Epoch 4340  \tTraining Loss: 0.00013924897664094378\tValidation Loss: 0.0001414489814742172\n",
      "Epoch 4341  \tTraining Loss: 0.00013924336569789763\tValidation Loss: 0.00014144362978322104\n",
      "Epoch 4342  \tTraining Loss: 0.0001392377595332022\tValidation Loss: 0.00014143803902773585\n",
      "Epoch 4343  \tTraining Loss: 0.00013923215324477943\tValidation Loss: 0.00014143275046083468\n",
      "Epoch 4344  \tTraining Loss: 0.00013922655330378958\tValidation Loss: 0.00014142711963989024\n",
      "Epoch 4345  \tTraining Loss: 0.0001392209515201754\tValidation Loss: 0.00014142179383731113\n",
      "Epoch 4346  \tTraining Loss: 0.00013921535577926986\tValidation Loss: 0.00014141621223186422\n",
      "Epoch 4347  \tTraining Loss: 0.00013920976103913796\tValidation Loss: 0.00014141092047258264\n",
      "Epoch 4348  \tTraining Loss: 0.00013920416951443602\tValidation Loss: 0.00014140530890669828\n",
      "Epoch 4349  \tTraining Loss: 0.00013919858027529195\tValidation Loss: 0.00014139996582720628\n",
      "Epoch 4350  \tTraining Loss: 0.000139192992776253\tValidation Loss: 0.00014139440544534059\n",
      "Epoch 4351  \tTraining Loss: 0.00013918740961545652\tValidation Loss: 0.00014138912670918339\n",
      "Epoch 4352  \tTraining Loss: 0.00013918182717789567\tValidation Loss: 0.00014138352663095637\n",
      "Epoch 4353  \tTraining Loss: 0.00013917624934067994\tValidation Loss: 0.00014137819392903602\n",
      "Epoch 4354  \tTraining Loss: 0.00013917067097896042\tValidation Loss: 0.00014137264391344675\n",
      "Epoch 4355  \tTraining Loss: 0.0001391650990804879\tValidation Loss: 0.00014136737522662672\n",
      "Epoch 4356  \tTraining Loss: 0.0001391595258744574\tValidation Loss: 0.0001413617853353826\n",
      "Epoch 4357  \tTraining Loss: 0.00013915395921942008\tValidation Loss: 0.0001413564625745831\n",
      "Epoch 4358  \tTraining Loss: 0.0001391483905453378\tValidation Loss: 0.00014135093902472275\n",
      "Epoch 4359  \tTraining Loss: 0.00013914282827722082\tValidation Loss: 0.00014134568066707882\n",
      "Epoch 4360  \tTraining Loss: 0.0001391372668656811\tValidation Loss: 0.00014134008680498968\n",
      "Epoch 4361  \tTraining Loss: 0.00013913170816832907\tValidation Loss: 0.00014133478436059566\n",
      "Epoch 4362  \tTraining Loss: 0.00013912615201079292\tValidation Loss: 0.00014132924299838567\n",
      "Epoch 4363  \tTraining Loss: 0.00013912059812930223\tValidation Loss: 0.0001413240059068094\n",
      "Epoch 4364  \tTraining Loss: 0.00013911504781115988\tValidation Loss: 0.00014131842485777679\n",
      "Epoch 4365  \tTraining Loss: 0.00013910949845683679\tValidation Loss: 0.00014131313380497356\n",
      "Epoch 4366  \tTraining Loss: 0.00013910395312442619\tValidation Loss: 0.00014130760273226549\n",
      "Epoch 4367  \tTraining Loss: 0.0001390984086581143\tValidation Loss: 0.00014130237591667674\n",
      "Epoch 4368  \tTraining Loss: 0.00013909286907990654\tValidation Loss: 0.00014129680465498417\n",
      "Epoch 4369  \tTraining Loss: 0.0001390873292417517\tValidation Loss: 0.0001412915336639547\n",
      "Epoch 4370  \tTraining Loss: 0.00013908179757441232\tValidation Loss: 0.00014128602557665222\n",
      "Epoch 4371  \tTraining Loss: 0.0001390762672240488\tValidation Loss: 0.0001412808190714238\n",
      "Epoch 4372  \tTraining Loss: 0.0001390707385602445\tValidation Loss: 0.00014127526738912056\n",
      "Epoch 4373  \tTraining Loss: 0.0001390652152771293\tValidation Loss: 0.0001412700054109846\n",
      "Epoch 4374  \tTraining Loss: 0.0001390596910913072\tValidation Loss: 0.0001412644872828403\n",
      "Epoch 4375  \tTraining Loss: 0.00013905417022349868\tValidation Loss: 0.00014125928857442298\n",
      "Epoch 4376  \tTraining Loss: 0.0001390486551139987\tValidation Loss: 0.0001412537607897373\n",
      "Epoch 4377  \tTraining Loss: 0.00013904313797294109\tValidation Loss: 0.00014124848160451849\n",
      "Epoch 4378  \tTraining Loss: 0.00013903762502441355\tValidation Loss: 0.00014124300024364174\n",
      "Epoch 4379  \tTraining Loss: 0.0001390321164414759\tValidation Loss: 0.00014123776480915422\n",
      "Epoch 4380  \tTraining Loss: 0.00013902660774446377\tValidation Loss: 0.00014123231510997493\n",
      "Epoch 4381  \tTraining Loss: 0.00013902110193212154\tValidation Loss: 0.0001412270338162979\n",
      "Epoch 4382  \tTraining Loss: 0.00013901560094960327\tValidation Loss: 0.00014122155877609513\n",
      "Epoch 4383  \tTraining Loss: 0.0001390100989938446\tValidation Loss: 0.0001412163516802885\n",
      "Epoch 4384  \tTraining Loss: 0.00013900460144915518\tValidation Loss: 0.0001412108382520696\n",
      "Epoch 4385  \tTraining Loss: 0.00013899910762095067\tValidation Loss: 0.0001412056075403872\n",
      "Epoch 4386  \tTraining Loss: 0.00013899361340342434\tValidation Loss: 0.0001412001187777043\n",
      "Epoch 4387  \tTraining Loss: 0.0001389881222187898\tValidation Loss: 0.00014119489802409755\n",
      "Epoch 4388  \tTraining Loss: 0.00013898263677434528\tValidation Loss: 0.0001411894975902175\n",
      "Epoch 4389  \tTraining Loss: 0.00013897714978413814\tValidation Loss: 0.00014118420915714864\n",
      "Epoch 4390  \tTraining Loss: 0.00013897166617492823\tValidation Loss: 0.00014117874882798155\n",
      "Epoch 4391  \tTraining Loss: 0.00013896618755928196\tValidation Loss: 0.00014117354055871755\n",
      "Epoch 4392  \tTraining Loss: 0.00013896070855673684\tValidation Loss: 0.00014116811884203574\n",
      "Epoch 4393  \tTraining Loss: 0.00013895523205837662\tValidation Loss: 0.0001411628496257023\n",
      "Epoch 4394  \tTraining Loss: 0.00013894976018441135\tValidation Loss: 0.00014115742554338391\n",
      "Epoch 4395  \tTraining Loss: 0.00013894428739436027\tValidation Loss: 0.00014115225097524733\n",
      "Epoch 4396  \tTraining Loss: 0.00013893881623891542\tValidation Loss: 0.00014114685192184322\n",
      "Epoch 4397  \tTraining Loss: 0.0001389333492640899\tValidation Loss: 0.000141141616290864\n",
      "Epoch 4398  \tTraining Loss: 0.00013892788456534313\tValidation Loss: 0.00014113618666957144\n",
      "Epoch 4399  \tTraining Loss: 0.00013892242008950722\tValidation Loss: 0.0001411309732558047\n",
      "Epoch 4400  \tTraining Loss: 0.00013891696060663563\tValidation Loss: 0.00014112560231214347\n",
      "Epoch 4401  \tTraining Loss: 0.00013891150381178577\tValidation Loss: 0.00014112037801199145\n",
      "Epoch 4402  \tTraining Loss: 0.00013890604673173486\tValidation Loss: 0.00014111492569734318\n",
      "Epoch 4403  \tTraining Loss: 0.00013890059341734143\tValidation Loss: 0.00014110974628545376\n",
      "Epoch 4404  \tTraining Loss: 0.00013889514455008332\tValidation Loss: 0.00014110438942671583\n",
      "Epoch 4405  \tTraining Loss: 0.0001388896950256736\tValidation Loss: 0.00014109914345003578\n",
      "Epoch 4406  \tTraining Loss: 0.00013888424875576193\tValidation Loss: 0.0001410937256489433\n",
      "Epoch 4407  \tTraining Loss: 0.00013887880719262572\tValidation Loss: 0.00014108856077003852\n",
      "Epoch 4408  \tTraining Loss: 0.0001388733651689432\tValidation Loss: 0.00014108318229251308\n",
      "Epoch 4409  \tTraining Loss: 0.00013886792603323153\tValidation Loss: 0.00014107795560779985\n",
      "Epoch 4410  \tTraining Loss: 0.00013886249095316713\tValidation Loss: 0.0001410725494775726\n",
      "Epoch 4411  \tTraining Loss: 0.0001388570575566112\tValidation Loss: 0.00014106739502821075\n",
      "Epoch 4412  \tTraining Loss: 0.00013885162514147\tValidation Loss: 0.00014106202615648896\n",
      "Epoch 4413  \tTraining Loss: 0.00013884619667360964\tValidation Loss: 0.00014105682359757263\n",
      "Epoch 4414  \tTraining Loss: 0.0001388407712925388\tValidation Loss: 0.0001410514299333806\n",
      "Epoch 4415  \tTraining Loss: 0.00013883534579819536\tValidation Loss: 0.00014104625239271212\n",
      "Epoch 4416  \tTraining Loss: 0.00013882992383133175\tValidation Loss: 0.00014104091777156585\n",
      "Epoch 4417  \tTraining Loss: 0.0001388245069909354\tValidation Loss: 0.0001410357302659925\n",
      "Epoch 4418  \tTraining Loss: 0.00013881908849232362\tValidation Loss: 0.00014103031452659444\n",
      "Epoch 4419  \tTraining Loss: 0.00013881367329100552\tValidation Loss: 0.0001410251564638604\n",
      "Epoch 4420  \tTraining Loss: 0.0001388082620937516\tValidation Loss: 0.00014101983367390046\n",
      "Epoch 4421  \tTraining Loss: 0.00013880285366697087\tValidation Loss: 0.00014101465657432742\n",
      "Epoch 4422  \tTraining Loss: 0.0001387974447117724\tValidation Loss: 0.00014100925035851156\n",
      "Epoch 4423  \tTraining Loss: 0.0001387920395335467\tValidation Loss: 0.00014100411625835905\n",
      "Epoch 4424  \tTraining Loss: 0.00013878663875830567\tValidation Loss: 0.00014099880650387685\n",
      "Epoch 4425  \tTraining Loss: 0.00013878123770610693\tValidation Loss: 0.00014099360593164376\n",
      "Epoch 4426  \tTraining Loss: 0.0001387758391335233\tValidation Loss: 0.0001409882187888269\n",
      "Epoch 4427  \tTraining Loss: 0.00013877044458469266\tValidation Loss: 0.0001409830962730806\n",
      "Epoch 4428  \tTraining Loss: 0.0001387650522032046\tValidation Loss: 0.00014097779708572948\n",
      "Epoch 4429  \tTraining Loss: 0.00013875966075855702\tValidation Loss: 0.0001409726059673707\n",
      "Epoch 4430  \tTraining Loss: 0.0001387542719971981\tValidation Loss: 0.00014096724271114877\n",
      "Epoch 4431  \tTraining Loss: 0.00013874888796710477\tValidation Loss: 0.00014096213366697748\n",
      "Epoch 4432  \tTraining Loss: 0.00013874350307878673\tValidation Loss: 0.00014095681072409715\n",
      "Epoch 4433  \tTraining Loss: 0.00013873812177465724\tValidation Loss: 0.00014095163859068332\n",
      "Epoch 4434  \tTraining Loss: 0.0001387327432556322\tValidation Loss: 0.00014094628692397379\n",
      "Epoch 4435  \tTraining Loss: 0.00013872736787774864\tValidation Loss: 0.00014094118830995794\n",
      "Epoch 4436  \tTraining Loss: 0.0001387219923756137\tValidation Loss: 0.00014093587497346347\n",
      "Epoch 4437  \tTraining Loss: 0.0001387166205963623\tValidation Loss: 0.00014093071197726986\n",
      "Epoch 4438  \tTraining Loss: 0.00013871125213206595\tValidation Loss: 0.0001409253693293301\n",
      "Epoch 4439  \tTraining Loss: 0.00013870588553825043\tValidation Loss: 0.0001409202799403274\n",
      "Epoch 4440  \tTraining Loss: 0.00013870051938236853\tValidation Loss: 0.00014091497577937366\n",
      "Epoch 4441  \tTraining Loss: 0.00013869515742852975\tValidation Loss: 0.0001409098360796495\n",
      "Epoch 4442  \tTraining Loss: 0.00013868979870675742\tValidation Loss: 0.0001409045076132263\n",
      "Epoch 4443  \tTraining Loss: 0.0001386844407583248\tValidation Loss: 0.000140899360449794\n",
      "Epoch 4444  \tTraining Loss: 0.00013867908545893174\tValidation Loss: 0.00014089407259314197\n",
      "Epoch 4445  \tTraining Loss: 0.00013867373348952256\tValidation Loss: 0.00014088892031980895\n",
      "Epoch 4446  \tTraining Loss: 0.0001386683849614017\tValidation Loss: 0.00014088359845293094\n",
      "Epoch 4447  \tTraining Loss: 0.00013866303536059685\tValidation Loss: 0.00014087848020497516\n",
      "Epoch 4448  \tTraining Loss: 0.0001386576895523998\tValidation Loss: 0.00014087311778742916\n",
      "Epoch 4449  \tTraining Loss: 0.00013865234777866065\tValidation Loss: 0.00014086810803852246\n",
      "Epoch 4450  \tTraining Loss: 0.0001386470074140251\tValidation Loss: 0.00014086272409314857\n",
      "Epoch 4451  \tTraining Loss: 0.00013864166828387624\tValidation Loss: 0.00014085761981636112\n",
      "Epoch 4452  \tTraining Loss: 0.00013863633057399372\tValidation Loss: 0.00014085226529011896\n",
      "Epoch 4453  \tTraining Loss: 0.00013863099930378465\tValidation Loss: 0.0001408472645392056\n",
      "Epoch 4454  \tTraining Loss: 0.00013862566738026246\tValidation Loss: 0.00014084205497971465\n",
      "Epoch 4455  \tTraining Loss: 0.00013862033772486004\tValidation Loss: 0.00014083667710646284\n",
      "Epoch 4456  \tTraining Loss: 0.000138615009899516\tValidation Loss: 0.0001408315979615861\n",
      "Epoch 4457  \tTraining Loss: 0.00013860968678948662\tValidation Loss: 0.00014082633446656372\n",
      "Epoch 4458  \tTraining Loss: 0.0001386043655360735\tValidation Loss: 0.00014082123786186704\n",
      "Epoch 4459  \tTraining Loss: 0.000138599043723344\tValidation Loss: 0.00014081588067693218\n",
      "Epoch 4460  \tTraining Loss: 0.00013859372609411575\tValidation Loss: 0.00014081081396466146\n",
      "Epoch 4461  \tTraining Loss: 0.00013858841133601176\tValidation Loss: 0.00014080556082752025\n",
      "Epoch 4462  \tTraining Loss: 0.00013858310015043738\tValidation Loss: 0.00014080047407672032\n",
      "Epoch 4463  \tTraining Loss: 0.00013857778700284587\tValidation Loss: 0.0001407951472603026\n",
      "Epoch 4464  \tTraining Loss: 0.00013857247869393914\tValidation Loss: 0.00014079005607203077\n",
      "Epoch 4465  \tTraining Loss: 0.00013856717352950956\tValidation Loss: 0.00014078479252378057\n",
      "Epoch 4466  \tTraining Loss: 0.0001385618703539444\tValidation Loss: 0.00014077978290416587\n",
      "Epoch 4467  \tTraining Loss: 0.00013855656782729665\tValidation Loss: 0.0001407744388397389\n",
      "Epoch 4468  \tTraining Loss: 0.0001385512677478003\tValidation Loss: 0.00014076934983647032\n",
      "Epoch 4469  \tTraining Loss: 0.00013854597222393147\tValidation Loss: 0.00014076409422663172\n",
      "Epoch 4470  \tTraining Loss: 0.00013854067768777518\tValidation Loss: 0.00014075909336602\n",
      "Epoch 4471  \tTraining Loss: 0.00013853538492801772\tValidation Loss: 0.00014075375797829962\n",
      "Epoch 4472  \tTraining Loss: 0.00013853009351559012\tValidation Loss: 0.00014074867768629838\n",
      "Epoch 4473  \tTraining Loss: 0.00013852480732294643\tValidation Loss: 0.00014074343087526394\n",
      "Epoch 4474  \tTraining Loss: 0.00013851952187155777\tValidation Loss: 0.0001407384602579356\n",
      "Epoch 4475  \tTraining Loss: 0.0001385142381434891\tValidation Loss: 0.00014073309965922365\n",
      "Epoch 4476  \tTraining Loss: 0.00013850895650007345\tValidation Loss: 0.00014072806101513425\n",
      "Epoch 4477  \tTraining Loss: 0.00013850367796005954\tValidation Loss: 0.0001407227893441689\n",
      "Epoch 4478  \tTraining Loss: 0.00013849840306136692\tValidation Loss: 0.00014071783999845935\n",
      "Epoch 4479  \tTraining Loss: 0.00013849312750150152\tValidation Loss: 0.00014071248875490913\n",
      "Epoch 4480  \tTraining Loss: 0.00013848785557696552\tValidation Loss: 0.0001407074597099867\n",
      "Epoch 4481  \tTraining Loss: 0.00013848258507000394\tValidation Loss: 0.0001407023413652052\n",
      "Epoch 4482  \tTraining Loss: 0.00013847732001202005\tValidation Loss: 0.00014069710056898336\n",
      "Epoch 4483  \tTraining Loss: 0.00013847205321659072\tValidation Loss: 0.00014069209387941524\n",
      "Epoch 4484  \tTraining Loss: 0.00013846679078527164\tValidation Loss: 0.00014068679692566625\n",
      "Epoch 4485  \tTraining Loss: 0.00013846152857713005\tValidation Loss: 0.00014068174745816522\n",
      "Epoch 4486  \tTraining Loss: 0.0001384562722262402\tValidation Loss: 0.00014067653036107334\n",
      "Epoch 4487  \tTraining Loss: 0.00013845101572751203\tValidation Loss: 0.0001406715895292992\n",
      "Epoch 4488  \tTraining Loss: 0.0001384457617880183\tValidation Loss: 0.00014066625741115417\n",
      "Epoch 4489  \tTraining Loss: 0.0001384405089463577\tValidation Loss: 0.00014066124745264717\n",
      "Epoch 4490  \tTraining Loss: 0.00013843526015684275\tValidation Loss: 0.00014065600408228886\n",
      "Epoch 4491  \tTraining Loss: 0.0001384300142074167\tValidation Loss: 0.00014065108421558312\n",
      "Epoch 4492  \tTraining Loss: 0.0001384247684631883\tValidation Loss: 0.00014064576136578453\n",
      "Epoch 4493  \tTraining Loss: 0.0001384195250797622\tValidation Loss: 0.0001406407609954002\n",
      "Epoch 4494  \tTraining Loss: 0.0001384142840721642\tValidation Loss: 0.0001406355025158962\n",
      "Epoch 4495  \tTraining Loss: 0.0001384090471502084\tValidation Loss: 0.0001406305966372737\n",
      "Epoch 4496  \tTraining Loss: 0.00013840381174039582\tValidation Loss: 0.00014062531665583216\n",
      "Epoch 4497  \tTraining Loss: 0.00013839857692895657\tValidation Loss: 0.0001406203149684293\n",
      "Epoch 4498  \tTraining Loss: 0.00013839334450967597\tValidation Loss: 0.00014061506272108348\n",
      "Epoch 4499  \tTraining Loss: 0.00013838811577162601\tValidation Loss: 0.00014061016451476048\n",
      "Epoch 4500  \tTraining Loss: 0.00013838288998950247\tValidation Loss: 0.0001406048921732454\n",
      "Epoch 4501  \tTraining Loss: 0.00013837766437588286\tValidation Loss: 0.0001405998982813203\n",
      "Epoch 4502  \tTraining Loss: 0.0001383724405536072\tValidation Loss: 0.0001405946538915711\n",
      "Epoch 4503  \tTraining Loss: 0.00013836722048288593\tValidation Loss: 0.00014058968697751035\n",
      "Epoch 4504  \tTraining Loss: 0.0001383620022497496\tValidation Loss: 0.0001405845302653395\n",
      "Epoch 4505  \tTraining Loss: 0.00013835678838872267\tValidation Loss: 0.00014057954056448933\n",
      "Epoch 4506  \tTraining Loss: 0.0001383515721386905\tValidation Loss: 0.00014057428722058534\n",
      "Epoch 4507  \tTraining Loss: 0.0001383463610874955\tValidation Loss: 0.0001405693247512726\n",
      "Epoch 4508  \tTraining Loss: 0.00013834115041179526\tValidation Loss: 0.00014056415080118827\n",
      "Epoch 4509  \tTraining Loss: 0.00013833594543880055\tValidation Loss: 0.00014055917283582023\n",
      "Epoch 4510  \tTraining Loss: 0.00013833073966836845\tValidation Loss: 0.00014055410656236056\n",
      "Epoch 4511  \tTraining Loss: 0.00013832553687018783\tValidation Loss: 0.00014054889419924384\n",
      "Epoch 4512  \tTraining Loss: 0.00013832033509802197\tValidation Loss: 0.00014054395979197064\n",
      "Epoch 4513  \tTraining Loss: 0.00013831513752280756\tValidation Loss: 0.0001405387277164032\n",
      "Epoch 4514  \tTraining Loss: 0.00013830994128032298\tValidation Loss: 0.00014053376487501093\n",
      "Epoch 4515  \tTraining Loss: 0.0001383047485414159\tValidation Loss: 0.00014052860712639634\n",
      "Epoch 4516  \tTraining Loss: 0.00013829955485575256\tValidation Loss: 0.0001405236738157002\n",
      "Epoch 4517  \tTraining Loss: 0.00013829436598791116\tValidation Loss: 0.0001405184488834633\n",
      "Epoch 4518  \tTraining Loss: 0.0001382891771757895\tValidation Loss: 0.00014051347036087478\n",
      "Epoch 4519  \tTraining Loss: 0.00013828399363599047\tValidation Loss: 0.00014050832447287957\n",
      "Epoch 4520  \tTraining Loss: 0.0001382788102254851\tValidation Loss: 0.00014050338213918013\n",
      "Epoch 4521  \tTraining Loss: 0.00013827362940741172\tValidation Loss: 0.00014049825884919457\n",
      "Epoch 4522  \tTraining Loss: 0.0001382684496241361\tValidation Loss: 0.00014049324732551827\n",
      "Epoch 4523  \tTraining Loss: 0.00013826327320253304\tValidation Loss: 0.00014048807804695426\n",
      "Epoch 4524  \tTraining Loss: 0.0001382580988754773\tValidation Loss: 0.000140483145085207\n",
      "Epoch 4525  \tTraining Loss: 0.00013825292826415657\tValidation Loss: 0.00014047806504219803\n",
      "Epoch 4526  \tTraining Loss: 0.00013824775672089293\tValidation Loss: 0.00014047305127737463\n",
      "Epoch 4527  \tTraining Loss: 0.00013824258883047178\tValidation Loss: 0.00014046788781017528\n",
      "Epoch 4528  \tTraining Loss: 0.00013823742223267868\tValidation Loss: 0.0001404629386183779\n",
      "Epoch 4529  \tTraining Loss: 0.00013823225975921003\tValidation Loss: 0.0001404578694778587\n",
      "Epoch 4530  \tTraining Loss: 0.00013822709937907565\tValidation Loss: 0.00014045289949559008\n",
      "Epoch 4531  \tTraining Loss: 0.0001382219391064886\tValidation Loss: 0.00014044773430219306\n",
      "Epoch 4532  \tTraining Loss: 0.00013821678135300815\tValidation Loss: 0.00014044279101997303\n",
      "Epoch 4533  \tTraining Loss: 0.00013821162650879136\tValidation Loss: 0.0001404377060440622\n",
      "Epoch 4534  \tTraining Loss: 0.0001382064737935203\tValidation Loss: 0.00014043274641265324\n",
      "Epoch 4535  \tTraining Loss: 0.00013820132465499572\tValidation Loss: 0.00014042776926786852\n",
      "Epoch 4536  \tTraining Loss: 0.00013819617492211476\tValidation Loss: 0.00014042257632938506\n",
      "Epoch 4537  \tTraining Loss: 0.0001381910286051072\tValidation Loss: 0.00014041773064310274\n",
      "Epoch 4538  \tTraining Loss: 0.00013818588425244845\tValidation Loss: 0.0001404125175416785\n",
      "Epoch 4539  \tTraining Loss: 0.00013818074195080083\tValidation Loss: 0.00014040761915454033\n",
      "Epoch 4540  \tTraining Loss: 0.00013817560236568008\tValidation Loss: 0.00014040248339686605\n",
      "Epoch 4541  \tTraining Loss: 0.00013817046581089223\tValidation Loss: 0.0001403976746243603\n",
      "Epoch 4542  \tTraining Loss: 0.00013816532936715225\tValidation Loss: 0.00014039245802848358\n",
      "Epoch 4543  \tTraining Loss: 0.0001381601954821728\tValidation Loss: 0.00014038756492169377\n",
      "Epoch 4544  \tTraining Loss: 0.00013815506369926125\tValidation Loss: 0.0001403824132725252\n",
      "Epoch 4545  \tTraining Loss: 0.000138149934262343\tValidation Loss: 0.00014037759152895447\n",
      "Epoch 4546  \tTraining Loss: 0.0001381448086361871\tValidation Loss: 0.00014037242098743983\n",
      "Epoch 4547  \tTraining Loss: 0.00013813968403619025\tValidation Loss: 0.00014036756240380107\n",
      "Epoch 4548  \tTraining Loss: 0.00013813456012532057\tValidation Loss: 0.0001403624063107078\n",
      "Epoch 4549  \tTraining Loss: 0.0001381294389666306\tValidation Loss: 0.00014035753588273413\n",
      "Epoch 4550  \tTraining Loss: 0.0001381243205686063\tValidation Loss: 0.00014035245335974634\n",
      "Epoch 4551  \tTraining Loss: 0.00013811920391133988\tValidation Loss: 0.00014034756449320815\n",
      "Epoch 4552  \tTraining Loss: 0.00013811409266343476\tValidation Loss: 0.00014034241629561843\n",
      "Epoch 4553  \tTraining Loss: 0.00013810898048663425\tValidation Loss: 0.00014033756797175262\n",
      "Epoch 4554  \tTraining Loss: 0.00013810387310695866\tValidation Loss: 0.00014033246118020938\n",
      "Epoch 4555  \tTraining Loss: 0.0001380987654609936\tValidation Loss: 0.00014032755215107225\n",
      "Epoch 4556  \tTraining Loss: 0.0001380936615474751\tValidation Loss: 0.00014032244488654141\n",
      "Epoch 4557  \tTraining Loss: 0.00013808855921466616\tValidation Loss: 0.00014031756396638346\n",
      "Epoch 4558  \tTraining Loss: 0.00013808346093284278\tValidation Loss: 0.00014031255290903935\n",
      "Epoch 4559  \tTraining Loss: 0.000138078362229916\tValidation Loss: 0.00014030763439501208\n",
      "Epoch 4560  \tTraining Loss: 0.00013807326669515457\tValidation Loss: 0.00014030251182498574\n",
      "Epoch 4561  \tTraining Loss: 0.00013806817230449495\tValidation Loss: 0.00014029762786540642\n",
      "Epoch 4562  \tTraining Loss: 0.00013806308022473678\tValidation Loss: 0.00014029263214261156\n",
      "Epoch 4563  \tTraining Loss: 0.00013805799234605577\tValidation Loss: 0.00014028768034976823\n",
      "Epoch 4564  \tTraining Loss: 0.00013805290434410435\tValidation Loss: 0.00014028261816893047\n",
      "Epoch 4565  \tTraining Loss: 0.0001380478200771723\tValidation Loss: 0.00014027776705742125\n",
      "Epoch 4566  \tTraining Loss: 0.00013804273602633895\tValidation Loss: 0.0001402727651943035\n",
      "Epoch 4567  \tTraining Loss: 0.00013803765617536874\tValidation Loss: 0.00014026781724425091\n",
      "Epoch 4568  \tTraining Loss: 0.00013803257488026536\tValidation Loss: 0.00014026273928229792\n",
      "Epoch 4569  \tTraining Loss: 0.0001380274994672676\tValidation Loss: 0.0001402579028525142\n",
      "Epoch 4570  \tTraining Loss: 0.00013802242445214085\tValidation Loss: 0.00014025286901028577\n",
      "Epoch 4571  \tTraining Loss: 0.00013801735362271788\tValidation Loss: 0.00014024801809589408\n",
      "Epoch 4572  \tTraining Loss: 0.00013801228113939867\tValidation Loss: 0.00014024293074116025\n",
      "Epoch 4573  \tTraining Loss: 0.00013800721347154458\tValidation Loss: 0.00014023807940960012\n",
      "Epoch 4574  \tTraining Loss: 0.00013800214637193535\tValidation Loss: 0.00014023304262343194\n",
      "Epoch 4575  \tTraining Loss: 0.00013799708171677725\tValidation Loss: 0.0001402281769624056\n",
      "Epoch 4576  \tTraining Loss: 0.00013799201983784684\tValidation Loss: 0.00014022311153215442\n",
      "Epoch 4577  \tTraining Loss: 0.00013798696009309535\tValidation Loss: 0.0001402182944433924\n",
      "Epoch 4578  \tTraining Loss: 0.00013798190291564936\tValidation Loss: 0.00014021343377711756\n",
      "Epoch 4579  \tTraining Loss: 0.0001379768458424466\tValidation Loss: 0.00014020832154046912\n",
      "Epoch 4580  \tTraining Loss: 0.00013797179243621495\tValidation Loss: 0.00014020347415122662\n",
      "Epoch 4581  \tTraining Loss: 0.00013796673978744867\tValidation Loss: 0.00014019839535591122\n",
      "Epoch 4582  \tTraining Loss: 0.00013796168954003187\tValidation Loss: 0.00014019364790067828\n",
      "Epoch 4583  \tTraining Loss: 0.0001379566428345573\tValidation Loss: 0.00014018855637206278\n",
      "Epoch 4584  \tTraining Loss: 0.00013795159717375482\tValidation Loss: 0.00014018374542755735\n",
      "Epoch 4585  \tTraining Loss: 0.00013794655436899768\tValidation Loss: 0.00014017870012539865\n",
      "Epoch 4586  \tTraining Loss: 0.0001379415114207151\tValidation Loss: 0.00014017395331961425\n",
      "Epoch 4587  \tTraining Loss: 0.00013793647344509011\tValidation Loss: 0.00014016884763728412\n",
      "Epoch 4588  \tTraining Loss: 0.00013793143459261005\tValidation Loss: 0.0001401640120085152\n",
      "Epoch 4589  \tTraining Loss: 0.00013792639892452364\tValidation Loss: 0.00014015898818559936\n",
      "Epoch 4590  \tTraining Loss: 0.00013792136611041517\tValidation Loss: 0.00014015425133725084\n",
      "Epoch 4591  \tTraining Loss: 0.00013791633555445802\tValidation Loss: 0.00014014917930808675\n",
      "Epoch 4592  \tTraining Loss: 0.0001379113069615308\tValidation Loss: 0.00014014437630548062\n",
      "Epoch 4593  \tTraining Loss: 0.00013790627796835632\tValidation Loss: 0.00014013940042732588\n",
      "Epoch 4594  \tTraining Loss: 0.0001379012546923702\tValidation Loss: 0.0001401345618699778\n",
      "Epoch 4595  \tTraining Loss: 0.0001378962305689884\tValidation Loss: 0.00014012949147653282\n",
      "Epoch 4596  \tTraining Loss: 0.00013789120914661443\tValidation Loss: 0.00014012471703968968\n",
      "Epoch 4597  \tTraining Loss: 0.0001378861898268469\tValidation Loss: 0.00014011976002693775\n",
      "Epoch 4598  \tTraining Loss: 0.0001378811737975163\tValidation Loss: 0.00014011493797398878\n",
      "Epoch 4599  \tTraining Loss: 0.00013787616004886725\tValidation Loss: 0.00014010991444536426\n",
      "Epoch 4600  \tTraining Loss: 0.00013787114523973142\tValidation Loss: 0.00014010513414889567\n",
      "Epoch 4601  \tTraining Loss: 0.0001378661354414121\tValidation Loss: 0.0001401001802503376\n",
      "Epoch 4602  \tTraining Loss: 0.0001378611265655094\tValidation Loss: 0.0001400953197304615\n",
      "Epoch 4603  \tTraining Loss: 0.00013785611949572167\tValidation Loss: 0.00014009031678321508\n",
      "Epoch 4604  \tTraining Loss: 0.0001378511141280128\tValidation Loss: 0.00014008552408437515\n",
      "Epoch 4605  \tTraining Loss: 0.0001378461113931099\tValidation Loss: 0.00014008061950976434\n",
      "Epoch 4606  \tTraining Loss: 0.00013784111207484404\tValidation Loss: 0.00014007577716654897\n",
      "Epoch 4607  \tTraining Loss: 0.00013783611322797676\tValidation Loss: 0.0001400708063933568\n",
      "Epoch 4608  \tTraining Loss: 0.0001378311162650507\tValidation Loss: 0.00014006600612828868\n",
      "Epoch 4609  \tTraining Loss: 0.00013782612130925698\tValidation Loss: 0.000140061077604766\n",
      "Epoch 4610  \tTraining Loss: 0.00013782112940462443\tValidation Loss: 0.00014005624529227141\n",
      "Epoch 4611  \tTraining Loss: 0.00013781613811088533\tValidation Loss: 0.00014005121056034228\n",
      "Epoch 4612  \tTraining Loss: 0.0001378111486185978\tValidation Loss: 0.00014004645805728314\n",
      "Epoch 4613  \tTraining Loss: 0.0001378061631360769\tValidation Loss: 0.00014004152144164608\n",
      "Epoch 4614  \tTraining Loss: 0.0001378011782628131\tValidation Loss: 0.0001400366750485917\n",
      "Epoch 4615  \tTraining Loss: 0.0001377961964099884\tValidation Loss: 0.000140031706525517\n",
      "Epoch 4616  \tTraining Loss: 0.00013779121595050293\tValidation Loss: 0.00014002695079149726\n",
      "Epoch 4617  \tTraining Loss: 0.0001377862370918676\tValidation Loss: 0.00014002204399849136\n",
      "Epoch 4618  \tTraining Loss: 0.00013778126135256417\tValidation Loss: 0.00014001718792198945\n",
      "Epoch 4619  \tTraining Loss: 0.0001377762852377506\tValidation Loss: 0.00014001220379354694\n",
      "Epoch 4620  \tTraining Loss: 0.00013777131312046287\tValidation Loss: 0.0001400074301245911\n",
      "Epoch 4621  \tTraining Loss: 0.00013776634237781818\tValidation Loss: 0.0001400025226941842\n",
      "Epoch 4622  \tTraining Loss: 0.00013776137455153318\tValidation Loss: 0.00013999771626315493\n",
      "Epoch 4623  \tTraining Loss: 0.0001377564073823259\tValidation Loss: 0.00013999272726401184\n",
      "Epoch 4624  \tTraining Loss: 0.00013775144353224406\tValidation Loss: 0.0001399880238643504\n",
      "Epoch 4625  \tTraining Loss: 0.0001377464814238439\tValidation Loss: 0.00013998309642320004\n",
      "Epoch 4626  \tTraining Loss: 0.00013774152040302515\tValidation Loss: 0.0001399782694584187\n",
      "Epoch 4627  \tTraining Loss: 0.0001377365616230929\tValidation Loss: 0.00013997330226807772\n",
      "Epoch 4628  \tTraining Loss: 0.00013773160447937052\tValidation Loss: 0.00013996854530078892\n",
      "Epoch 4629  \tTraining Loss: 0.0001377266499586827\tValidation Loss: 0.00013996367791355721\n",
      "Epoch 4630  \tTraining Loss: 0.00013772169856022962\tValidation Loss: 0.00013995884993121702\n",
      "Epoch 4631  \tTraining Loss: 0.0001377167462773389\tValidation Loss: 0.0001399538683144413\n",
      "Epoch 4632  \tTraining Loss: 0.0001377117978543574\tValidation Loss: 0.00013994915654379992\n",
      "Epoch 4633  \tTraining Loss: 0.00013770685152038125\tValidation Loss: 0.00013994428188070172\n",
      "Epoch 4634  \tTraining Loss: 0.00013770190872380093\tValidation Loss: 0.00013993952392186607\n",
      "Epoch 4635  \tTraining Loss: 0.00013769696479187557\tValidation Loss: 0.00013993452186998338\n",
      "Epoch 4636  \tTraining Loss: 0.0001376920228700265\tValidation Loss: 0.00013992984438945187\n",
      "Epoch 4637  \tTraining Loss: 0.0001376870849719605\tValidation Loss: 0.00013992485604764356\n",
      "Epoch 4638  \tTraining Loss: 0.0001376821481250357\tValidation Loss: 0.00013992013880603354\n",
      "Epoch 4639  \tTraining Loss: 0.00013767721316399294\tValidation Loss: 0.00013991517939554408\n",
      "Epoch 4640  \tTraining Loss: 0.0001376722801275708\tValidation Loss: 0.00013991050208458892\n",
      "Epoch 4641  \tTraining Loss: 0.00013766734862562084\tValidation Loss: 0.00013990550014860416\n",
      "Epoch 4642  \tTraining Loss: 0.0001376624203165523\tValidation Loss: 0.00013990082898099508\n",
      "Epoch 4643  \tTraining Loss: 0.0001376574936258775\tValidation Loss: 0.00013989586202150562\n",
      "Epoch 4644  \tTraining Loss: 0.0001376525692477633\tValidation Loss: 0.00013989125565400793\n",
      "Epoch 4645  \tTraining Loss: 0.00013764764828048988\tValidation Loss: 0.00013988624014425472\n",
      "Epoch 4646  \tTraining Loss: 0.00013764273069140673\tValidation Loss: 0.00013988156324276768\n",
      "Epoch 4647  \tTraining Loss: 0.0001376378176305772\tValidation Loss: 0.00013987662976196094\n",
      "Epoch 4648  \tTraining Loss: 0.00013763290454204958\tValidation Loss: 0.00013987197809556734\n",
      "Epoch 4649  \tTraining Loss: 0.00013762799379230735\tValidation Loss: 0.0001398670249616061\n",
      "Epoch 4650  \tTraining Loss: 0.0001376230851806493\tValidation Loss: 0.00013986231166609588\n",
      "Epoch 4651  \tTraining Loss: 0.0001376181783568471\tValidation Loss: 0.00013985741624066098\n",
      "Epoch 4652  \tTraining Loss: 0.00013761327453078908\tValidation Loss: 0.00013985278459280022\n",
      "Epoch 4653  \tTraining Loss: 0.000137608371586337\tValidation Loss: 0.00013984780206758462\n",
      "Epoch 4654  \tTraining Loss: 0.00013760346960552258\tValidation Loss: 0.00013984313442253298\n",
      "Epoch 4655  \tTraining Loss: 0.0001375985717013201\tValidation Loss: 0.00013983828922808916\n",
      "Epoch 4656  \tTraining Loss: 0.0001375936749071695\tValidation Loss: 0.00013983353364952413\n",
      "Epoch 4657  \tTraining Loss: 0.0001375887798765722\tValidation Loss: 0.00013982863684994557\n",
      "Epoch 4658  \tTraining Loss: 0.00013758388631524793\tValidation Loss: 0.00013982394813721565\n",
      "Epoch 4659  \tTraining Loss: 0.00013757899475259962\tValidation Loss: 0.00013981912126041508\n",
      "Epoch 4660  \tTraining Loss: 0.00013757410696695418\tValidation Loss: 0.00013981441529139943\n",
      "Epoch 4661  \tTraining Loss: 0.0001375692193713328\tValidation Loss: 0.00013980947700128094\n",
      "Epoch 4662  \tTraining Loss: 0.00013756433320327973\tValidation Loss: 0.00013980483301131725\n",
      "Epoch 4663  \tTraining Loss: 0.00013755945051558826\tValidation Loss: 0.00013979997759503394\n",
      "Epoch 4664  \tTraining Loss: 0.00013755456821632465\tValidation Loss: 0.00013979515112685274\n",
      "Epoch 4665  \tTraining Loss: 0.0001375496900509137\tValidation Loss: 0.00013979049119200527\n",
      "Epoch 4666  \tTraining Loss: 0.0001375448115629696\tValidation Loss: 0.00013978558644340766\n",
      "Epoch 4667  \tTraining Loss: 0.00013753993567558844\tValidation Loss: 0.00013978101146936458\n",
      "Epoch 4668  \tTraining Loss: 0.00013753506248879953\tValidation Loss: 0.0001397760587721152\n",
      "Epoch 4669  \tTraining Loss: 0.000137530189983512\tValidation Loss: 0.00013977142536417554\n",
      "Epoch 4670  \tTraining Loss: 0.0001375253207023694\tValidation Loss: 0.00013976659377578404\n",
      "Epoch 4671  \tTraining Loss: 0.00013752045293811683\tValidation Loss: 0.00013976187973310008\n",
      "Epoch 4672  \tTraining Loss: 0.00013751558566948123\tValidation Loss: 0.00013975698885132073\n",
      "Epoch 4673  \tTraining Loss: 0.00013751072192048263\tValidation Loss: 0.00013975236687755938\n",
      "Epoch 4674  \tTraining Loss: 0.00013750585990391652\tValidation Loss: 0.00013974756156496187\n",
      "Epoch 4675  \tTraining Loss: 0.00013750099976264461\tValidation Loss: 0.00013974286793789033\n",
      "Epoch 4676  \tTraining Loss: 0.00013749614132718466\tValidation Loss: 0.0001397379705608889\n",
      "Epoch 4677  \tTraining Loss: 0.000137491283662051\tValidation Loss: 0.00013973333164167114\n",
      "Epoch 4678  \tTraining Loss: 0.00013748642955637355\tValidation Loss: 0.00013972857216299712\n",
      "Epoch 4679  \tTraining Loss: 0.0001374815779771295\tValidation Loss: 0.00013972384892924783\n",
      "Epoch 4680  \tTraining Loss: 0.00013747672583815868\tValidation Loss: 0.00013971899720759674\n",
      "Epoch 4681  \tTraining Loss: 0.00013747187733956624\tValidation Loss: 0.0001397143523470978\n",
      "Epoch 4682  \tTraining Loss: 0.0001374670299482682\tValidation Loss: 0.00013970954652096822\n",
      "Epoch 4683  \tTraining Loss: 0.00013746218463340633\tValidation Loss: 0.00013970492314858006\n",
      "Epoch 4684  \tTraining Loss: 0.0001374573427667087\tValidation Loss: 0.00013970001659744055\n",
      "Epoch 4685  \tTraining Loss: 0.00013745249974882613\tValidation Loss: 0.00013969544824291453\n",
      "Epoch 4686  \tTraining Loss: 0.00013744766126715406\tValidation Loss: 0.00013969056729307112\n",
      "Epoch 4687  \tTraining Loss: 0.00013744282311064913\tValidation Loss: 0.00013968592549799342\n",
      "Epoch 4688  \tTraining Loss: 0.00013743798718615706\tValidation Loss: 0.00013968110212073077\n",
      "Epoch 4689  \tTraining Loss: 0.000137433154210535\tValidation Loss: 0.00013967640120275286\n",
      "Epoch 4690  \tTraining Loss: 0.00013742832244325147\tValidation Loss: 0.00013967173470078424\n",
      "Epoch 4691  \tTraining Loss: 0.0001374234913197804\tValidation Loss: 0.00013966690774977158\n",
      "Epoch 4692  \tTraining Loss: 0.0001374186637784031\tValidation Loss: 0.00013966228459850997\n",
      "Epoch 4693  \tTraining Loss: 0.00013741383713394555\tValidation Loss: 0.00013965752933637026\n",
      "Epoch 4694  \tTraining Loss: 0.0001374090135762796\tValidation Loss: 0.00013965287279943777\n",
      "Epoch 4695  \tTraining Loss: 0.00013740419080781593\tValidation Loss: 0.0001396480124576725\n",
      "Epoch 4696  \tTraining Loss: 0.00013739936922753835\tValidation Loss: 0.00013964340994999407\n",
      "Epoch 4697  \tTraining Loss: 0.0001373945510059919\tValidation Loss: 0.0001396386585215193\n",
      "Epoch 4698  \tTraining Loss: 0.00013738973273376727\tValidation Loss: 0.0001396340148503208\n",
      "Epoch 4699  \tTraining Loss: 0.0001373849063020051\tValidation Loss: 0.00013962912352679893\n",
      "Epoch 4700  \tTraining Loss: 0.00013738008234328203\tValidation Loss: 0.00013962453415885593\n",
      "Epoch 4701  \tTraining Loss: 0.0001373752603903206\tValidation Loss: 0.0001396197356079316\n",
      "Epoch 4702  \tTraining Loss: 0.00013737043959597184\tValidation Loss: 0.00013961510309107287\n",
      "Epoch 4703  \tTraining Loss: 0.00013736562208305079\tValidation Loss: 0.00013961027042872187\n",
      "Epoch 4704  \tTraining Loss: 0.00013736080545144127\tValidation Loss: 0.0001396056971029954\n",
      "Epoch 4705  \tTraining Loss: 0.00013735599119746376\tValidation Loss: 0.00013960083539586352\n",
      "Epoch 4706  \tTraining Loss: 0.00013735117847357687\tValidation Loss: 0.0001395962132627532\n",
      "Epoch 4707  \tTraining Loss: 0.00013734636687003204\tValidation Loss: 0.00013959138087409065\n",
      "Epoch 4708  \tTraining Loss: 0.00013734155836621058\tValidation Loss: 0.00013958689646002807\n",
      "Epoch 4709  \tTraining Loss: 0.00013733675273933826\tValidation Loss: 0.00013958197927878282\n",
      "Epoch 4710  \tTraining Loss: 0.00013733194612501395\tValidation Loss: 0.00013957737740047832\n",
      "Epoch 4711  \tTraining Loss: 0.00013732714328623202\tValidation Loss: 0.00013957257830146544\n",
      "Epoch 4712  \tTraining Loss: 0.00013732234166460367\tValidation Loss: 0.0001395680340790614\n",
      "Epoch 4713  \tTraining Loss: 0.00013731754187621228\tValidation Loss: 0.00013956321677732205\n",
      "Epoch 4714  \tTraining Loss: 0.00013731274567081646\tValidation Loss: 0.00013955858323163814\n",
      "Epoch 4715  \tTraining Loss: 0.0001373079485760386\tValidation Loss: 0.00013955376509737757\n",
      "Epoch 4716  \tTraining Loss: 0.00013730315460253216\tValidation Loss: 0.0001395492672141848\n",
      "Epoch 4717  \tTraining Loss: 0.00013729836322114503\tValidation Loss: 0.0001395443904544695\n",
      "Epoch 4718  \tTraining Loss: 0.00013729357196630963\tValidation Loss: 0.00013953980384567564\n",
      "Epoch 4719  \tTraining Loss: 0.00013728878406310565\tValidation Loss: 0.00013953504835935863\n",
      "Epoch 4720  \tTraining Loss: 0.00013728399852511029\tValidation Loss: 0.00013953049352282782\n",
      "Epoch 4721  \tTraining Loss: 0.0001372792128296724\tValidation Loss: 0.00013952563711200648\n",
      "Epoch 4722  \tTraining Loss: 0.00013727443001796292\tValidation Loss: 0.0001395210838933659\n",
      "Epoch 4723  \tTraining Loss: 0.00013726964891703814\tValidation Loss: 0.0001395162683659221\n",
      "Epoch 4724  \tTraining Loss: 0.00013726486988583015\tValidation Loss: 0.00013951178867129698\n",
      "Epoch 4725  \tTraining Loss: 0.0001372600934461094\tValidation Loss: 0.00013950694040643403\n",
      "Epoch 4726  \tTraining Loss: 0.00013725531753813554\tValidation Loss: 0.00013950235482426064\n",
      "Epoch 4727  \tTraining Loss: 0.00013725054293246453\tValidation Loss: 0.0001394976169708172\n",
      "Epoch 4728  \tTraining Loss: 0.00013724577227659806\tValidation Loss: 0.00013949303209386828\n",
      "Epoch 4729  \tTraining Loss: 0.0001372410019390684\tValidation Loss: 0.00013948824754898983\n",
      "Epoch 4730  \tTraining Loss: 0.0001372362339766198\tValidation Loss: 0.0001394836659245335\n",
      "Epoch 4731  \tTraining Loss: 0.00013723146785434088\tValidation Loss: 0.00013947896229387754\n",
      "Epoch 4732  \tTraining Loss: 0.00013722670373521827\tValidation Loss: 0.00013947434673482057\n",
      "Epoch 4733  \tTraining Loss: 0.00013722194000919739\tValidation Loss: 0.00013946957974636545\n",
      "Epoch 4734  \tTraining Loss: 0.00013721718004705875\tValidation Loss: 0.0001394650168151688\n",
      "Epoch 4735  \tTraining Loss: 0.00013721242091625858\tValidation Loss: 0.0001394603239633187\n",
      "Epoch 4736  \tTraining Loss: 0.00013720766428964462\tValidation Loss: 0.00013945572918268315\n",
      "Epoch 4737  \tTraining Loss: 0.0001372029088813644\tValidation Loss: 0.000139450930576743\n",
      "Epoch 4738  \tTraining Loss: 0.00013719815478120066\tValidation Loss: 0.000139446389672591\n",
      "Epoch 4739  \tTraining Loss: 0.0001371934029258175\tValidation Loss: 0.00013944170194920717\n",
      "Epoch 4740  \tTraining Loss: 0.00013718865438486065\tValidation Loss: 0.00013943712951947098\n",
      "Epoch 4741  \tTraining Loss: 0.00013718390581849858\tValidation Loss: 0.00013943232706176685\n",
      "Epoch 4742  \tTraining Loss: 0.00013717915933520496\tValidation Loss: 0.000139427818529161\n",
      "Epoch 4743  \tTraining Loss: 0.0001371744153795407\tValidation Loss: 0.00013942309919669224\n",
      "Epoch 4744  \tTraining Loss: 0.0001371696725061807\tValidation Loss: 0.00013941851888989574\n",
      "Epoch 4745  \tTraining Loss: 0.000137164931184568\tValidation Loss: 0.00013941377498912438\n",
      "Epoch 4746  \tTraining Loss: 0.00013716019341114641\tValidation Loss: 0.0001394093205096584\n",
      "Epoch 4747  \tTraining Loss: 0.00013715545616309398\tValidation Loss: 0.00013940448634233954\n",
      "Epoch 4748  \tTraining Loss: 0.00013715072053191785\tValidation Loss: 0.00013939997993303097\n",
      "Epoch 4749  \tTraining Loss: 0.00013714598695177142\tValidation Loss: 0.00013939521245010612\n",
      "Epoch 4750  \tTraining Loss: 0.00013714125487421106\tValidation Loss: 0.00013939075210651886\n",
      "Epoch 4751  \tTraining Loss: 0.00013713652524316398\tValidation Loss: 0.00013938600623563093\n",
      "Epoch 4752  \tTraining Loss: 0.00013713179978725847\tValidation Loss: 0.00013938146076457368\n",
      "Epoch 4753  \tTraining Loss: 0.00013712707274168476\tValidation Loss: 0.00013937674398815787\n",
      "Epoch 4754  \tTraining Loss: 0.00013712234782526432\tValidation Loss: 0.00013937230184401836\n",
      "Epoch 4755  \tTraining Loss: 0.0001371176254964073\tValidation Loss: 0.0001393674905410323\n",
      "Epoch 4756  \tTraining Loss: 0.00013711290382633647\tValidation Loss: 0.0001393629718973616\n",
      "Epoch 4757  \tTraining Loss: 0.000137108185439583\tValidation Loss: 0.00013935830433230392\n",
      "Epoch 4758  \tTraining Loss: 0.00013710347109373023\tValidation Loss: 0.00013935385021329155\n",
      "Epoch 4759  \tTraining Loss: 0.00013709875352334278\tValidation Loss: 0.00013934904807004004\n",
      "Epoch 4760  \tTraining Loss: 0.00013709403952632672\tValidation Loss: 0.000139344560440359\n",
      "Epoch 4761  \tTraining Loss: 0.00013708932775770825\tValidation Loss: 0.00013933987021789356\n",
      "Epoch 4762  \tTraining Loss: 0.00013708461763401093\tValidation Loss: 0.00013933534943274457\n",
      "Epoch 4763  \tTraining Loss: 0.00013707991079575993\tValidation Loss: 0.00013933063001940344\n",
      "Epoch 4764  \tTraining Loss: 0.00013707520527001126\tValidation Loss: 0.0001393261523023639\n",
      "Epoch 4765  \tTraining Loss: 0.0001370704989026592\tValidation Loss: 0.00013932147110376833\n",
      "Epoch 4766  \tTraining Loss: 0.00013706579587966324\tValidation Loss: 0.00013931695173231175\n",
      "Epoch 4767  \tTraining Loss: 0.0001370610948494046\tValidation Loss: 0.00013931220631957319\n",
      "Epoch 4768  \tTraining Loss: 0.00013705639518883932\tValidation Loss: 0.0001393077645228959\n",
      "Epoch 4769  \tTraining Loss: 0.00013705169973270061\tValidation Loss: 0.00013930313792900054\n",
      "Epoch 4770  \tTraining Loss: 0.00013704700365113864\tValidation Loss: 0.00013929857399166764\n",
      "Epoch 4771  \tTraining Loss: 0.0001370423083778603\tValidation Loss: 0.00013929384710773302\n",
      "Epoch 4772  \tTraining Loss: 0.0001370376154563971\tValidation Loss: 0.0001392893662263554\n",
      "Epoch 4773  \tTraining Loss: 0.00013703292558132213\tValidation Loss: 0.00013928474150687537\n",
      "Epoch 4774  \tTraining Loss: 0.00013702823761011098\tValidation Loss: 0.0001392802467210103\n",
      "Epoch 4775  \tTraining Loss: 0.0001370235516026397\tValidation Loss: 0.0001392755373581459\n",
      "Epoch 4776  \tTraining Loss: 0.00013701886550901974\tValidation Loss: 0.00013927113202386282\n",
      "Epoch 4777  \tTraining Loss: 0.00013701418227675096\tValidation Loss: 0.00013926635902127747\n",
      "Epoch 4778  \tTraining Loss: 0.0001370094994702582\tValidation Loss: 0.0001392618787995096\n",
      "Epoch 4779  \tTraining Loss: 0.0001370048194940044\tValidation Loss: 0.00013925720427347338\n",
      "Epoch 4780  \tTraining Loss: 0.00013700014252402632\tValidation Loss: 0.00013925282938274857\n",
      "Epoch 4781  \tTraining Loss: 0.00013699546764571872\tValidation Loss: 0.00013924808828120151\n",
      "Epoch 4782  \tTraining Loss: 0.0001369907912997922\tValidation Loss: 0.00013924362746253497\n",
      "Epoch 4783  \tTraining Loss: 0.00013698611851083284\tValidation Loss: 0.0001392389165416123\n",
      "Epoch 4784  \tTraining Loss: 0.00013698144678883165\tValidation Loss: 0.00013923451531681978\n",
      "Epoch 4785  \tTraining Loss: 0.00013697677726573268\tValidation Loss: 0.00013922980112658151\n",
      "Epoch 4786  \tTraining Loss: 0.00013697210887915462\tValidation Loss: 0.00013922535664618788\n",
      "Epoch 4787  \tTraining Loss: 0.0001369674348119728\tValidation Loss: 0.00013922070324011467\n",
      "Epoch 4788  \tTraining Loss: 0.0001369627611322466\tValidation Loss: 0.00013921620234259087\n",
      "Epoch 4789  \tTraining Loss: 0.00013695808854635364\tValidation Loss: 0.00013921152077304245\n",
      "Epoch 4790  \tTraining Loss: 0.000136953417382641\tValidation Loss: 0.00013920706740317172\n",
      "Epoch 4791  \tTraining Loss: 0.00013694875099633878\tValidation Loss: 0.00013920242409145328\n",
      "Epoch 4792  \tTraining Loss: 0.00013694408415886744\tValidation Loss: 0.00013919800648043003\n",
      "Epoch 4793  \tTraining Loss: 0.00013693941914444228\tValidation Loss: 0.00013919329455678732\n",
      "Epoch 4794  \tTraining Loss: 0.00013693475487260243\tValidation Loss: 0.00013918884458195298\n",
      "Epoch 4795  \tTraining Loss: 0.00013693009603260405\tValidation Loss: 0.00013918423292589346\n",
      "Epoch 4796  \tTraining Loss: 0.00013692543537443175\tValidation Loss: 0.0001391797517446505\n",
      "Epoch 4797  \tTraining Loss: 0.00013692077622736918\tValidation Loss: 0.0001391750849905115\n",
      "Epoch 4798  \tTraining Loss: 0.00013691612052144433\tValidation Loss: 0.00013917075524660124\n",
      "Epoch 4799  \tTraining Loss: 0.00013691146952202519\tValidation Loss: 0.0001391659724010161\n",
      "Epoch 4800  \tTraining Loss: 0.00013690681413612785\tValidation Loss: 0.0001391615527682444\n",
      "Epoch 4801  \tTraining Loss: 0.00013690216259932235\tValidation Loss: 0.00013915692806200898\n",
      "Epoch 4802  \tTraining Loss: 0.00013689751475055826\tValidation Loss: 0.00013915253775272752\n",
      "Epoch 4803  \tTraining Loss: 0.00013689286841221795\tValidation Loss: 0.00013914780130884794\n",
      "Epoch 4804  \tTraining Loss: 0.0001368882207888741\tValidation Loss: 0.0001391434189172852\n",
      "Epoch 4805  \tTraining Loss: 0.00013688357764566998\tValidation Loss: 0.00013913877661841706\n",
      "Epoch 4806  \tTraining Loss: 0.0001368789363454132\tValidation Loss: 0.00013913439305264383\n",
      "Epoch 4807  \tTraining Loss: 0.0001368742963700348\tValidation Loss: 0.00013912966388764455\n",
      "Epoch 4808  \tTraining Loss: 0.0001368696559426972\tValidation Loss: 0.00013912528574969472\n",
      "Epoch 4809  \tTraining Loss: 0.0001368650191400646\tValidation Loss: 0.00013912061867771868\n",
      "Epoch 4810  \tTraining Loss: 0.00013686038552888536\tValidation Loss: 0.00013911626644557977\n",
      "Epoch 4811  \tTraining Loss: 0.00013685575171547368\tValidation Loss: 0.00013911159439803557\n",
      "Epoch 4812  \tTraining Loss: 0.0001368511189934399\tValidation Loss: 0.0001391071701752018\n",
      "Epoch 4813  \tTraining Loss: 0.00013684648925165943\tValidation Loss: 0.00013910257926878813\n",
      "Epoch 4814  \tTraining Loss: 0.00013684186177716494\tValidation Loss: 0.00013909795235901074\n",
      "Epoch 4815  \tTraining Loss: 0.00013683723439247925\tValidation Loss: 0.00013909359693070614\n",
      "Epoch 4816  \tTraining Loss: 0.000136832608524922\tValidation Loss: 0.00013908895991393558\n",
      "Epoch 4817  \tTraining Loss: 0.00013682798690472866\tValidation Loss: 0.00013908463909278668\n",
      "Epoch 4818  \tTraining Loss: 0.0001368233663824171\tValidation Loss: 0.00013907993016380694\n",
      "Epoch 4819  \tTraining Loss: 0.00013681874525280234\tValidation Loss: 0.00013907552958078594\n",
      "Epoch 4820  \tTraining Loss: 0.00013681412636184047\tValidation Loss: 0.00013907089602941764\n",
      "Epoch 4821  \tTraining Loss: 0.00013680951191262947\tValidation Loss: 0.0001390665609143941\n",
      "Epoch 4822  \tTraining Loss: 0.00013680489675261075\tValidation Loss: 0.00013906189204962338\n",
      "Epoch 4823  \tTraining Loss: 0.00013680028253502288\tValidation Loss: 0.0001390575296553198\n",
      "Epoch 4824  \tTraining Loss: 0.0001367956726907336\tValidation Loss: 0.00013905293360415392\n",
      "Epoch 4825  \tTraining Loss: 0.000136791064145832\tValidation Loss: 0.0001390484915631813\n",
      "Epoch 4826  \tTraining Loss: 0.00013678645499845814\tValidation Loss: 0.00013904387195455057\n",
      "Epoch 4827  \tTraining Loss: 0.00013678184801082987\tValidation Loss: 0.00013903948131330774\n",
      "Epoch 4828  \tTraining Loss: 0.00013677724568043025\tValidation Loss: 0.00013903492632368915\n",
      "Epoch 4829  \tTraining Loss: 0.00013677264232904375\tValidation Loss: 0.00013903050035200183\n",
      "Epoch 4830  \tTraining Loss: 0.00013676804036100707\tValidation Loss: 0.0001390259183219792\n",
      "Epoch 4831  \tTraining Loss: 0.00013676344183883614\tValidation Loss: 0.0001390215072891474\n",
      "Epoch 4832  \tTraining Loss: 0.00013675884550533283\tValidation Loss: 0.00013901695784602317\n",
      "Epoch 4833  \tTraining Loss: 0.00013675424813296526\tValidation Loss: 0.00013901253888780386\n",
      "Epoch 4834  \tTraining Loss: 0.0001367496530422596\tValidation Loss: 0.00013900792063349649\n",
      "Epoch 4835  \tTraining Loss: 0.00013674506195155792\tValidation Loss: 0.0001390036065674009\n",
      "Epoch 4836  \tTraining Loss: 0.00013674047111104842\tValidation Loss: 0.00013899898905028586\n",
      "Epoch 4837  \tTraining Loss: 0.00013673588082497857\tValidation Loss: 0.00013899459585581107\n",
      "Epoch 4838  \tTraining Loss: 0.00013673129406633418\tValidation Loss: 0.0001389899901902436\n",
      "Epoch 4839  \tTraining Loss: 0.00013672670897797468\tValidation Loss: 0.00013898568459422134\n",
      "Epoch 4840  \tTraining Loss: 0.0001367221242107487\tValidation Loss: 0.0001389810462144752\n",
      "Epoch 4841  \tTraining Loss: 0.00013671754076674292\tValidation Loss: 0.00013897667099032452\n",
      "Epoch 4842  \tTraining Loss: 0.0001367129617108081\tValidation Loss: 0.00013897207924700198\n",
      "Epoch 4843  \tTraining Loss: 0.0001367083819280026\tValidation Loss: 0.0001389676776496068\n",
      "Epoch 4844  \tTraining Loss: 0.0001367038044304696\tValidation Loss: 0.0001389632505437133\n",
      "Epoch 4845  \tTraining Loss: 0.00013669923016418364\tValidation Loss: 0.00013895863114751447\n",
      "Epoch 4846  \tTraining Loss: 0.00013669465478743123\tValidation Loss: 0.00013895438791923533\n",
      "Epoch 4847  \tTraining Loss: 0.00013669008236606573\tValidation Loss: 0.00013894975001634962\n",
      "Epoch 4848  \tTraining Loss: 0.0001366855118984346\tValidation Loss: 0.00013894538419168307\n",
      "Epoch 4849  \tTraining Loss: 0.00013668094333268914\tValidation Loss: 0.0001389407765877891\n",
      "Epoch 4850  \tTraining Loss: 0.00013667637552134199\tValidation Loss: 0.00013893656909044294\n",
      "Epoch 4851  \tTraining Loss: 0.0001366718108669685\tValidation Loss: 0.0001389318988006584\n",
      "Epoch 4852  \tTraining Loss: 0.000136667247685364\tValidation Loss: 0.00013892751966135557\n",
      "Epoch 4853  \tTraining Loss: 0.00013666268460388483\tValidation Loss: 0.00013892300249789674\n",
      "Epoch 4854  \tTraining Loss: 0.00013665812715863373\tValidation Loss: 0.0001389187084748865\n",
      "Epoch 4855  \tTraining Loss: 0.00013665356998234006\tValidation Loss: 0.0001389140629513129\n",
      "Epoch 4856  \tTraining Loss: 0.00013664901218921367\tValidation Loss: 0.0001389097578338523\n",
      "Epoch 4857  \tTraining Loss: 0.0001366444615890326\tValidation Loss: 0.00013890523199630315\n",
      "Epoch 4858  \tTraining Loss: 0.00013663990711582327\tValidation Loss: 0.0001389008580696924\n",
      "Epoch 4859  \tTraining Loss: 0.0001366353570920968\tValidation Loss: 0.00013889628642197325\n",
      "Epoch 4860  \tTraining Loss: 0.00013663080741070887\tValidation Loss: 0.00013889193225291916\n",
      "Epoch 4861  \tTraining Loss: 0.00013662626022555878\tValidation Loss: 0.00013888747352530466\n",
      "Epoch 4862  \tTraining Loss: 0.00013662171579926515\tValidation Loss: 0.00013888305904400945\n",
      "Epoch 4863  \tTraining Loss: 0.0001366171691425849\tValidation Loss: 0.00013887853601835686\n",
      "Epoch 4864  \tTraining Loss: 0.0001366126296933371\tValidation Loss: 0.00013887425258865755\n",
      "Epoch 4865  \tTraining Loss: 0.00013660808789404437\tValidation Loss: 0.00013886966033522955\n",
      "Epoch 4866  \tTraining Loss: 0.00013660354840340492\tValidation Loss: 0.0001388651921355406\n",
      "Epoch 4867  \tTraining Loss: 0.0001365990113657025\tValidation Loss: 0.00013886086070726144\n",
      "Epoch 4868  \tTraining Loss: 0.00013659447438344737\tValidation Loss: 0.000138856417804516\n",
      "Epoch 4869  \tTraining Loss: 0.0001365899428443038\tValidation Loss: 0.00013885201662835025\n",
      "Epoch 4870  \tTraining Loss: 0.00013658540725578172\tValidation Loss: 0.00013884752112230256\n",
      "Epoch 4871  \tTraining Loss: 0.00013658087861369343\tValidation Loss: 0.00013884318669673127\n",
      "Epoch 4872  \tTraining Loss: 0.00013657634941991627\tValidation Loss: 0.00013883871137956763\n",
      "Epoch 4873  \tTraining Loss: 0.00013657182175047666\tValidation Loss: 0.00013883435049473448\n",
      "Epoch 4874  \tTraining Loss: 0.00013656729591162838\tValidation Loss: 0.00013882978360934818\n",
      "Epoch 4875  \tTraining Loss: 0.00013656276974803372\tValidation Loss: 0.00013882557950001593\n",
      "Epoch 4876  \tTraining Loss: 0.00013655825040816843\tValidation Loss: 0.00013882095769356093\n",
      "Epoch 4877  \tTraining Loss: 0.00013655372609518894\tValidation Loss: 0.0001388167032944154\n",
      "Epoch 4878  \tTraining Loss: 0.0001365492091695577\tValidation Loss: 0.00013881214993898932\n",
      "Epoch 4879  \tTraining Loss: 0.00013654469048422372\tValidation Loss: 0.000138807916286966\n",
      "Epoch 4880  \tTraining Loss: 0.0001365401753422004\tValidation Loss: 0.00013880333682971612\n",
      "Epoch 4881  \tTraining Loss: 0.0001365356603958428\tValidation Loss: 0.00013879901162418571\n",
      "Epoch 4882  \tTraining Loss: 0.00013653114632670163\tValidation Loss: 0.00013879450346188918\n",
      "Epoch 4883  \tTraining Loss: 0.00013652663685573247\tValidation Loss: 0.00013879028049673794\n",
      "Epoch 4884  \tTraining Loss: 0.0001365221260850487\tValidation Loss: 0.00013878570908674724\n",
      "Epoch 4885  \tTraining Loss: 0.00013651762000052334\tValidation Loss: 0.0001387812802134326\n",
      "Epoch 4886  \tTraining Loss: 0.0001365131117597756\tValidation Loss: 0.00013877706760139334\n",
      "Epoch 4887  \tTraining Loss: 0.00013650860853410302\tValidation Loss: 0.00013877253256616243\n",
      "Epoch 4888  \tTraining Loss: 0.0001365041056308134\tValidation Loss: 0.0001387682031049899\n",
      "Epoch 4889  \tTraining Loss: 0.0001364996029410129\tValidation Loss: 0.00013876370542853414\n",
      "Epoch 4890  \tTraining Loss: 0.00013649510412201503\tValidation Loss: 0.00013875946624243172\n",
      "Epoch 4891  \tTraining Loss: 0.00013649060503865442\tValidation Loss: 0.00013875494989658635\n",
      "Epoch 4892  \tTraining Loss: 0.00013648611050767986\tValidation Loss: 0.00013875065791015925\n",
      "Epoch 4893  \tTraining Loss: 0.00013648161334833353\tValidation Loss: 0.00013874620246925035\n",
      "Epoch 4894  \tTraining Loss: 0.00013647712238351844\tValidation Loss: 0.0001387418553652031\n",
      "Epoch 4895  \tTraining Loss: 0.0001364726285378508\tValidation Loss: 0.0001387374025163764\n",
      "Epoch 4896  \tTraining Loss: 0.00013646813979740146\tValidation Loss: 0.0001387331205501098\n",
      "Epoch 4897  \tTraining Loss: 0.00013646365104161412\tValidation Loss: 0.000138728659032915\n",
      "Epoch 4898  \tTraining Loss: 0.00013645916379886574\tValidation Loss: 0.00013872435488788252\n",
      "Epoch 4899  \tTraining Loss: 0.00013645467953613143\tValidation Loss: 0.0001387198869601323\n",
      "Epoch 4900  \tTraining Loss: 0.000136450195404565\tValidation Loss: 0.0001387155998961597\n",
      "Epoch 4901  \tTraining Loss: 0.00013644571465988602\tValidation Loss: 0.00013871114783573315\n",
      "Epoch 4902  \tTraining Loss: 0.00013644123203498297\tValidation Loss: 0.00013870685244969723\n",
      "Epoch 4903  \tTraining Loss: 0.0001364367557484531\tValidation Loss: 0.0001387023632157261\n",
      "Epoch 4904  \tTraining Loss: 0.00013643227585272003\tValidation Loss: 0.0001386980282153545\n",
      "Epoch 4905  \tTraining Loss: 0.00013642780312398973\tValidation Loss: 0.00013869370897656936\n",
      "Epoch 4906  \tTraining Loss: 0.00013642332606863956\tValidation Loss: 0.00013868924908454729\n",
      "Epoch 4907  \tTraining Loss: 0.00013641885586589372\tValidation Loss: 0.00013868505852097834\n",
      "Epoch 4908  \tTraining Loss: 0.00013641438584759423\tValidation Loss: 0.00013868057691120728\n",
      "Epoch 4909  \tTraining Loss: 0.0001364099159720734\tValidation Loss: 0.00013867629022851436\n",
      "Epoch 4910  \tTraining Loss: 0.0001364054482294072\tValidation Loss: 0.00013867178438734296\n",
      "Epoch 4911  \tTraining Loss: 0.00013640098178007755\tValidation Loss: 0.0001386676439378654\n",
      "Epoch 4912  \tTraining Loss: 0.00013639651915208363\tValidation Loss: 0.00013866308189882339\n",
      "Epoch 4913  \tTraining Loss: 0.00013639205427318045\tValidation Loss: 0.00013865884662658654\n",
      "Epoch 4914  \tTraining Loss: 0.0001363875944566741\tValidation Loss: 0.00013865438434987283\n",
      "Epoch 4915  \tTraining Loss: 0.00013638313416271047\tValidation Loss: 0.00013865022760006348\n",
      "Epoch 4916  \tTraining Loss: 0.0001363786786177953\tValidation Loss: 0.0001386456721875153\n",
      "Epoch 4917  \tTraining Loss: 0.0001363742189591702\tValidation Loss: 0.00013864144443637795\n",
      "Epoch 4918  \tTraining Loss: 0.00013636976672275672\tValidation Loss: 0.0001386370201385594\n",
      "Epoch 4919  \tTraining Loss: 0.0001363653111531218\tValidation Loss: 0.00013863278022011776\n",
      "Epoch 4920  \tTraining Loss: 0.00013636086298318752\tValidation Loss: 0.00013862827479383576\n",
      "Epoch 4921  \tTraining Loss: 0.0001363564091596316\tValidation Loss: 0.00013862393581785291\n",
      "Epoch 4922  \tTraining Loss: 0.00013635196290475866\tValidation Loss: 0.0001386198131609675\n",
      "Epoch 4923  \tTraining Loss: 0.0001363475159998868\tValidation Loss: 0.00013861528801718982\n",
      "Epoch 4924  \tTraining Loss: 0.00013634307108911342\tValidation Loss: 0.00013861105658924115\n",
      "Epoch 4925  \tTraining Loss: 0.00013633862692550625\tValidation Loss: 0.00013860664329148434\n",
      "Epoch 4926  \tTraining Loss: 0.00013633418500550457\tValidation Loss: 0.00013860238716223295\n",
      "Epoch 4927  \tTraining Loss: 0.0001363297444785405\tValidation Loss: 0.00013859793705421126\n",
      "Epoch 4928  \tTraining Loss: 0.00013632530537143313\tValidation Loss: 0.0001385937218580558\n",
      "Epoch 4929  \tTraining Loss: 0.00013632086862363886\tValidation Loss: 0.0001385893165058488\n",
      "Epoch 4930  \tTraining Loss: 0.00013631643220450787\tValidation Loss: 0.00013858509597295977\n",
      "Epoch 4931  \tTraining Loss: 0.00013631199921422836\tValidation Loss: 0.0001385806002731733\n",
      "Epoch 4932  \tTraining Loss: 0.0001363075651997011\tValidation Loss: 0.000138576435412982\n",
      "Epoch 4933  \tTraining Loss: 0.00013630313623120478\tValidation Loss: 0.0001385720230503502\n",
      "Epoch 4934  \tTraining Loss: 0.0001362987046998859\tValidation Loss: 0.00013856777989381492\n",
      "Epoch 4935  \tTraining Loss: 0.00013629427814168638\tValidation Loss: 0.00013856331460584636\n",
      "Epoch 4936  \tTraining Loss: 0.00013628985007990387\tValidation Loss: 0.00013855907780583427\n",
      "Epoch 4937  \tTraining Loss: 0.0001362854279516858\tValidation Loss: 0.00013855479517773747\n",
      "Epoch 4938  \tTraining Loss: 0.00013628100234404293\tValidation Loss: 0.0001385504038552891\n",
      "Epoch 4939  \tTraining Loss: 0.0001362765828548352\tValidation Loss: 0.0001385461589063378\n",
      "Epoch 4940  \tTraining Loss: 0.00013627216088657855\tValidation Loss: 0.0001385418405164822\n",
      "Epoch 4941  \tTraining Loss: 0.00013626774507823544\tValidation Loss: 0.00013853755547716138\n",
      "Epoch 4942  \tTraining Loss: 0.00013626332494280486\tValidation Loss: 0.0001385331272489348\n",
      "Epoch 4943  \tTraining Loss: 0.00013625891034630905\tValidation Loss: 0.00013852897074118653\n",
      "Epoch 4944  \tTraining Loss: 0.0001362544934872643\tValidation Loss: 0.00013852451548098337\n",
      "Epoch 4945  \tTraining Loss: 0.00013625008228817042\tValidation Loss: 0.00013852032323372646\n",
      "Epoch 4946  \tTraining Loss: 0.0001362456683650075\tValidation Loss: 0.00013851593716935367\n",
      "Epoch 4947  \tTraining Loss: 0.00013624126070007462\tValidation Loss: 0.00013851176464639434\n",
      "Epoch 4948  \tTraining Loss: 0.00013623684955524316\tValidation Loss: 0.0001385073152339842\n",
      "Epoch 4949  \tTraining Loss: 0.00013623244435408596\tValidation Loss: 0.00013850299125005942\n",
      "Epoch 4950  \tTraining Loss: 0.00013622803687643483\tValidation Loss: 0.0001384988824382824\n",
      "Epoch 4951  \tTraining Loss: 0.00013622363508833648\tValidation Loss: 0.00013849439930029502\n",
      "Epoch 4952  \tTraining Loss: 0.00013621922972822126\tValidation Loss: 0.00013849023023553055\n",
      "Epoch 4953  \tTraining Loss: 0.00013621483110111212\tValidation Loss: 0.00013848583114845677\n",
      "Epoch 4954  \tTraining Loss: 0.0001362104301399311\tValidation Loss: 0.000138481749261575\n",
      "Epoch 4955  \tTraining Loss: 0.00013620603506975376\tValidation Loss: 0.0001384772461595801\n",
      "Epoch 4956  \tTraining Loss: 0.00013620163549358182\tValidation Loss: 0.00013847308258706017\n",
      "Epoch 4957  \tTraining Loss: 0.00013619724326884995\tValidation Loss: 0.00013846872245101366\n",
      "Epoch 4958  \tTraining Loss: 0.00013619284826429495\tValidation Loss: 0.0001384645474692919\n",
      "Epoch 4959  \tTraining Loss: 0.000136188459072506\tValidation Loss: 0.00013846010437565463\n",
      "Epoch 4960  \tTraining Loss: 0.00013618406573968325\tValidation Loss: 0.00013845595555339103\n",
      "Epoch 4961  \tTraining Loss: 0.00013617967991520535\tValidation Loss: 0.00013845160245085992\n",
      "Epoch 4962  \tTraining Loss: 0.00013617529049270898\tValidation Loss: 0.00013844743419585916\n",
      "Epoch 4963  \tTraining Loss: 0.00013617090814657207\tValidation Loss: 0.00013844302619137974\n",
      "Epoch 4964  \tTraining Loss: 0.00013616652176742515\tValidation Loss: 0.0001384387814095757\n",
      "Epoch 4965  \tTraining Loss: 0.0001361621411702165\tValidation Loss: 0.00013843455681757097\n",
      "Epoch 4966  \tTraining Loss: 0.00013615775788587171\tValidation Loss: 0.00013843018532921382\n",
      "Epoch 4967  \tTraining Loss: 0.00013615338073815029\tValidation Loss: 0.00013842609875720535\n",
      "Epoch 4968  \tTraining Loss: 0.00013614900196666995\tValidation Loss: 0.00013842166741247382\n",
      "Epoch 4969  \tTraining Loss: 0.0001361446264723069\tValidation Loss: 0.00013841748296775395\n",
      "Epoch 4970  \tTraining Loss: 0.0001361402500673996\tValidation Loss: 0.00013841312335653044\n",
      "Epoch 4971  \tTraining Loss: 0.00013613587849326965\tValidation Loss: 0.00013840907229583193\n",
      "Epoch 4972  \tTraining Loss: 0.000136131507229169\tValidation Loss: 0.00013840462300958816\n",
      "Epoch 4973  \tTraining Loss: 0.00013612713678125686\tValidation Loss: 0.000138400444274511\n",
      "Epoch 4974  \tTraining Loss: 0.0001361227672459811\tValidation Loss: 0.00013839615210019108\n",
      "Epoch 4975  \tTraining Loss: 0.00013611840090290063\tValidation Loss: 0.00013839179638781916\n",
      "Epoch 4976  \tTraining Loss: 0.00013611403516662805\tValidation Loss: 0.00013838770541059673\n",
      "Epoch 4977  \tTraining Loss: 0.00013610967165747165\tValidation Loss: 0.00013838330257540208\n",
      "Epoch 4978  \tTraining Loss: 0.00013610530869632563\tValidation Loss: 0.00013837924685720422\n",
      "Epoch 4979  \tTraining Loss: 0.00013610094778755894\tValidation Loss: 0.0001383748076766232\n",
      "Epoch 4980  \tTraining Loss: 0.00013609658834129167\tValidation Loss: 0.00013837074188310705\n",
      "Epoch 4981  \tTraining Loss: 0.00013609223148996756\tValidation Loss: 0.00013836637875700708\n",
      "Epoch 4982  \tTraining Loss: 0.00013608787498159492\tValidation Loss: 0.00013836221136766016\n",
      "Epoch 4983  \tTraining Loss: 0.00013608351920523098\tValidation Loss: 0.00013835782171007322\n",
      "Epoch 4984  \tTraining Loss: 0.00013607916600781096\tValidation Loss: 0.00013835371388653528\n",
      "Epoch 4985  \tTraining Loss: 0.0001360748139720149\tValidation Loss: 0.0001383494286927228\n",
      "Epoch 4986  \tTraining Loss: 0.00013607046505759138\tValidation Loss: 0.0001383452547305638\n",
      "Epoch 4987  \tTraining Loss: 0.0001360661146138022\tValidation Loss: 0.00013834087089278959\n",
      "Epoch 4988  \tTraining Loss: 0.00013606176933312914\tValidation Loss: 0.00013833672333346154\n",
      "Epoch 4989  \tTraining Loss: 0.00013605742314140966\tValidation Loss: 0.00013833250369330345\n",
      "Epoch 4990  \tTraining Loss: 0.00013605308127151672\tValidation Loss: 0.00013832817175607747\n",
      "Epoch 4991  \tTraining Loss: 0.0001360487392810538\tValidation Loss: 0.00013832406537825758\n",
      "Epoch 4992  \tTraining Loss: 0.00013604439796453967\tValidation Loss: 0.00013831976469619474\n",
      "Epoch 4993  \tTraining Loss: 0.00013604006070656595\tValidation Loss: 0.00013831561985421377\n",
      "Epoch 4994  \tTraining Loss: 0.00013603572076854883\tValidation Loss: 0.0001383112391877049\n",
      "Epoch 4995  \tTraining Loss: 0.000136031386750045\tValidation Loss: 0.00013830721094829174\n",
      "Epoch 4996  \tTraining Loss: 0.00013602705319473292\tValidation Loss: 0.00013830282937801733\n",
      "Epoch 4997  \tTraining Loss: 0.00013602271821397906\tValidation Loss: 0.00013829871965745838\n",
      "Epoch 4998  \tTraining Loss: 0.000136018389407735\tValidation Loss: 0.00013829443195018505\n",
      "Epoch 4999  \tTraining Loss: 0.00013601405481415818\tValidation Loss: 0.00013829012084719125\n",
      "Epoch 5000  \tTraining Loss: 0.000136009726315884\tValidation Loss: 0.00013828598597622225\n",
      "Epoch 5001  \tTraining Loss: 0.00013600539710617376\tValidation Loss: 0.00013828167517858765\n",
      "Epoch 5002  \tTraining Loss: 0.0001360010681873887\tValidation Loss: 0.00013827761104657309\n",
      "Epoch 5003  \tTraining Loss: 0.0001359967448342045\tValidation Loss: 0.00013827324803504217\n",
      "Epoch 5004  \tTraining Loss: 0.00013599241803218293\tValidation Loss: 0.00013826913262707352\n",
      "Epoch 5005  \tTraining Loss: 0.00013598809621427377\tValidation Loss: 0.00013826484005312548\n",
      "Epoch 5006  \tTraining Loss: 0.00013598377586278678\tValidation Loss: 0.00013826071878380572\n",
      "Epoch 5007  \tTraining Loss: 0.0001359794519299397\tValidation Loss: 0.00013825639076709106\n",
      "Epoch 5008  \tTraining Loss: 0.00013597513316306782\tValidation Loss: 0.00013825226719920054\n",
      "Epoch 5009  \tTraining Loss: 0.00013597081473025414\tValidation Loss: 0.00013824806557200347\n",
      "Epoch 5010  \tTraining Loss: 0.00013596649653831844\tValidation Loss: 0.00013824388546668952\n",
      "Epoch 5011  \tTraining Loss: 0.0001359621824941955\tValidation Loss: 0.00013823958090218355\n",
      "Epoch 5012  \tTraining Loss: 0.0001359578655808784\tValidation Loss: 0.00013823532233742298\n",
      "Epoch 5013  \tTraining Loss: 0.00013595355403858434\tValidation Loss: 0.00013823130098563976\n",
      "Epoch 5014  \tTraining Loss: 0.0001359492440762291\tValidation Loss: 0.00013822699334378855\n",
      "Epoch 5015  \tTraining Loss: 0.00013594493221049994\tValidation Loss: 0.0001382228547014717\n",
      "Epoch 5016  \tTraining Loss: 0.00013594062531448195\tValidation Loss: 0.0001382185976601841\n",
      "Epoch 5017  \tTraining Loss: 0.00013593631910741483\tValidation Loss: 0.00013821451719187215\n",
      "Epoch 5018  \tTraining Loss: 0.0001359320125674943\tValidation Loss: 0.00013821015716387257\n",
      "Epoch 5019  \tTraining Loss: 0.00013592771067010674\tValidation Loss: 0.0001382061318184621\n",
      "Epoch 5020  \tTraining Loss: 0.00013592340674149832\tValidation Loss: 0.0001382018436518803\n",
      "Epoch 5021  \tTraining Loss: 0.00013591910609506746\tValidation Loss: 0.00013819770610892318\n",
      "Epoch 5022  \tTraining Loss: 0.00013591480762786874\tValidation Loss: 0.0001381934228983377\n",
      "Epoch 5023  \tTraining Loss: 0.00013591050724546242\tValidation Loss: 0.00013818932103397863\n",
      "Epoch 5024  \tTraining Loss: 0.00013590621362525777\tValidation Loss: 0.00013818510930761356\n",
      "Epoch 5025  \tTraining Loss: 0.00013590191878075432\tValidation Loss: 0.0001381808608359255\n",
      "Epoch 5026  \tTraining Loss: 0.00013589762326910885\tValidation Loss: 0.00013817677328594366\n",
      "Epoch 5027  \tTraining Loss: 0.00013589333372918935\tValidation Loss: 0.0001381725966314945\n",
      "Epoch 5028  \tTraining Loss: 0.00013588904194390987\tValidation Loss: 0.0001381684235626784\n",
      "Epoch 5029  \tTraining Loss: 0.00013588475333173644\tValidation Loss: 0.00013816414628123268\n",
      "Epoch 5030  \tTraining Loss: 0.00013588046707841786\tValidation Loss: 0.00013816009694434276\n",
      "Epoch 5031  \tTraining Loss: 0.00013587617909817406\tValidation Loss: 0.00013815584217397373\n",
      "Epoch 5032  \tTraining Loss: 0.00013587189520764678\tValidation Loss: 0.0001381517228916226\n",
      "Epoch 5033  \tTraining Loss: 0.00013586761283878142\tValidation Loss: 0.00013814748758603291\n",
      "Epoch 5034  \tTraining Loss: 0.00013586332967182042\tValidation Loss: 0.0001381434471877429\n",
      "Epoch 5035  \tTraining Loss: 0.0001358590514151462\tValidation Loss: 0.00013813910334479508\n",
      "Epoch 5036  \tTraining Loss: 0.00013585477228333023\tValidation Loss: 0.00013813506896135622\n",
      "Epoch 5037  \tTraining Loss: 0.00013585049358643588\tValidation Loss: 0.00013813080036407691\n",
      "Epoch 5038  \tTraining Loss: 0.00013584622063896455\tValidation Loss: 0.00013812667884580222\n",
      "Epoch 5039  \tTraining Loss: 0.0001358419450097621\tValidation Loss: 0.00013812258603635252\n",
      "Epoch 5040  \tTraining Loss: 0.00013583767193571877\tValidation Loss: 0.00013811828356052877\n",
      "Epoch 5041  \tTraining Loss: 0.00013583340205926611\tValidation Loss: 0.00013811426705938836\n",
      "Epoch 5042  \tTraining Loss: 0.00013582913045120305\tValidation Loss: 0.0001381100311394817\n",
      "Epoch 5043  \tTraining Loss: 0.00013582486214084935\tValidation Loss: 0.0001381059296725065\n",
      "Epoch 5044  \tTraining Loss: 0.00013582059695512973\tValidation Loss: 0.0001381017128136175\n",
      "Epoch 5045  \tTraining Loss: 0.00013581632959343897\tValidation Loss: 0.00013809768978094788\n",
      "Epoch 5046  \tTraining Loss: 0.00013581206655602257\tValidation Loss: 0.00013809336294475454\n",
      "Epoch 5047  \tTraining Loss: 0.00013580780440749333\tValidation Loss: 0.00013808934601097935\n",
      "Epoch 5048  \tTraining Loss: 0.00013580354177781338\tValidation Loss: 0.0001380850842992375\n",
      "Epoch 5049  \tTraining Loss: 0.0001357992836163026\tValidation Loss: 0.00013808097696277555\n",
      "Epoch 5050  \tTraining Loss: 0.00013579502680549933\tValidation Loss: 0.0001380769142090656\n",
      "Epoch 5051  \tTraining Loss: 0.00013579076732308907\tValidation Loss: 0.0001380726209530576\n",
      "Epoch 5052  \tTraining Loss: 0.00013578651328720207\tValidation Loss: 0.00013806858665439364\n",
      "Epoch 5053  \tTraining Loss: 0.0001357822599403943\tValidation Loss: 0.00013806442748050938\n",
      "Epoch 5054  \tTraining Loss: 0.00013577800587785593\tValidation Loss: 0.0001380603498142099\n",
      "Epoch 5055  \tTraining Loss: 0.00013577375710562527\tValidation Loss: 0.00013805606759463552\n",
      "Epoch 5056  \tTraining Loss: 0.00013576950720389005\tValidation Loss: 0.00013805213484573885\n",
      "Epoch 5057  \tTraining Loss: 0.0001357652577183018\tValidation Loss: 0.00013804780329724226\n",
      "Epoch 5058  \tTraining Loss: 0.00013576101246247545\tValidation Loss: 0.00013804376785613595\n",
      "Epoch 5059  \tTraining Loss: 0.00013575676761024634\tValidation Loss: 0.00013803961202496446\n",
      "Epoch 5060  \tTraining Loss: 0.0001357525232016043\tValidation Loss: 0.0001380355989114456\n",
      "Epoch 5061  \tTraining Loss: 0.0001357482834745154\tValidation Loss: 0.000138031328388758\n",
      "Epoch 5062  \tTraining Loss: 0.00013574403997070064\tValidation Loss: 0.00013802713843311858\n",
      "Epoch 5063  \tTraining Loss: 0.00013573980012257435\tValidation Loss: 0.00013802319149110574\n",
      "Epoch 5064  \tTraining Loss: 0.00013573556493189838\tValidation Loss: 0.00013801893360185019\n",
      "Epoch 5065  \tTraining Loss: 0.0001357313266021029\tValidation Loss: 0.00013801491757743542\n",
      "Epoch 5066  \tTraining Loss: 0.0001357270913713441\tValidation Loss: 0.0001380106612567719\n",
      "Epoch 5067  \tTraining Loss: 0.0001357228597945367\tValidation Loss: 0.0001380067479411164\n",
      "Epoch 5068  \tTraining Loss: 0.00013571862606688638\tValidation Loss: 0.00013800243412847674\n",
      "Epoch 5069  \tTraining Loss: 0.0001357143943122007\tValidation Loss: 0.0001379984155708302\n",
      "Epoch 5070  \tTraining Loss: 0.00013571016777805112\tValidation Loss: 0.0001379942774882152\n",
      "Epoch 5071  \tTraining Loss: 0.00013570593921059294\tValidation Loss: 0.0001379901349330841\n",
      "Epoch 5072  \tTraining Loss: 0.00013570171180652104\tValidation Loss: 0.0001379860833440363\n",
      "Epoch 5073  \tTraining Loss: 0.00013569748858998845\tValidation Loss: 0.00013798188441200424\n",
      "Epoch 5074  \tTraining Loss: 0.00013569326341674286\tValidation Loss: 0.00013797792336105\n",
      "Epoch 5075  \tTraining Loss: 0.00013568904095845685\tValidation Loss: 0.0001379736430660106\n",
      "Epoch 5076  \tTraining Loss: 0.00013568482277991688\tValidation Loss: 0.00013796970258738694\n",
      "Epoch 5077  \tTraining Loss: 0.00013568060188719119\tValidation Loss: 0.00013796544013020158\n",
      "Epoch 5078  \tTraining Loss: 0.00013567638315311928\tValidation Loss: 0.00013796150592093606\n",
      "Epoch 5079  \tTraining Loss: 0.00013567216960873747\tValidation Loss: 0.0001379572682318829\n",
      "Epoch 5080  \tTraining Loss: 0.00013566795212525937\tValidation Loss: 0.00013795324377348486\n",
      "Epoch 5081  \tTraining Loss: 0.00013566373831943104\tValidation Loss: 0.00013794914679626544\n",
      "Epoch 5082  \tTraining Loss: 0.00013565953013175972\tValidation Loss: 0.00013794509804411858\n",
      "Epoch 5083  \tTraining Loss: 0.00013565531700944348\tValidation Loss: 0.00013794084891483708\n",
      "Epoch 5084  \tTraining Loss: 0.00013565110625246403\tValidation Loss: 0.00013793671520540788\n",
      "Epoch 5085  \tTraining Loss: 0.00013564690039477424\tValidation Loss: 0.0001379328063822014\n",
      "Epoch 5086  \tTraining Loss: 0.0001356426943293783\tValidation Loss: 0.00013792858498151597\n",
      "Epoch 5087  \tTraining Loss: 0.00013563848744823495\tValidation Loss: 0.00013792460293829138\n",
      "Epoch 5088  \tTraining Loss: 0.00013563428553954417\tValidation Loss: 0.00013792044179276953\n",
      "Epoch 5089  \tTraining Loss: 0.0001356300841532243\tValidation Loss: 0.00013791644333580546\n",
      "Epoch 5090  \tTraining Loss: 0.0001356258813626687\tValidation Loss: 0.0001379122068300784\n",
      "Epoch 5091  \tTraining Loss: 0.00013562168231006744\tValidation Loss: 0.00013790822944859498\n",
      "Epoch 5092  \tTraining Loss: 0.00013561748592265315\tValidation Loss: 0.00013790413138449028\n",
      "Epoch 5093  \tTraining Loss: 0.00013561328812045008\tValidation Loss: 0.00013790000418512807\n",
      "Epoch 5094  \tTraining Loss: 0.00013560909419060727\tValidation Loss: 0.00013789599091482545\n",
      "Epoch 5095  \tTraining Loss: 0.00013560490106744708\tValidation Loss: 0.00013789189502627723\n",
      "Epoch 5096  \tTraining Loss: 0.00013560070747573193\tValidation Loss: 0.00013788784905205653\n",
      "Epoch 5097  \tTraining Loss: 0.00013559651636338473\tValidation Loss: 0.0001378836463154739\n",
      "Epoch 5098  \tTraining Loss: 0.0001355923286495807\tValidation Loss: 0.00013787974887176225\n",
      "Epoch 5099  \tTraining Loss: 0.00013558813991004694\tValidation Loss: 0.00013787558358318807\n",
      "Epoch 5100  \tTraining Loss: 0.000135583952108467\tValidation Loss: 0.00013787156319108044\n",
      "Epoch 5101  \tTraining Loss: 0.00013557976940324713\tValidation Loss: 0.00013786740431523295\n",
      "Epoch 5102  \tTraining Loss: 0.00013557558355566855\tValidation Loss: 0.00013786334764766\n",
      "Epoch 5103  \tTraining Loss: 0.00013557140095453533\tValidation Loss: 0.00013785933449004837\n",
      "Epoch 5104  \tTraining Loss: 0.00013556722113195678\tValidation Loss: 0.00013785517758060358\n",
      "Epoch 5105  \tTraining Loss: 0.0001355630425585067\tValidation Loss: 0.0001378512742016377\n",
      "Epoch 5106  \tTraining Loss: 0.00013555886317136731\tValidation Loss: 0.00013784711101722563\n",
      "Epoch 5107  \tTraining Loss: 0.00013555468661877464\tValidation Loss: 0.00013784310158373105\n",
      "Epoch 5108  \tTraining Loss: 0.00013555051223709496\tValidation Loss: 0.00013783895498522788\n",
      "Epoch 5109  \tTraining Loss: 0.00013554633635750693\tValidation Loss: 0.00013783504547708217\n",
      "Epoch 5110  \tTraining Loss: 0.00013554216447306722\tValidation Loss: 0.00013783084625113406\n",
      "Epoch 5111  \tTraining Loss: 0.0001355379951524343\tValidation Loss: 0.0001378269112199973\n",
      "Epoch 5112  \tTraining Loss: 0.0001355338237094541\tValidation Loss: 0.00013782271558970564\n",
      "Epoch 5113  \tTraining Loss: 0.00013552965453361167\tValidation Loss: 0.00013781870615164717\n",
      "Epoch 5114  \tTraining Loss: 0.00013552549153463125\tValidation Loss: 0.00013781474317492006\n",
      "Epoch 5115  \tTraining Loss: 0.00013552132802793773\tValidation Loss: 0.00013781057149460659\n",
      "Epoch 5116  \tTraining Loss: 0.00013551716597536318\tValidation Loss: 0.000137806766039398\n",
      "Epoch 5117  \tTraining Loss: 0.000135513007108285\tValidation Loss: 0.00013780259053575096\n",
      "Epoch 5118  \tTraining Loss: 0.00013550884808936618\tValidation Loss: 0.00013779864659814266\n",
      "Epoch 5119  \tTraining Loss: 0.0001355046905449873\tValidation Loss: 0.00013779451229738458\n",
      "Epoch 5120  \tTraining Loss: 0.0001355005352532265\tValidation Loss: 0.00013779051796684833\n",
      "Epoch 5121  \tTraining Loss: 0.00013549638132667917\tValidation Loss: 0.000137786537742021\n",
      "Epoch 5122  \tTraining Loss: 0.00013549222747257007\tValidation Loss: 0.00013778241432897844\n",
      "Epoch 5123  \tTraining Loss: 0.00013548807615303344\tValidation Loss: 0.00013777860977533775\n",
      "Epoch 5124  \tTraining Loss: 0.0001354839265226776\tValidation Loss: 0.0001377744136337105\n",
      "Epoch 5125  \tTraining Loss: 0.0001354797767511478\tValidation Loss: 0.00013777050300182536\n",
      "Epoch 5126  \tTraining Loss: 0.00013547562932004472\tValidation Loss: 0.00013776644175076728\n",
      "Epoch 5127  \tTraining Loss: 0.00013547148362058948\tValidation Loss: 0.00013776233693117097\n",
      "Epoch 5128  \tTraining Loss: 0.00013546733944346602\tValidation Loss: 0.0001377584107142641\n",
      "Epoch 5129  \tTraining Loss: 0.00013546319565892808\tValidation Loss: 0.00013775433644748884\n",
      "Epoch 5130  \tTraining Loss: 0.00013545905436928841\tValidation Loss: 0.00013775052416276242\n",
      "Epoch 5131  \tTraining Loss: 0.0001354549142217428\tValidation Loss: 0.0001377463292772899\n",
      "Epoch 5132  \tTraining Loss: 0.0001354507740838822\tValidation Loss: 0.00013774242916339603\n",
      "Epoch 5133  \tTraining Loss: 0.00013544663649032743\tValidation Loss: 0.00013773837942485678\n",
      "Epoch 5134  \tTraining Loss: 0.00013544250007598387\tValidation Loss: 0.0001377343019673894\n",
      "Epoch 5135  \tTraining Loss: 0.00013543836495544278\tValidation Loss: 0.0001377304305497645\n",
      "Epoch 5136  \tTraining Loss: 0.00013543423263752752\tValidation Loss: 0.0001377262754118133\n",
      "Epoch 5137  \tTraining Loss: 0.00013543009980590518\tValidation Loss: 0.0001377224670485688\n",
      "Epoch 5138  \tTraining Loss: 0.00013542596904639266\tValidation Loss: 0.0001377183090386098\n",
      "Epoch 5139  \tTraining Loss: 0.00013542183881950815\tValidation Loss: 0.00013771442103905314\n",
      "Epoch 5140  \tTraining Loss: 0.0001354177110795348\tValidation Loss: 0.00013771038147973903\n",
      "Epoch 5141  \tTraining Loss: 0.00013541358416555946\tValidation Loss: 0.00013770631374246807\n",
      "Epoch 5142  \tTraining Loss: 0.00013540945871057963\tValidation Loss: 0.00013770246876746554\n",
      "Epoch 5143  \tTraining Loss: 0.00013540533519367513\tValidation Loss: 0.00013769844615696292\n",
      "Epoch 5144  \tTraining Loss: 0.00013540121314681094\tValidation Loss: 0.0001376944746571554\n",
      "Epoch 5145  \tTraining Loss: 0.00013539709216717249\tValidation Loss: 0.00013769033534267696\n",
      "Epoch 5146  \tTraining Loss: 0.0001353929712020967\tValidation Loss: 0.00013768647590926313\n",
      "Epoch 5147  \tTraining Loss: 0.0001353888532277833\tValidation Loss: 0.0001376824509992318\n",
      "Epoch 5148  \tTraining Loss: 0.00013538473575815235\tValidation Loss: 0.00013767839495480396\n",
      "Epoch 5149  \tTraining Loss: 0.0001353806199727931\tValidation Loss: 0.0001376745603790066\n",
      "Epoch 5150  \tTraining Loss: 0.00013537650591037093\tValidation Loss: 0.0001376705193495218\n",
      "Epoch 5151  \tTraining Loss: 0.00013537239260740785\tValidation Loss: 0.00013766659842005021\n",
      "Epoch 5152  \tTraining Loss: 0.00013536828079069736\tValidation Loss: 0.00013766250129178056\n",
      "Epoch 5153  \tTraining Loss: 0.00013536416994983135\tValidation Loss: 0.0001376585675528914\n",
      "Epoch 5154  \tTraining Loss: 0.00013536006179602007\tValidation Loss: 0.0001376546451553439\n",
      "Epoch 5155  \tTraining Loss: 0.00013535595441568064\tValidation Loss: 0.00013765052392353857\n",
      "Epoch 5156  \tTraining Loss: 0.000135351847750808\tValidation Loss: 0.00013764673842746794\n",
      "Epoch 5157  \tTraining Loss: 0.00013534774403363688\tValidation Loss: 0.00013764268578577462\n",
      "Epoch 5158  \tTraining Loss: 0.00013534363960622737\tValidation Loss: 0.00013763863990109296\n",
      "Epoch 5159  \tTraining Loss: 0.0001353395372465168\tValidation Loss: 0.00013763478709883406\n",
      "Epoch 5160  \tTraining Loss: 0.00013533543629258907\tValidation Loss: 0.00013763077722466868\n",
      "Epoch 5161  \tTraining Loss: 0.00013533133697623333\tValidation Loss: 0.0001376268723571369\n",
      "Epoch 5162  \tTraining Loss: 0.00013532723855944043\tValidation Loss: 0.00013762279057594973\n",
      "Epoch 5163  \tTraining Loss: 0.00013532314169646567\tValidation Loss: 0.00013761903711246047\n",
      "Epoch 5164  \tTraining Loss: 0.0001353190473181712\tValidation Loss: 0.00013761487667768226\n",
      "Epoch 5165  \tTraining Loss: 0.0001353149530615683\tValidation Loss: 0.00013761085211665025\n",
      "Epoch 5166  \tTraining Loss: 0.00013531085984406536\tValidation Loss: 0.0001376070277977716\n",
      "Epoch 5167  \tTraining Loss: 0.00013530676866898184\tValidation Loss: 0.00013760303234553055\n",
      "Epoch 5168  \tTraining Loss: 0.00013530267846746026\tValidation Loss: 0.00013759913894077086\n",
      "Epoch 5169  \tTraining Loss: 0.00013529858961945418\tValidation Loss: 0.0001375950676249658\n",
      "Epoch 5170  \tTraining Loss: 0.00013529450234430382\tValidation Loss: 0.00013759121777190717\n",
      "Epoch 5171  \tTraining Loss: 0.00013529041767984087\tValidation Loss: 0.00013758729295876027\n",
      "Epoch 5172  \tTraining Loss: 0.00013528633195976795\tValidation Loss: 0.0001375832232646811\n",
      "Epoch 5173  \tTraining Loss: 0.00013528224837096667\tValidation Loss: 0.0001375794520680567\n",
      "Epoch 5174  \tTraining Loss: 0.0001352781669543986\tValidation Loss: 0.000137575339711099\n",
      "Epoch 5175  \tTraining Loss: 0.00013527408559097239\tValidation Loss: 0.00013757136371806864\n",
      "Epoch 5176  \tTraining Loss: 0.00013527000655125918\tValidation Loss: 0.00013756752661744952\n",
      "Epoch 5177  \tTraining Loss: 0.00013526592958309006\tValidation Loss: 0.00013756354092107106\n",
      "Epoch 5178  \tTraining Loss: 0.00013526185296692308\tValidation Loss: 0.00013755965285704234\n",
      "Epoch 5179  \tTraining Loss: 0.00013525777710923046\tValidation Loss: 0.00013755559500642223\n",
      "Epoch 5180  \tTraining Loss: 0.00013525370297382414\tValidation Loss: 0.00013755183369312254\n",
      "Epoch 5181  \tTraining Loss: 0.00013524963085726407\tValidation Loss: 0.00013754773076701647\n",
      "Epoch 5182  \tTraining Loss: 0.00013524555885612724\tValidation Loss: 0.00013754376410102318\n",
      "Epoch 5183  \tTraining Loss: 0.00013524148900077328\tValidation Loss: 0.00013754001623103622\n",
      "Epoch 5184  \tTraining Loss: 0.0001352374207664949\tValidation Loss: 0.00013753594854078288\n",
      "Epoch 5185  \tTraining Loss: 0.00013523335373170724\tValidation Loss: 0.00013753212847885892\n",
      "Epoch 5186  \tTraining Loss: 0.0001352292878468305\tValidation Loss: 0.00013752813335008972\n",
      "Epoch 5187  \tTraining Loss: 0.00013522522318134262\tValidation Loss: 0.0001375241248274739\n",
      "Epoch 5188  \tTraining Loss: 0.0001352211596590014\tValidation Loss: 0.00013752029243884297\n",
      "Epoch 5189  \tTraining Loss: 0.00013521709828050077\tValidation Loss: 0.000137516225433419\n",
      "Epoch 5190  \tTraining Loss: 0.00013521303722280854\tValidation Loss: 0.0001375124915859797\n",
      "Epoch 5191  \tTraining Loss: 0.0001352089782548013\tValidation Loss: 0.00013750840691595113\n",
      "Epoch 5192  \tTraining Loss: 0.00013520491966060983\tValidation Loss: 0.00013750448709747213\n",
      "Epoch 5193  \tTraining Loss: 0.0001352008637781165\tValidation Loss: 0.00013750073550407055\n",
      "Epoch 5194  \tTraining Loss: 0.00013519680841819512\tValidation Loss: 0.00013749665008706372\n",
      "Epoch 5195  \tTraining Loss: 0.00013519275381744042\tValidation Loss: 0.00013749283489261386\n",
      "Epoch 5196  \tTraining Loss: 0.00013518870134654302\tValidation Loss: 0.00013748887146121524\n",
      "Epoch 5197  \tTraining Loss: 0.00013518465001488824\tValidation Loss: 0.00013748487854770133\n",
      "Epoch 5198  \tTraining Loss: 0.00013518059975786166\tValidation Loss: 0.00013748107765337666\n",
      "Epoch 5199  \tTraining Loss: 0.00013517655089404025\tValidation Loss: 0.00013747714980816218\n",
      "Epoch 5200  \tTraining Loss: 0.00013517250437394693\tValidation Loss: 0.00013747330346476695\n",
      "Epoch 5201  \tTraining Loss: 0.00013516845838027418\tValidation Loss: 0.00013746925542002403\n",
      "Epoch 5202  \tTraining Loss: 0.00013516441322073645\tValidation Loss: 0.0001374653063356499\n",
      "Epoch 5203  \tTraining Loss: 0.00013516037058373376\tValidation Loss: 0.00013746155311018606\n",
      "Epoch 5204  \tTraining Loss: 0.0001351563280957368\tValidation Loss: 0.0001374574953171303\n",
      "Epoch 5205  \tTraining Loss: 0.000135152286707758\tValidation Loss: 0.00013745369852440902\n",
      "Epoch 5206  \tTraining Loss: 0.00013514824750629698\tValidation Loss: 0.00013744975115273798\n",
      "Epoch 5207  \tTraining Loss: 0.0001351442092341924\tValidation Loss: 0.00013744577256573603\n",
      "Epoch 5208  \tTraining Loss: 0.00013514017226321005\tValidation Loss: 0.00013744201643970148\n",
      "Epoch 5209  \tTraining Loss: 0.00013513613710165425\tValidation Loss: 0.00013743805555238743\n",
      "Epoch 5210  \tTraining Loss: 0.00013513210281746496\tValidation Loss: 0.00013743421231329864\n",
      "Epoch 5211  \tTraining Loss: 0.00013512806973084592\tValidation Loss: 0.00013743019483831192\n",
      "Epoch 5212  \tTraining Loss: 0.00013512403764186066\tValidation Loss: 0.00013742634128591086\n",
      "Epoch 5213  \tTraining Loss: 0.0001351200079214196\tValidation Loss: 0.00013742251332503152\n",
      "Epoch 5214  \tTraining Loss: 0.00013511597854655918\tValidation Loss: 0.0001374185037220768\n",
      "Epoch 5215  \tTraining Loss: 0.00013511195060303207\tValidation Loss: 0.00013741475490320539\n",
      "Epoch 5216  \tTraining Loss: 0.00013510792547867986\tValidation Loss: 0.00013741078097238515\n",
      "Epoch 5217  \tTraining Loss: 0.0001351039003717589\tValidation Loss: 0.00013740681335765956\n",
      "Epoch 5218  \tTraining Loss: 0.00013509987634890625\tValidation Loss: 0.00013740303305464442\n",
      "Epoch 5219  \tTraining Loss: 0.00013509585374011897\tValidation Loss: 0.0001373991046353901\n",
      "Epoch 5220  \tTraining Loss: 0.00013509183251490865\tValidation Loss: 0.0001373952773618375\n",
      "Epoch 5221  \tTraining Loss: 0.00013508781251450063\tValidation Loss: 0.00013739127478028042\n",
      "Epoch 5222  \tTraining Loss: 0.0001350837936591608\tValidation Loss: 0.0001373874353205016\n",
      "Epoch 5223  \tTraining Loss: 0.00013507977677918638\tValidation Loss: 0.00013738362066161448\n",
      "Epoch 5224  \tTraining Loss: 0.00013507576046212547\tValidation Loss: 0.00013737962455616427\n",
      "Epoch 5225  \tTraining Loss: 0.00013507174554068107\tValidation Loss: 0.00013737595569351962\n",
      "Epoch 5226  \tTraining Loss: 0.00013506773337213627\tValidation Loss: 0.0001373718951293669\n",
      "Epoch 5227  \tTraining Loss: 0.00013506372032897834\tValidation Loss: 0.00013736798589147623\n",
      "Epoch 5228  \tTraining Loss: 0.00013505970948975726\tValidation Loss: 0.0001373642963651396\n",
      "Epoch 5229  \tTraining Loss: 0.00013505570052932243\tValidation Loss: 0.00013736025936453942\n",
      "Epoch 5230  \tTraining Loss: 0.00013505169166704468\tValidation Loss: 0.00013735646891185595\n",
      "Epoch 5231  \tTraining Loss: 0.00013504768434989808\tValidation Loss: 0.00013735247098728537\n",
      "Epoch 5232  \tTraining Loss: 0.00013504367804081672\tValidation Loss: 0.00013734862438057697\n",
      "Epoch 5233  \tTraining Loss: 0.00013503967296201735\tValidation Loss: 0.00013734481699802134\n",
      "Epoch 5234  \tTraining Loss: 0.00013503566924430427\tValidation Loss: 0.00013734085919064564\n",
      "Epoch 5235  \tTraining Loss: 0.00013503166718885416\tValidation Loss: 0.00013733714590141404\n",
      "Epoch 5236  \tTraining Loss: 0.00013502766647160793\tValidation Loss: 0.00013733310542763118\n",
      "Epoch 5237  \tTraining Loss: 0.0001350236660473559\tValidation Loss: 0.00013732920208511397\n",
      "Epoch 5238  \tTraining Loss: 0.0001350196677619886\tValidation Loss: 0.00013732551812310648\n",
      "Epoch 5239  \tTraining Loss: 0.00013501567098017395\tValidation Loss: 0.0001373214863252528\n",
      "Epoch 5240  \tTraining Loss: 0.00013501167446926287\tValidation Loss: 0.00013731772341024986\n",
      "Epoch 5241  \tTraining Loss: 0.00013500768018949113\tValidation Loss: 0.0001373138133788263\n",
      "Epoch 5242  \tTraining Loss: 0.00013500368701320172\tValidation Loss: 0.00013730987216791667\n",
      "Epoch 5243  \tTraining Loss: 0.00013499969478744295\tValidation Loss: 0.00013730615394511982\n",
      "Epoch 5244  \tTraining Loss: 0.00013499570447117237\tValidation Loss: 0.00013730223198292672\n",
      "Epoch 5245  \tTraining Loss: 0.00013499171506137648\tValidation Loss: 0.00013729829258853622\n",
      "Epoch 5246  \tTraining Loss: 0.00013498772681267744\tValidation Loss: 0.00013729454543577173\n",
      "Epoch 5247  \tTraining Loss: 0.00013498373968092532\tValidation Loss: 0.0001372906098606739\n",
      "Epoch 5248  \tTraining Loss: 0.0001349797548699798\tValidation Loss: 0.00013728691021460418\n",
      "Epoch 5249  \tTraining Loss: 0.0001349757704232485\tValidation Loss: 0.00013728289096913833\n",
      "Epoch 5250  \tTraining Loss: 0.00013497178673305176\tValidation Loss: 0.00013727900874603148\n",
      "Epoch 5251  \tTraining Loss: 0.000134967805488032\tValidation Loss: 0.00013727534459565853\n",
      "Epoch 5252  \tTraining Loss: 0.00013496382509198165\tValidation Loss: 0.00013727133186001983\n",
      "Epoch 5253  \tTraining Loss: 0.00013495984566988536\tValidation Loss: 0.0001372676190813637\n",
      "Epoch 5254  \tTraining Loss: 0.00013495586835296283\tValidation Loss: 0.0001372637091059715\n",
      "Epoch 5255  \tTraining Loss: 0.0001349518914862471\tValidation Loss: 0.00013725978268556085\n",
      "Epoch 5256  \tTraining Loss: 0.0001349479159742323\tValidation Loss: 0.000137256048723838\n",
      "Epoch 5257  \tTraining Loss: 0.00013494394191543562\tValidation Loss: 0.00013725216252955883\n",
      "Epoch 5258  \tTraining Loss: 0.0001349399692926399\tValidation Loss: 0.0001372483772208192\n",
      "Epoch 5259  \tTraining Loss: 0.0001349359976469064\tValidation Loss: 0.00013724441827345208\n",
      "Epoch 5260  \tTraining Loss: 0.0001349320270453847\tValidation Loss: 0.00013724062289910847\n",
      "Epoch 5261  \tTraining Loss: 0.00013492805866863699\tValidation Loss: 0.00013723685074186606\n",
      "Epoch 5262  \tTraining Loss: 0.00013492409062052616\tValidation Loss: 0.00013723289837439543\n",
      "Epoch 5263  \tTraining Loss: 0.00013492012406674818\tValidation Loss: 0.0001372292734271909\n",
      "Epoch 5264  \tTraining Loss: 0.00013491616002957532\tValidation Loss: 0.0001372252560333625\n",
      "Epoch 5265  \tTraining Loss: 0.00013491219539933614\tValidation Loss: 0.00013722141868795793\n",
      "Epoch 5266  \tTraining Loss: 0.0001349082330577857\tValidation Loss: 0.00013721774834364814\n",
      "Epoch 5267  \tTraining Loss: 0.0001349042719869534\tValidation Loss: 0.00013721375386026015\n",
      "Epoch 5268  \tTraining Loss: 0.00013490031129346347\tValidation Loss: 0.00013721002929805633\n",
      "Epoch 5269  \tTraining Loss: 0.00013489635258468673\tValidation Loss: 0.0001372060941137234\n",
      "Epoch 5270  \tTraining Loss: 0.0001348923950734558\tValidation Loss: 0.00013720231487418684\n",
      "Epoch 5271  \tTraining Loss: 0.00013488843890929506\tValidation Loss: 0.0001371985571144894\n",
      "Epoch 5272  \tTraining Loss: 0.00013488448364272788\tValidation Loss: 0.0001371946189753051\n",
      "Epoch 5273  \tTraining Loss: 0.0001348805299297276\tValidation Loss: 0.00013719100798584402\n",
      "Epoch 5274  \tTraining Loss: 0.00013487657837864318\tValidation Loss: 0.00013718700425791974\n",
      "Epoch 5275  \tTraining Loss: 0.00013487262621920393\tValidation Loss: 0.000137183151981539\n",
      "Epoch 5276  \tTraining Loss: 0.00013486867645806495\tValidation Loss: 0.00013717951924938586\n",
      "Epoch 5277  \tTraining Loss: 0.00013486472808840248\tValidation Loss: 0.00013717553916688268\n",
      "Epoch 5278  \tTraining Loss: 0.00013486077997571178\tValidation Loss: 0.0001371716943807254\n",
      "Epoch 5279  \tTraining Loss: 0.00013485683412121737\tValidation Loss: 0.0001371680678230761\n",
      "Epoch 5280  \tTraining Loss: 0.00013485288941812455\tValidation Loss: 0.00013716409294731386\n",
      "Epoch 5281  \tTraining Loss: 0.00013484894519032108\tValidation Loss: 0.00013716038600588098\n",
      "Epoch 5282  \tTraining Loss: 0.0001348450030879657\tValidation Loss: 0.00013715653331684065\n",
      "Epoch 5283  \tTraining Loss: 0.0001348410621070948\tValidation Loss: 0.0001371526765347371\n",
      "Epoch 5284  \tTraining Loss: 0.00013483712247241273\tValidation Loss: 0.00013714898999927895\n",
      "Epoch 5285  \tTraining Loss: 0.00013483318386532897\tValidation Loss: 0.00013714512377172806\n",
      "Epoch 5286  \tTraining Loss: 0.0001348292464056389\tValidation Loss: 0.0001371413732203193\n",
      "Epoch 5287  \tTraining Loss: 0.00013482531022359015\tValidation Loss: 0.00013713745072348372\n",
      "Epoch 5288  \tTraining Loss: 0.00013482137496688144\tValidation Loss: 0.00013713369262300333\n",
      "Epoch 5289  \tTraining Loss: 0.00013481744179964927\tValidation Loss: 0.00013712995684985135\n",
      "Epoch 5290  \tTraining Loss: 0.00013481350919482705\tValidation Loss: 0.00013712604202093215\n",
      "Epoch 5291  \tTraining Loss: 0.00013480957764054144\tValidation Loss: 0.0001371222896697768\n",
      "Epoch 5292  \tTraining Loss: 0.00013480564846779806\tValidation Loss: 0.00013711855883158513\n",
      "Epoch 5293  \tTraining Loss: 0.00013480171943450382\tValidation Loss: 0.0001371146486558461\n",
      "Epoch 5294  \tTraining Loss: 0.00013479779169610676\tValidation Loss: 0.00013711103369032265\n",
      "Epoch 5295  \tTraining Loss: 0.0001347938664212243\tValidation Loss: 0.00013710710896406233\n",
      "Epoch 5296  \tTraining Loss: 0.00013478994127663716\tValidation Loss: 0.00013710326922533746\n",
      "Epoch 5297  \tTraining Loss: 0.00013478601733966857\tValidation Loss: 0.00013709966322481503\n",
      "Epoch 5298  \tTraining Loss: 0.0001347820955393157\tValidation Loss: 0.0001370957102161853\n",
      "Epoch 5299  \tTraining Loss: 0.00013477817371591262\tValidation Loss: 0.0001370920257953785\n",
      "Epoch 5300  \tTraining Loss: 0.0001347742539520419\tValidation Loss: 0.0001370881319867548\n",
      "Epoch 5301  \tTraining Loss: 0.00013477033540280008\tValidation Loss: 0.0001370843945091679\n",
      "Epoch 5302  \tTraining Loss: 0.00013476641809534115\tValidation Loss: 0.00013708070584865322\n",
      "Epoch 5303  \tTraining Loss: 0.00013476250207604046\tValidation Loss: 0.00013707678519744748\n",
      "Epoch 5304  \tTraining Loss: 0.00013475858683133115\tValidation Loss: 0.00013707304989044535\n",
      "Epoch 5305  \tTraining Loss: 0.0001347546733482325\tValidation Loss: 0.00013706933662963565\n",
      "Epoch 5306  \tTraining Loss: 0.0001347507605222755\tValidation Loss: 0.00013706544410740438\n",
      "Epoch 5307  \tTraining Loss: 0.00013474684942591393\tValidation Loss: 0.0001370618787273974\n",
      "Epoch 5308  \tTraining Loss: 0.00013474294024210702\tValidation Loss: 0.00013705791996101508\n",
      "Epoch 5309  \tTraining Loss: 0.00013473903055567343\tValidation Loss: 0.00013705411277376817\n",
      "Epoch 5310  \tTraining Loss: 0.00013473512335882758\tValidation Loss: 0.00013705052502999157\n",
      "Epoch 5311  \tTraining Loss: 0.00013473121729749114\tValidation Loss: 0.00013704659003313483\n",
      "Epoch 5312  \tTraining Loss: 0.0001347273115814797\tValidation Loss: 0.00013704279034837727\n",
      "Epoch 5313  \tTraining Loss: 0.00013472340822054518\tValidation Loss: 0.0001370392087428715\n",
      "Epoch 5314  \tTraining Loss: 0.00013471950574426214\tValidation Loss: 0.00013703527890679095\n",
      "Epoch 5315  \tTraining Loss: 0.00013471560381212252\tValidation Loss: 0.00013703161644495028\n",
      "Epoch 5316  \tTraining Loss: 0.0001347117041821504\tValidation Loss: 0.00013702780953853914\n",
      "Epoch 5317  \tTraining Loss: 0.00013470780518523044\tValidation Loss: 0.00013702396891829594\n",
      "Epoch 5318  \tTraining Loss: 0.00013470390722087103\tValidation Loss: 0.00013702031878338837\n",
      "Epoch 5319  \tTraining Loss: 0.00013470001108107572\tValidation Loss: 0.00013701651799129054\n",
      "Epoch 5320  \tTraining Loss: 0.00013469611593380375\tValidation Loss: 0.00013701271456992729\n",
      "Epoch 5321  \tTraining Loss: 0.0001346922221418101\tValidation Loss: 0.00013700904968789882\n",
      "Epoch 5322  \tTraining Loss: 0.00013468832947113716\tValidation Loss: 0.00013700527823762022\n",
      "Epoch 5323  \tTraining Loss: 0.00013468443849381305\tValidation Loss: 0.00013700157682567277\n",
      "Epoch 5324  \tTraining Loss: 0.00013468054824934483\tValidation Loss: 0.0001369976776413915\n",
      "Epoch 5325  \tTraining Loss: 0.00013467665959549055\tValidation Loss: 0.0001369939904613333\n",
      "Epoch 5326  \tTraining Loss: 0.0001346727723669724\tValidation Loss: 0.0001369903012334231\n",
      "Epoch 5327  \tTraining Loss: 0.00013466888590321364\tValidation Loss: 0.00013698643071838353\n",
      "Epoch 5328  \tTraining Loss: 0.0001346650009139456\tValidation Loss: 0.00013698269825919912\n",
      "Epoch 5329  \tTraining Loss: 0.00013466111775572697\tValidation Loss: 0.00013697903589973402\n",
      "Epoch 5330  \tTraining Loss: 0.0001346572347622079\tValidation Loss: 0.00013697517341514117\n",
      "Epoch 5331  \tTraining Loss: 0.00013465335341596773\tValidation Loss: 0.00013697160249774484\n",
      "Epoch 5332  \tTraining Loss: 0.00013464947389718934\tValidation Loss: 0.00013696766501203894\n",
      "Epoch 5333  \tTraining Loss: 0.00013464559458072344\tValidation Loss: 0.00013696391380632868\n",
      "Epoch 5334  \tTraining Loss: 0.0001346417172199285\tValidation Loss: 0.00013696039118324762\n",
      "Epoch 5335  \tTraining Loss: 0.00013463784148987608\tValidation Loss: 0.00013695646526762505\n",
      "Epoch 5336  \tTraining Loss: 0.00013463396537675737\tValidation Loss: 0.00013695268947639072\n",
      "Epoch 5337  \tTraining Loss: 0.00013463009190352334\tValidation Loss: 0.00013694910833170204\n",
      "Epoch 5338  \tTraining Loss: 0.00013462621944150638\tValidation Loss: 0.00013694523016082735\n",
      "Epoch 5339  \tTraining Loss: 0.00013462234724741264\tValidation Loss: 0.00013694159723443204\n",
      "Epoch 5340  \tTraining Loss: 0.00013461847740405203\tValidation Loss: 0.00013693781771575368\n",
      "Epoch 5341  \tTraining Loss: 0.00013461460823608056\tValidation Loss: 0.00013693400347921067\n",
      "Epoch 5342  \tTraining Loss: 0.00013461074013233139\tValidation Loss: 0.00013693035511525492\n",
      "Epoch 5343  \tTraining Loss: 0.00013460687397957963\tValidation Loss: 0.000136926636124715\n",
      "Epoch 5344  \tTraining Loss: 0.00013460300875385014\tValidation Loss: 0.0001369228064160451\n",
      "Epoch 5345  \tTraining Loss: 0.00013459914400475558\tValidation Loss: 0.00013691918635576542\n",
      "Epoch 5346  \tTraining Loss: 0.0001345952811116488\tValidation Loss: 0.00013691541756660723\n",
      "Epoch 5347  \tTraining Loss: 0.0001345914193987808\tValidation Loss: 0.00013691172009959614\n",
      "Epoch 5348  \tTraining Loss: 0.0001345875588536171\tValidation Loss: 0.0001369079018445492\n",
      "Epoch 5349  \tTraining Loss: 0.00013458369924357932\tValidation Loss: 0.00013690422524531635\n",
      "Epoch 5350  \tTraining Loss: 0.0001345798413115459\tValidation Loss: 0.00013690059734344174\n",
      "Epoch 5351  \tTraining Loss: 0.00013457598450901644\tValidation Loss: 0.00013689673985516608\n",
      "Epoch 5352  \tTraining Loss: 0.00013457212848026187\tValidation Loss: 0.00013689303625570682\n",
      "Epoch 5353  \tTraining Loss: 0.00013456827433108495\tValidation Loss: 0.00013688940331110233\n",
      "Epoch 5354  \tTraining Loss: 0.00013456442062833673\tValidation Loss: 0.00013688557181546808\n",
      "Epoch 5355  \tTraining Loss: 0.00013456056838003855\tValidation Loss: 0.0001368819001468472\n",
      "Epoch 5356  \tTraining Loss: 0.00013455671779926637\tValidation Loss: 0.00013687824701472855\n",
      "Epoch 5357  \tTraining Loss: 0.0001345528676851431\tValidation Loss: 0.00013687439230916412\n",
      "Epoch 5358  \tTraining Loss: 0.000134549019325025\tValidation Loss: 0.000136870882722501\n",
      "Epoch 5359  \tTraining Loss: 0.00013454517238616363\tValidation Loss: 0.00013686700903408984\n",
      "Epoch 5360  \tTraining Loss: 0.00013454132557268774\tValidation Loss: 0.00013686326811867533\n",
      "Epoch 5361  \tTraining Loss: 0.00013453748088181901\tValidation Loss: 0.00013685974474271666\n",
      "Epoch 5362  \tTraining Loss: 0.00013453363746485444\tValidation Loss: 0.00013685587333059582\n",
      "Epoch 5363  \tTraining Loss: 0.0001345297943182371\tValidation Loss: 0.00013685211223606264\n",
      "Epoch 5364  \tTraining Loss: 0.00013452595351437795\tValidation Loss: 0.00013684861932200832\n",
      "Epoch 5365  \tTraining Loss: 0.00013452211350257795\tValidation Loss: 0.00013684475587532732\n",
      "Epoch 5366  \tTraining Loss: 0.0001345182739753643\tValidation Loss: 0.00013684105245372955\n",
      "Epoch 5367  \tTraining Loss: 0.00013451443709263622\tValidation Loss: 0.00013683751337894984\n",
      "Epoch 5368  \tTraining Loss: 0.000134510600326119\tValidation Loss: 0.00013683368215260458\n",
      "Epoch 5369  \tTraining Loss: 0.00013450676477687017\tValidation Loss: 0.00013683004170857385\n",
      "Epoch 5370  \tTraining Loss: 0.00013450293114985407\tValidation Loss: 0.00013682632514169587\n",
      "Epoch 5371  \tTraining Loss: 0.0001344990979127942\tValidation Loss: 0.00013682255194870103\n",
      "Epoch 5372  \tTraining Loss: 0.0001344952656980283\tValidation Loss: 0.00013681896616901803\n",
      "Epoch 5373  \tTraining Loss: 0.00013449143568845516\tValidation Loss: 0.00013681523209873784\n",
      "Epoch 5374  \tTraining Loss: 0.0001344876060596464\tValidation Loss: 0.0001368114614926429\n",
      "Epoch 5375  \tTraining Loss: 0.0001344837775880694\tValidation Loss: 0.00013680785594039527\n",
      "Epoch 5376  \tTraining Loss: 0.0001344799512267384\tValidation Loss: 0.00013680415281792612\n",
      "Epoch 5377  \tTraining Loss: 0.00013447612507972128\tValidation Loss: 0.0001368003903702745\n",
      "Epoch 5378  \tTraining Loss: 0.00013447230019373815\tValidation Loss: 0.0001367968140023914\n",
      "Epoch 5379  \tTraining Loss: 0.0001344684772167932\tValidation Loss: 0.00013679308888538282\n",
      "Epoch 5380  \tTraining Loss: 0.00013446465466924147\tValidation Loss: 0.00013678932667719295\n",
      "Epoch 5381  \tTraining Loss: 0.00013446083344512968\tValidation Loss: 0.00013678572918813232\n",
      "Epoch 5382  \tTraining Loss: 0.00013445701416197345\tValidation Loss: 0.00013678203428175664\n",
      "Epoch 5383  \tTraining Loss: 0.00013445319510293149\tValidation Loss: 0.00013677841115388166\n",
      "Epoch 5384  \tTraining Loss: 0.00013444937756056333\tValidation Loss: 0.0001367746806923444\n",
      "Epoch 5385  \tTraining Loss: 0.00013444556154232167\tValidation Loss: 0.0001367709215757517\n",
      "Epoch 5386  \tTraining Loss: 0.00013444174597361893\tValidation Loss: 0.00013676735339207728\n",
      "Epoch 5387  \tTraining Loss: 0.0001344379321327309\tValidation Loss: 0.00013676363793841033\n",
      "Epoch 5388  \tTraining Loss: 0.00013443411961868462\tValidation Loss: 0.00013675986079691175\n",
      "Epoch 5389  \tTraining Loss: 0.00013443030783205582\tValidation Loss: 0.00013675632405168035\n",
      "Epoch 5390  \tTraining Loss: 0.00013442649751769348\tValidation Loss: 0.0001367526503004448\n",
      "Epoch 5391  \tTraining Loss: 0.00013442268885944557\tValidation Loss: 0.00013674891164636158\n",
      "Epoch 5392  \tTraining Loss: 0.00013441888059251156\tValidation Loss: 0.00013674532497681127\n",
      "Epoch 5393  \tTraining Loss: 0.00013441507354237443\tValidation Loss: 0.00013674161496414892\n",
      "Epoch 5394  \tTraining Loss: 0.0001344112679076929\tValidation Loss: 0.0001367378698024134\n",
      "Epoch 5395  \tTraining Loss: 0.00013440746323282606\tValidation Loss: 0.0001367342894438799\n",
      "Epoch 5396  \tTraining Loss: 0.0001344036601183269\tValidation Loss: 0.00013673061255916526\n",
      "Epoch 5397  \tTraining Loss: 0.00013439985791016627\tValidation Loss: 0.00013672687557244654\n",
      "Epoch 5398  \tTraining Loss: 0.0001343960567584142\tValidation Loss: 0.00013672332446101814\n",
      "Epoch 5399  \tTraining Loss: 0.0001343922570539097\tValidation Loss: 0.00013671962544637483\n",
      "Epoch 5400  \tTraining Loss: 0.00013438845841532855\tValidation Loss: 0.00013671601984367458\n",
      "Epoch 5401  \tTraining Loss: 0.00013438466086468433\tValidation Loss: 0.00013671224493742618\n",
      "Epoch 5402  \tTraining Loss: 0.0001343808645684391\tValidation Loss: 0.0001367086094952274\n",
      "Epoch 5403  \tTraining Loss: 0.00013437706968619624\tValidation Loss: 0.00013670504216049546\n",
      "Epoch 5404  \tTraining Loss: 0.0001343732755228719\tValidation Loss: 0.00013670127683178512\n",
      "Epoch 5405  \tTraining Loss: 0.00013436948277268436\tValidation Loss: 0.00013669767111717287\n",
      "Epoch 5406  \tTraining Loss: 0.00013436569124792918\tValidation Loss: 0.0001366940815384388\n",
      "Epoch 5407  \tTraining Loss: 0.00013436190060292985\tValidation Loss: 0.00013669031649179457\n",
      "Epoch 5408  \tTraining Loss: 0.00013435811144549278\tValidation Loss: 0.00013668671438362594\n",
      "Epoch 5409  \tTraining Loss: 0.00013435432340417624\tValidation Loss: 0.0001366831286791875\n",
      "Epoch 5410  \tTraining Loss: 0.00013435053628457896\tValidation Loss: 0.00013667934279667048\n",
      "Epoch 5411  \tTraining Loss: 0.00013434675081337057\tValidation Loss: 0.00013667577097694158\n",
      "Epoch 5412  \tTraining Loss: 0.00013434296616080793\tValidation Loss: 0.00013667219272742512\n",
      "Epoch 5413  \tTraining Loss: 0.0001343391824847467\tValidation Loss: 0.00013666850212537508\n",
      "Epoch 5414  \tTraining Loss: 0.00013433540051324488\tValidation Loss: 0.00013666478064928989\n",
      "Epoch 5415  \tTraining Loss: 0.0001343316191487085\tValidation Loss: 0.0001366612493680623\n",
      "Epoch 5416  \tTraining Loss: 0.00013432783925419568\tValidation Loss: 0.0001366576053381882\n",
      "Epoch 5417  \tTraining Loss: 0.00013432406095357718\tValidation Loss: 0.00013665387006089657\n",
      "Epoch 5418  \tTraining Loss: 0.000134320283015191\tValidation Loss: 0.00013665034366339363\n",
      "Epoch 5419  \tTraining Loss: 0.0001343165070958481\tValidation Loss: 0.00013664667051065091\n",
      "Epoch 5420  \tTraining Loss: 0.00013431273144199726\tValidation Loss: 0.0001366429603644597\n",
      "Epoch 5421  \tTraining Loss: 0.00013430895711498192\tValidation Loss: 0.0001366394376634886\n",
      "Epoch 5422  \tTraining Loss: 0.0001343051842654369\tValidation Loss: 0.00013663576802987645\n",
      "Epoch 5423  \tTraining Loss: 0.00013430141241194243\tValidation Loss: 0.00013663219068686916\n",
      "Epoch 5424  \tTraining Loss: 0.0001342976416922861\tValidation Loss: 0.00013662844480672294\n",
      "Epoch 5425  \tTraining Loss: 0.00013429387219584122\tValidation Loss: 0.0001366248633516444\n",
      "Epoch 5426  \tTraining Loss: 0.00013429010389532452\tValidation Loss: 0.00013662127303305482\n",
      "Epoch 5427  \tTraining Loss: 0.00013428633671599567\tValidation Loss: 0.00013661755956082694\n",
      "Epoch 5428  \tTraining Loss: 0.0001342825707310714\tValidation Loss: 0.0001366139864281861\n",
      "Epoch 5429  \tTraining Loss: 0.00013427880578438176\tValidation Loss: 0.0001366104254879108\n",
      "Epoch 5430  \tTraining Loss: 0.0001342750419307928\tValidation Loss: 0.00013660668952807133\n",
      "Epoch 5431  \tTraining Loss: 0.00013427127957133314\tValidation Loss: 0.00013660311655183915\n",
      "Epoch 5432  \tTraining Loss: 0.0001342675180122478\tValidation Loss: 0.00013659955900155538\n",
      "Epoch 5433  \tTraining Loss: 0.00013426375769376995\tValidation Loss: 0.00013659589348457825\n",
      "Epoch 5434  \tTraining Loss: 0.00013425999884330146\tValidation Loss: 0.0001365921969370425\n",
      "Epoch 5435  \tTraining Loss: 0.00013425624048546409\tValidation Loss: 0.0001365886653886006\n",
      "Epoch 5436  \tTraining Loss: 0.00013425248401111254\tValidation Loss: 0.00013658503980203653\n",
      "Epoch 5437  \tTraining Loss: 0.00013424872840262658\tValidation Loss: 0.0001365813527162065\n",
      "Epoch 5438  \tTraining Loss: 0.00013424497347753849\tValidation Loss: 0.0001365778510506607\n",
      "Epoch 5439  \tTraining Loss: 0.00013424122043818803\tValidation Loss: 0.00013657420316751494\n",
      "Epoch 5440  \tTraining Loss: 0.0001342374682192444\tValidation Loss: 0.00013657051630727927\n",
      "Epoch 5441  \tTraining Loss: 0.00013423371675769688\tValidation Loss: 0.00013656701812991888\n",
      "Epoch 5442  \tTraining Loss: 0.00013422996726903275\tValidation Loss: 0.00013656337420843114\n",
      "Epoch 5443  \tTraining Loss: 0.00013422621836871548\tValidation Loss: 0.0001365596912520942\n",
      "Epoch 5444  \tTraining Loss: 0.00013422247035614556\tValidation Loss: 0.00013655619689655413\n",
      "Epoch 5445  \tTraining Loss: 0.00013421872449037058\tValidation Loss: 0.00013655253179680894\n",
      "Epoch 5446  \tTraining Loss: 0.00013421497897448545\tValidation Loss: 0.00013654887912106068\n",
      "Epoch 5447  \tTraining Loss: 0.00013421123435321798\tValidation Loss: 0.0001365454588017465\n",
      "Epoch 5448  \tTraining Loss: 0.00013420749213430985\tValidation Loss: 0.000136541723690209\n",
      "Epoch 5449  \tTraining Loss: 0.00013420374989995458\tValidation Loss: 0.00013653807126093044\n",
      "Epoch 5450  \tTraining Loss: 0.00013420000883541123\tValidation Loss: 0.0001365346593100032\n",
      "Epoch 5451  \tTraining Loss: 0.00013419626975556684\tValidation Loss: 0.00013653090113463506\n",
      "Epoch 5452  \tTraining Loss: 0.00013419253066676532\tValidation Loss: 0.00013652731152169817\n",
      "Epoch 5453  \tTraining Loss: 0.0001341887936957705\tValidation Loss: 0.00013652388510316166\n",
      "Epoch 5454  \tTraining Loss: 0.0001341850576658066\tValidation Loss: 0.0001365201268720078\n",
      "Epoch 5455  \tTraining Loss: 0.00013418132196910641\tValidation Loss: 0.00013651647966735127\n",
      "Epoch 5456  \tTraining Loss: 0.00013417758845058763\tValidation Loss: 0.00013651310216243276\n",
      "Epoch 5457  \tTraining Loss: 0.00013417385583216148\tValidation Loss: 0.00013650935439698403\n",
      "Epoch 5458  \tTraining Loss: 0.00013417012349753365\tValidation Loss: 0.00013650573816604896\n",
      "Epoch 5459  \tTraining Loss: 0.00013416639341368308\tValidation Loss: 0.0001365023386488583\n",
      "Epoch 5460  \tTraining Loss: 0.00013416266414569927\tValidation Loss: 0.00013649859137274483\n",
      "Epoch 5461  \tTraining Loss: 0.00013415893528276112\tValidation Loss: 0.00013649504569244168\n",
      "Epoch 5462  \tTraining Loss: 0.00013415520875036818\tValidation Loss: 0.0001364915219769739\n",
      "Epoch 5463  \tTraining Loss: 0.00013415148251253825\tValidation Loss: 0.00013648782606974112\n",
      "Epoch 5464  \tTraining Loss: 0.00013414775740445495\tValidation Loss: 0.00013648429390983096\n",
      "Epoch 5465  \tTraining Loss: 0.00013414403406508297\tValidation Loss: 0.00013648075080377242\n",
      "Epoch 5466  \tTraining Loss: 0.00013414031136779736\tValidation Loss: 0.00013647708624613483\n",
      "Epoch 5467  \tTraining Loss: 0.00013413658974917127\tValidation Loss: 0.00013647356195957376\n",
      "Epoch 5468  \tTraining Loss: 0.0001341328696234221\tValidation Loss: 0.00013647004818943473\n",
      "Epoch 5469  \tTraining Loss: 0.00013412915020912585\tValidation Loss: 0.00013646636071554784\n",
      "Epoch 5470  \tTraining Loss: 0.00013412543219458667\tValidation Loss: 0.00013646283643279887\n",
      "Epoch 5471  \tTraining Loss: 0.0001341217152815525\tValidation Loss: 0.00013645932592877975\n",
      "Epoch 5472  \tTraining Loss: 0.0001341179992703895\tValidation Loss: 0.00013645564223074678\n",
      "Epoch 5473  \tTraining Loss: 0.0001341142848018695\tValidation Loss: 0.0001364521217747788\n",
      "Epoch 5474  \tTraining Loss: 0.00013411057108634626\tValidation Loss: 0.00013644861495996506\n",
      "Epoch 5475  \tTraining Loss: 0.00013410685862791448\tValidation Loss: 0.00013644500208166398\n",
      "Epoch 5476  \tTraining Loss: 0.00013410314754508867\tValidation Loss: 0.00013644135691268458\n",
      "Epoch 5477  \tTraining Loss: 0.00013409943685974507\tValidation Loss: 0.00013643790121978044\n",
      "Epoch 5478  \tTraining Loss: 0.00013409572821532727\tValidation Loss: 0.00013643427644564676\n",
      "Epoch 5479  \tTraining Loss: 0.0001340920203652448\tValidation Loss: 0.00013643066368484817\n",
      "Epoch 5480  \tTraining Loss: 0.0001340883129609234\tValidation Loss: 0.00013642721610215315\n",
      "Epoch 5481  \tTraining Loss: 0.0001340846078595714\tValidation Loss: 0.00013642364980144135\n",
      "Epoch 5482  \tTraining Loss: 0.00013408090350411597\tValidation Loss: 0.00013641999000195382\n",
      "Epoch 5483  \tTraining Loss: 0.00013407719922109958\tValidation Loss: 0.00013641654124940078\n",
      "Epoch 5484  \tTraining Loss: 0.00013407349760739205\tValidation Loss: 0.00013641294987621713\n",
      "Epoch 5485  \tTraining Loss: 0.00013406979609315004\tValidation Loss: 0.0001364093179961409\n",
      "Epoch 5486  \tTraining Loss: 0.00013406609555741746\tValidation Loss: 0.00013640581157422339\n",
      "Epoch 5487  \tTraining Loss: 0.00013406239730641005\tValidation Loss: 0.0001364023199317973\n",
      "Epoch 5488  \tTraining Loss: 0.00013405869898931169\tValidation Loss: 0.00013639865554266245\n",
      "Epoch 5489  \tTraining Loss: 0.00013405500208162104\tValidation Loss: 0.0001363951546191036\n",
      "Epoch 5490  \tTraining Loss: 0.00013405130691937573\tValidation Loss: 0.0001363916667209848\n",
      "Epoch 5491  \tTraining Loss: 0.0001340476120373086\tValidation Loss: 0.00013638798094265997\n",
      "Epoch 5492  \tTraining Loss: 0.0001340439187634525\tValidation Loss: 0.00013638451063185988\n",
      "Epoch 5493  \tTraining Loss: 0.0001340402266314714\tValidation Loss: 0.00013638102999650943\n",
      "Epoch 5494  \tTraining Loss: 0.00013403653511221536\tValidation Loss: 0.00013637737346266102\n",
      "Epoch 5495  \tTraining Loss: 0.00013403284526564328\tValidation Loss: 0.00013637387987060022\n",
      "Epoch 5496  \tTraining Loss: 0.00013402915631515334\tValidation Loss: 0.0001363703989656466\n",
      "Epoch 5497  \tTraining Loss: 0.00013402546834021445\tValidation Loss: 0.00013636681301782268\n",
      "Epoch 5498  \tTraining Loss: 0.00013402178160002947\tValidation Loss: 0.00013636322257972326\n",
      "Epoch 5499  \tTraining Loss: 0.00013401809544889384\tValidation Loss: 0.00013635979732676655\n",
      "Epoch 5500  \tTraining Loss: 0.00013401441057956355\tValidation Loss: 0.0001363561563001033\n",
      "Epoch 5501  \tTraining Loss: 0.0001340107264292395\tValidation Loss: 0.00013635260198185557\n",
      "Epoch 5502  \tTraining Loss: 0.00013400704364343916\tValidation Loss: 0.00013634918711077774\n",
      "Epoch 5503  \tTraining Loss: 0.00013400336213857726\tValidation Loss: 0.00013634555140625864\n",
      "Epoch 5504  \tTraining Loss: 0.00013399968132010669\tValidation Loss: 0.00013634200150274377\n",
      "Epoch 5505  \tTraining Loss: 0.00013399600181332737\tValidation Loss: 0.00013633859062671153\n",
      "Epoch 5506  \tTraining Loss: 0.00013399232370917913\tValidation Loss: 0.0001363349334029736\n",
      "Epoch 5507  \tTraining Loss: 0.00013398864630700353\tValidation Loss: 0.00013633141411300922\n",
      "Epoch 5508  \tTraining Loss: 0.00013398496998991372\tValidation Loss: 0.00013632801043548707\n",
      "Epoch 5509  \tTraining Loss: 0.00013398129523751833\tValidation Loss: 0.00013632438245982342\n",
      "Epoch 5510  \tTraining Loss: 0.00013397762105853072\tValidation Loss: 0.00013632083969227036\n",
      "Epoch 5511  \tTraining Loss: 0.00013397394813686976\tValidation Loss: 0.0001363173740402234\n",
      "Epoch 5512  \tTraining Loss: 0.0001339702768317857\tValidation Loss: 0.00013631391167598969\n",
      "Epoch 5513  \tTraining Loss: 0.00013396660604442123\tValidation Loss: 0.00013631027511563996\n",
      "Epoch 5514  \tTraining Loss: 0.00013396293626248698\tValidation Loss: 0.00013630673352571567\n",
      "Epoch 5515  \tTraining Loss: 0.00013395926801052207\tValidation Loss: 0.0001363033326071027\n",
      "Epoch 5516  \tTraining Loss: 0.00013395560071360972\tValidation Loss: 0.00013629971175840936\n",
      "Epoch 5517  \tTraining Loss: 0.00013395193425732446\tValidation Loss: 0.00013629617692389006\n",
      "Epoch 5518  \tTraining Loss: 0.0001339482692497309\tValidation Loss: 0.00013629278107779783\n",
      "Epoch 5519  \tTraining Loss: 0.0001339446053145924\tValidation Loss: 0.00013628916472818896\n",
      "Epoch 5520  \tTraining Loss: 0.00013394094215905133\tValidation Loss: 0.0001362856340065943\n",
      "Epoch 5521  \tTraining Loss: 0.00013393728044436894\tValidation Loss: 0.00013628227086551583\n",
      "Epoch 5522  \tTraining Loss: 0.0001339336202121146\tValidation Loss: 0.00013627863418908548\n",
      "Epoch 5523  \tTraining Loss: 0.0001339299600301248\tValidation Loss: 0.00013627510594882729\n",
      "Epoch 5524  \tTraining Loss: 0.00013392630150940902\tValidation Loss: 0.0001362716922255653\n",
      "Epoch 5525  \tTraining Loss: 0.00013392264435028577\tValidation Loss: 0.0001362681103014734\n",
      "Epoch 5526  \tTraining Loss: 0.00013391898772001883\tValidation Loss: 0.0001362645902967841\n",
      "Epoch 5527  \tTraining Loss: 0.00013391533237940045\tValidation Loss: 0.00013626120534752464\n",
      "Epoch 5528  \tTraining Loss: 0.00013391167851347372\tValidation Loss: 0.0001362575995773563\n",
      "Epoch 5529  \tTraining Loss: 0.0001339080252211234\tValidation Loss: 0.00013625407921213803\n",
      "Epoch 5530  \tTraining Loss: 0.00013390437321700758\tValidation Loss: 0.00013625063618477387\n",
      "Epoch 5531  \tTraining Loss: 0.0001339007227144555\tValidation Loss: 0.00013624719576220144\n",
      "Epoch 5532  \tTraining Loss: 0.00013389707278924903\tValidation Loss: 0.00013624358155414352\n",
      "Epoch 5533  \tTraining Loss: 0.0001338934237454924\tValidation Loss: 0.00013624006320598158\n",
      "Epoch 5534  \tTraining Loss: 0.0001338897761705203\tValidation Loss: 0.00013623668490890948\n",
      "Epoch 5535  \tTraining Loss: 0.00013388612962844762\tValidation Loss: 0.00013623308710042535\n",
      "Epoch 5536  \tTraining Loss: 0.00013388248490259243\tValidation Loss: 0.00013622971000839564\n",
      "Epoch 5537  \tTraining Loss: 0.00013387884238453881\tValidation Loss: 0.00013622621485904154\n",
      "Epoch 5538  \tTraining Loss: 0.00013387519870574062\tValidation Loss: 0.000136222601199206\n",
      "Epoch 5539  \tTraining Loss: 0.00013387155802586413\tValidation Loss: 0.0001362192239770813\n",
      "Epoch 5540  \tTraining Loss: 0.00013386791798921497\tValidation Loss: 0.00013621573240530288\n",
      "Epoch 5541  \tTraining Loss: 0.00013386427767648825\tValidation Loss: 0.00013621225804252176\n",
      "Epoch 5542  \tTraining Loss: 0.00013386064132023332\tValidation Loss: 0.0001362086287842158\n",
      "Epoch 5543  \tTraining Loss: 0.00013385700293575272\tValidation Loss: 0.00013620524302735163\n",
      "Epoch 5544  \tTraining Loss: 0.0001338533675059004\tValidation Loss: 0.000136201788924937\n",
      "Epoch 5545  \tTraining Loss: 0.00013384973321243083\tValidation Loss: 0.00013619816769789618\n",
      "Epoch 5546  \tTraining Loss: 0.0001338460980663521\tValidation Loss: 0.00013619478683513027\n",
      "Epoch 5547  \tTraining Loss: 0.0001338424670289707\tValidation Loss: 0.0001361913367297365\n",
      "Epoch 5548  \tTraining Loss: 0.0001338388348951928\tValidation Loss: 0.0001361877192977278\n",
      "Epoch 5549  \tTraining Loss: 0.00013383520389473563\tValidation Loss: 0.00013618454471260352\n",
      "Epoch 5550  \tTraining Loss: 0.00013383157616116348\tValidation Loss: 0.00013618077983072142\n",
      "Epoch 5551  \tTraining Loss: 0.0001338279459370972\tValidation Loss: 0.00013617725344887443\n",
      "Epoch 5552  \tTraining Loss: 0.00013382431992887137\tValidation Loss: 0.00013617402628499122\n",
      "Epoch 5553  \tTraining Loss: 0.00013382069391517902\tValidation Loss: 0.00013617032911678503\n",
      "Epoch 5554  \tTraining Loss: 0.00013381706786884278\tValidation Loss: 0.00013616695470687723\n",
      "Epoch 5555  \tTraining Loss: 0.00013381344513973322\tValidation Loss: 0.00013616347883598558\n",
      "Epoch 5556  \tTraining Loss: 0.00013380982097309724\tValidation Loss: 0.0001361598885095214\n",
      "Epoch 5557  \tTraining Loss: 0.00013380620020959796\tValidation Loss: 0.00013615653469982678\n",
      "Epoch 5558  \tTraining Loss: 0.0001338025791564058\tValidation Loss: 0.00013615306694446638\n",
      "Epoch 5559  \tTraining Loss: 0.00013379895915843808\tValidation Loss: 0.00013614961596131696\n",
      "Epoch 5560  \tTraining Loss: 0.00013379534160198737\tValidation Loss: 0.00013614601042340958\n",
      "Epoch 5561  \tTraining Loss: 0.0001337917224557925\tValidation Loss: 0.0001361426475827068\n",
      "Epoch 5562  \tTraining Loss: 0.0001337881077439079\tValidation Loss: 0.0001361392161621813\n",
      "Epoch 5563  \tTraining Loss: 0.0001337844918064678\tValidation Loss: 0.0001361356181146441\n",
      "Epoch 5564  \tTraining Loss: 0.00013378087704999031\tValidation Loss: 0.0001361322655303511\n",
      "Epoch 5565  \tTraining Loss: 0.0001337772649779305\tValidation Loss: 0.00013612880520449212\n",
      "Epoch 5566  \tTraining Loss: 0.0001337736513026314\tValidation Loss: 0.0001361252288409845\n",
      "Epoch 5567  \tTraining Loss: 0.000133770041835153\tValidation Loss: 0.00013612188796796555\n",
      "Epoch 5568  \tTraining Loss: 0.00013376643123096408\tValidation Loss: 0.00013611850131752226\n",
      "Epoch 5569  \tTraining Loss: 0.00013376282269799282\tValidation Loss: 0.00013611500037272886\n",
      "Epoch 5570  \tTraining Loss: 0.00013375921503684757\tValidation Loss: 0.00013611139344398796\n",
      "Epoch 5571  \tTraining Loss: 0.0001337556072459399\tValidation Loss: 0.00013610817270516353\n",
      "Epoch 5572  \tTraining Loss: 0.00013375200337763918\tValidation Loss: 0.00013610449611514932\n",
      "Epoch 5573  \tTraining Loss: 0.00013374839685597126\tValidation Loss: 0.00013610101042100162\n",
      "Epoch 5574  \tTraining Loss: 0.00013374479456557223\tValidation Loss: 0.00013609781344810286\n",
      "Epoch 5575  \tTraining Loss: 0.00013374119211516944\tValidation Loss: 0.0001360941461992202\n",
      "Epoch 5576  \tTraining Loss: 0.00013373758996075121\tValidation Loss: 0.00013609079978278707\n",
      "Epoch 5577  \tTraining Loss: 0.00013373399059795433\tValidation Loss: 0.00013608735194439207\n",
      "Epoch 5578  \tTraining Loss: 0.00013373039000635936\tValidation Loss: 0.0001360839233701085\n",
      "Epoch 5579  \tTraining Loss: 0.00013372679362423423\tValidation Loss: 0.00013608034166483443\n",
      "Epoch 5580  \tTraining Loss: 0.0001337231948026386\tValidation Loss: 0.00013607700218474313\n",
      "Epoch 5581  \tTraining Loss: 0.00013371959988525738\tValidation Loss: 0.0001360735939413131\n",
      "Epoch 5582  \tTraining Loss: 0.00013371600472324104\tValidation Loss: 0.00013607001974976415\n",
      "Epoch 5583  \tTraining Loss: 0.00013371241002325309\tValidation Loss: 0.00013606669008932746\n",
      "Epoch 5584  \tTraining Loss: 0.0001337088184986895\tValidation Loss: 0.00013606325313763535\n",
      "Epoch 5585  \tTraining Loss: 0.00013370522503185743\tValidation Loss: 0.00013605983389253893\n",
      "Epoch 5586  \tTraining Loss: 0.0001337016361981234\tValidation Loss: 0.00013605632977283118\n",
      "Epoch 5587  \tTraining Loss: 0.00013369804528539004\tValidation Loss: 0.0001360529364037602\n",
      "Epoch 5588  \tTraining Loss: 0.00013369445740579617\tValidation Loss: 0.00013604952252220222\n",
      "Epoch 5589  \tTraining Loss: 0.00013369086969498286\tValidation Loss: 0.00013604595276963871\n",
      "Epoch 5590  \tTraining Loss: 0.00013368728265086995\tValidation Loss: 0.00013604275733177965\n",
      "Epoch 5591  \tTraining Loss: 0.00013368369848699727\tValidation Loss: 0.00013603910538994231\n",
      "Epoch 5592  \tTraining Loss: 0.00013368011230558988\tValidation Loss: 0.00013603577660544568\n",
      "Epoch 5593  \tTraining Loss: 0.0001336765309426288\tValidation Loss: 0.0001360323477939504\n",
      "Epoch 5594  \tTraining Loss: 0.00013367294715101928\tValidation Loss: 0.00013602880497529162\n",
      "Epoch 5595  \tTraining Loss: 0.00013366936688609217\tValidation Loss: 0.00013602549691733385\n",
      "Epoch 5596  \tTraining Loss: 0.00013366578640405704\tValidation Loss: 0.0001360220758193079\n",
      "Epoch 5597  \tTraining Loss: 0.00013366220725175525\tValidation Loss: 0.00013601867074698097\n",
      "Epoch 5598  \tTraining Loss: 0.00013365862980202793\tValidation Loss: 0.00013601511235295974\n",
      "Epoch 5599  \tTraining Loss: 0.00013365505154583476\tValidation Loss: 0.00013601180061656763\n",
      "Epoch 5600  \tTraining Loss: 0.0001336514773734708\tValidation Loss: 0.0001360083828215271\n",
      "Epoch 5601  \tTraining Loss: 0.00013364790077162857\tValidation Loss: 0.0001360048495727\n",
      "Epoch 5602  \tTraining Loss: 0.00013364432833949715\tValidation Loss: 0.00013600155027226006\n",
      "Epoch 5603  \tTraining Loss: 0.000133640754977959\tValidation Loss: 0.00013599820648912852\n",
      "Epoch 5604  \tTraining Loss: 0.000133637183822269\tValidation Loss: 0.00013599474704801404\n",
      "Epoch 5605  \tTraining Loss: 0.00013363361304639118\tValidation Loss: 0.0001359911830576359\n",
      "Epoch 5606  \tTraining Loss: 0.00013363004279998936\tValidation Loss: 0.00013598800303794335\n",
      "Epoch 5607  \tTraining Loss: 0.00013362647551433645\tValidation Loss: 0.0001359843695854069\n",
      "Epoch 5608  \tTraining Loss: 0.00013362290636104092\tValidation Loss: 0.00013598105905697003\n",
      "Epoch 5609  \tTraining Loss: 0.00013361934156491616\tValidation Loss: 0.00013597764911433023\n",
      "Epoch 5610  \tTraining Loss: 0.0001336157746387629\tValidation Loss: 0.0001359741254618335\n",
      "Epoch 5611  \tTraining Loss: 0.0001336122115876806\tValidation Loss: 0.00013597083590230241\n",
      "Epoch 5612  \tTraining Loss: 0.0001336086475071492\tValidation Loss: 0.00013596730614141893\n",
      "Epoch 5613  \tTraining Loss: 0.00013360508556526626\tValidation Loss: 0.0001359641413946776\n",
      "Epoch 5614  \tTraining Loss: 0.0001336015246768577\tValidation Loss: 0.0001359605195487025\n",
      "Epoch 5615  \tTraining Loss: 0.00013359796368236576\tValidation Loss: 0.00013595721876413577\n",
      "Epoch 5616  \tTraining Loss: 0.00013359440549541097\tValidation Loss: 0.0001359538179936083\n",
      "Epoch 5617  \tTraining Loss: 0.00013359084632937125\tValidation Loss: 0.00013595043576577914\n",
      "Epoch 5618  \tTraining Loss: 0.00013358729065791053\tValidation Loss: 0.00013594690168294954\n",
      "Epoch 5619  \tTraining Loss: 0.00013358373285424327\tValidation Loss: 0.00013594367721459768\n",
      "Epoch 5620  \tTraining Loss: 0.00013358018017546567\tValidation Loss: 0.00013594025181498532\n",
      "Epoch 5621  \tTraining Loss: 0.00013357662496086737\tValidation Loss: 0.00013593671063475007\n",
      "Epoch 5622  \tTraining Loss: 0.00013357307243493896\tValidation Loss: 0.00013593342205077844\n",
      "Epoch 5623  \tTraining Loss: 0.00013356952069429307\tValidation Loss: 0.00013593002926829233\n",
      "Epoch 5624  \tTraining Loss: 0.00013356596969043378\tValidation Loss: 0.00013592665442270792\n",
      "Epoch 5625  \tTraining Loss: 0.00013356242030109373\tValidation Loss: 0.0001359231281326431\n",
      "Epoch 5626  \tTraining Loss: 0.00013355887090991483\tValidation Loss: 0.00013591997500767464\n",
      "Epoch 5627  \tTraining Loss: 0.00013355532450993825\tValidation Loss: 0.00013591636791899408\n",
      "Epoch 5628  \tTraining Loss: 0.0001335517762758109\tValidation Loss: 0.00013591308178447295\n",
      "Epoch 5629  \tTraining Loss: 0.00013354823229399018\tValidation Loss: 0.00013590969622644778\n",
      "Epoch 5630  \tTraining Loss: 0.00013354468611988802\tValidation Loss: 0.00013590619685368224\n",
      "Epoch 5631  \tTraining Loss: 0.00013354114418479872\tValidation Loss: 0.0001359029304608805\n",
      "Epoch 5632  \tTraining Loss: 0.0001335376006255454\tValidation Loss: 0.00013589942461772402\n",
      "Epoch 5633  \tTraining Loss: 0.00013353405991549158\tValidation Loss: 0.00013589628246184999\n",
      "Epoch 5634  \tTraining Loss: 0.00013353051998676743\tValidation Loss: 0.00013589273714420479\n",
      "Epoch 5635  \tTraining Loss: 0.0001335269812906111\tValidation Loss: 0.0001358893942797817\n",
      "Epoch 5636  \tTraining Loss: 0.00013352344286850308\tValidation Loss: 0.00013588600338211552\n",
      "Epoch 5637  \tTraining Loss: 0.0001335199074782051\tValidation Loss: 0.00013588264091415328\n",
      "Epoch 5638  \tTraining Loss: 0.00013351637014388456\tValidation Loss: 0.00013587912958221642\n",
      "Epoch 5639  \tTraining Loss: 0.00013351283762538056\tValidation Loss: 0.00013587599104877599\n",
      "Epoch 5640  \tTraining Loss: 0.00013350930274578637\tValidation Loss: 0.0001358725318022325\n",
      "Epoch 5641  \tTraining Loss: 0.00013350577087007314\tValidation Loss: 0.00013586900932966028\n",
      "Epoch 5642  \tTraining Loss: 0.0001335022394973701\tValidation Loss: 0.00013586574431423464\n",
      "Epoch 5643  \tTraining Loss: 0.00013349870836224567\tValidation Loss: 0.00013586237756965989\n",
      "Epoch 5644  \tTraining Loss: 0.00013349518038833187\tValidation Loss: 0.00013585902822604918\n",
      "Epoch 5645  \tTraining Loss: 0.00013349165023915873\tValidation Loss: 0.00013585565961463333\n",
      "Epoch 5646  \tTraining Loss: 0.00013348812438062637\tValidation Loss: 0.00013585227916505742\n",
      "Epoch 5647  \tTraining Loss: 0.0001334845977881739\tValidation Loss: 0.00013584892911848136\n",
      "Epoch 5648  \tTraining Loss: 0.00013348107217014618\tValidation Loss: 0.00013584550041528896\n",
      "Epoch 5649  \tTraining Loss: 0.00013347754890440306\tValidation Loss: 0.00013584218424491742\n",
      "Epoch 5650  \tTraining Loss: 0.00013347402433482483\tValidation Loss: 0.00013583881249905582\n",
      "Epoch 5651  \tTraining Loss: 0.0001334705039392472\tValidation Loss: 0.00013583546705956323\n",
      "Epoch 5652  \tTraining Loss: 0.00013346698098469837\tValidation Loss: 0.00013583210420648527\n",
      "Epoch 5653  \tTraining Loss: 0.0001334634619175815\tValidation Loss: 0.00013582873050836628\n",
      "Epoch 5654  \tTraining Loss: 0.00013345994290978521\tValidation Loss: 0.0001358253875698725\n",
      "Epoch 5655  \tTraining Loss: 0.00013345642394039492\tValidation Loss: 0.00013582189744147278\n",
      "Epoch 5656  \tTraining Loss: 0.0001334529080613407\tValidation Loss: 0.00013581877897474244\n",
      "Epoch 5657  \tTraining Loss: 0.00013344939071328914\tValidation Loss: 0.00013581534024654975\n",
      "Epoch 5658  \tTraining Loss: 0.00013344587696559595\tValidation Loss: 0.0001358118385112559\n",
      "Epoch 5659  \tTraining Loss: 0.00013344236240461123\tValidation Loss: 0.00013580859310553076\n",
      "Epoch 5660  \tTraining Loss: 0.00013343884921122436\tValidation Loss: 0.0001358052462902526\n",
      "Epoch 5661  \tTraining Loss: 0.0001334353381872558\tValidation Loss: 0.00013580191640920534\n",
      "Epoch 5662  \tTraining Loss: 0.0001334318257643097\tValidation Loss: 0.00013579850539808115\n",
      "Epoch 5663  \tTraining Loss: 0.0001334283179694321\tValidation Loss: 0.00013579533189554552\n",
      "Epoch 5664  \tTraining Loss: 0.00013342480756926522\tValidation Loss: 0.00013579188701398075\n",
      "Epoch 5665  \tTraining Loss: 0.00013342130026010204\tValidation Loss: 0.00013578838913871433\n",
      "Epoch 5666  \tTraining Loss: 0.00013341779352277268\tValidation Loss: 0.0001357851492752758\n",
      "Epoch 5667  \tTraining Loss: 0.0001334142867753651\tValidation Loss: 0.00013578180911515987\n",
      "Epoch 5668  \tTraining Loss: 0.00013341078357477433\tValidation Loss: 0.00013577848628562605\n",
      "Epoch 5669  \tTraining Loss: 0.00013340727802035426\tValidation Loss: 0.0001357751445747246\n",
      "Epoch 5670  \tTraining Loss: 0.00013340377629910425\tValidation Loss: 0.00013577179159211\n",
      "Epoch 5671  \tTraining Loss: 0.00013340027465280513\tValidation Loss: 0.0001357684685606154\n",
      "Epoch 5672  \tTraining Loss: 0.00013339677303289625\tValidation Loss: 0.00013576499866821347\n",
      "Epoch 5673  \tTraining Loss: 0.00013339327446801048\tValidation Loss: 0.00013576177244566497\n",
      "Epoch 5674  \tTraining Loss: 0.00013338977417321477\tValidation Loss: 0.00013575857321531157\n",
      "Epoch 5675  \tTraining Loss: 0.00013338627819761375\tValidation Loss: 0.00013575507856672842\n",
      "Epoch 5676  \tTraining Loss: 0.000133382781152295\tValidation Loss: 0.0001357517812575931\n",
      "Epoch 5677  \tTraining Loss: 0.0001333792846989766\tValidation Loss: 0.00013574831112413157\n",
      "Epoch 5678  \tTraining Loss: 0.00013337579124012177\tValidation Loss: 0.00013574521517329256\n",
      "Epoch 5679  \tTraining Loss: 0.00013337229596893107\tValidation Loss: 0.00013574180041042713\n",
      "Epoch 5680  \tTraining Loss: 0.0001333688047408766\tValidation Loss: 0.0001357383237159854\n",
      "Epoch 5681  \tTraining Loss: 0.00013336531259129566\tValidation Loss: 0.00013573522900988855\n",
      "Epoch 5682  \tTraining Loss: 0.00013336182208608432\tValidation Loss: 0.0001357317047038476\n",
      "Epoch 5683  \tTraining Loss: 0.00013335833367363723\tValidation Loss: 0.0001357284817352637\n",
      "Epoch 5684  \tTraining Loss: 0.00013335484328954494\tValidation Loss: 0.00013572516131265844\n",
      "Epoch 5685  \tTraining Loss: 0.00013335135741239255\tValidation Loss: 0.00013572185014688062\n",
      "Epoch 5686  \tTraining Loss: 0.000133347866656555\tValidation Loss: 0.00013571853439880023\n",
      "Epoch 5687  \tTraining Loss: 0.00013334437653130134\tValidation Loss: 0.00013571514339195346\n",
      "Epoch 5688  \tTraining Loss: 0.00013334088910519387\tValidation Loss: 0.00013571200989864274\n",
      "Epoch 5689  \tTraining Loss: 0.00013333740074358252\tValidation Loss: 0.0001357084599428031\n",
      "Epoch 5690  \tTraining Loss: 0.00013333391573621387\tValidation Loss: 0.00013570523383250356\n",
      "Epoch 5691  \tTraining Loss: 0.00013333042835621224\tValidation Loss: 0.00013570193575998773\n",
      "Epoch 5692  \tTraining Loss: 0.0001333269453561571\tValidation Loss: 0.00013569860523181542\n",
      "Epoch 5693  \tTraining Loss: 0.00013332346202667874\tValidation Loss: 0.00013569529976716223\n",
      "Epoch 5694  \tTraining Loss: 0.00013331997771453888\tValidation Loss: 0.00013569185067476488\n",
      "Epoch 5695  \tTraining Loss: 0.00013331649903582636\tValidation Loss: 0.00013568878835326667\n",
      "Epoch 5696  \tTraining Loss: 0.00013331301708751375\tValidation Loss: 0.00013568539025958753\n",
      "Epoch 5697  \tTraining Loss: 0.00013330953751857348\tValidation Loss: 0.0001356819265863339\n",
      "Epoch 5698  \tTraining Loss: 0.00013330606024984702\tValidation Loss: 0.00013567886326129735\n",
      "Epoch 5699  \tTraining Loss: 0.0001333025817439512\tValidation Loss: 0.00013567540692961214\n",
      "Epoch 5700  \tTraining Loss: 0.00013329910704896857\tValidation Loss: 0.00013567213116213604\n",
      "Epoch 5701  \tTraining Loss: 0.000133295630153115\tValidation Loss: 0.0001356688309002024\n",
      "Epoch 5702  \tTraining Loss: 0.00013329215645066422\tValidation Loss: 0.00013566550757794168\n",
      "Epoch 5703  \tTraining Loss: 0.00013328868325189383\tValidation Loss: 0.00013566221067865892\n",
      "Epoch 5704  \tTraining Loss: 0.00013328520962436026\tValidation Loss: 0.00013565891835529592\n",
      "Epoch 5705  \tTraining Loss: 0.00013328173999331547\tValidation Loss: 0.00013565559979702763\n",
      "Epoch 5706  \tTraining Loss: 0.00013327826872249794\tValidation Loss: 0.00013565230718134578\n",
      "Epoch 5707  \tTraining Loss: 0.00013327479921055804\tValidation Loss: 0.00013564888895494965\n",
      "Epoch 5708  \tTraining Loss: 0.00013327133173652802\tValidation Loss: 0.00013564582234770948\n",
      "Epoch 5709  \tTraining Loss: 0.00013326786281834383\tValidation Loss: 0.00013564250356103738\n",
      "Epoch 5710  \tTraining Loss: 0.0001332643983563666\tValidation Loss: 0.0001356390095972612\n",
      "Epoch 5711  \tTraining Loss: 0.00013326093226805817\tValidation Loss: 0.00013563580198974526\n",
      "Epoch 5712  \tTraining Loss: 0.0001332574670395779\tValidation Loss: 0.00013563250089936184\n",
      "Epoch 5713  \tTraining Loss: 0.00013325400548233493\tValidation Loss: 0.00013562923764343386\n",
      "Epoch 5714  \tTraining Loss: 0.00013325054126424538\tValidation Loss: 0.0001356259397097874\n",
      "Epoch 5715  \tTraining Loss: 0.00013324708064206688\tValidation Loss: 0.00013562264544739007\n",
      "Epoch 5716  \tTraining Loss: 0.000133243620893446\tValidation Loss: 0.00013561936577843588\n",
      "Epoch 5717  \tTraining Loss: 0.00013324016006566528\tValidation Loss: 0.00013561593765659767\n",
      "Epoch 5718  \tTraining Loss: 0.0001332367029940767\tValidation Loss: 0.0001356127711215208\n",
      "Epoch 5719  \tTraining Loss: 0.00013323324469108482\tValidation Loss: 0.0001356096149036871\n",
      "Epoch 5720  \tTraining Loss: 0.00013322978901130506\tValidation Loss: 0.00013560616308601293\n",
      "Epoch 5721  \tTraining Loss: 0.00013322633375431578\tValidation Loss: 0.00013560292517752933\n",
      "Epoch 5722  \tTraining Loss: 0.00013322287841860633\tValidation Loss: 0.00013559962582178055\n",
      "Epoch 5723  \tTraining Loss: 0.00013321942640852555\tValidation Loss: 0.00013559634991127759\n",
      "Epoch 5724  \tTraining Loss: 0.00013321597226861057\tValidation Loss: 0.00013559307745039637\n",
      "Epoch 5725  \tTraining Loss: 0.00013321252174329566\tValidation Loss: 0.00013558977891530073\n",
      "Epoch 5726  \tTraining Loss: 0.00013320907154873115\tValidation Loss: 0.000135586506038485\n",
      "Epoch 5727  \tTraining Loss: 0.00013320562052039197\tValidation Loss: 0.00013558323761156328\n",
      "Epoch 5728  \tTraining Loss: 0.0001332021739247923\tValidation Loss: 0.00013557981748148912\n",
      "Epoch 5729  \tTraining Loss: 0.00013319872545955176\tValidation Loss: 0.00013557676696619972\n",
      "Epoch 5730  \tTraining Loss: 0.00013319527904004746\tValidation Loss: 0.00013557336136048708\n",
      "Epoch 5731  \tTraining Loss: 0.00013319183473119267\tValidation Loss: 0.0001355701206812533\n",
      "Epoch 5732  \tTraining Loss: 0.00013318838792151128\tValidation Loss: 0.000135566832449022\n",
      "Epoch 5733  \tTraining Loss: 0.00013318494624094595\tValidation Loss: 0.0001355635582210733\n",
      "Epoch 5734  \tTraining Loss: 0.0001331815033409374\tValidation Loss: 0.00013556029676202702\n",
      "Epoch 5735  \tTraining Loss: 0.00013317806037552036\tValidation Loss: 0.00013555688759861473\n",
      "Epoch 5736  \tTraining Loss: 0.00013317462183616986\tValidation Loss: 0.00013555386494517425\n",
      "Epoch 5737  \tTraining Loss: 0.00013317118074890252\tValidation Loss: 0.00013555050868994397\n",
      "Epoch 5738  \tTraining Loss: 0.00013316774225198108\tValidation Loss: 0.00013554708813449728\n",
      "Epoch 5739  \tTraining Loss: 0.0001331643054543254\tValidation Loss: 0.00013554406535187983\n",
      "Epoch 5740  \tTraining Loss: 0.00013316086778275923\tValidation Loss: 0.0001355406531180369\n",
      "Epoch 5741  \tTraining Loss: 0.0001331574336838814\tValidation Loss: 0.00013553743585054267\n",
      "Epoch 5742  \tTraining Loss: 0.00013315399771236707\tValidation Loss: 0.00013553416039949206\n",
      "Epoch 5743  \tTraining Loss: 0.00013315056388990914\tValidation Loss: 0.0001355308761475139\n",
      "Epoch 5744  \tTraining Loss: 0.00013314713210696018\tValidation Loss: 0.00013552764105312034\n",
      "Epoch 5745  \tTraining Loss: 0.00013314369903052163\tValidation Loss: 0.00013552437426384123\n",
      "Epoch 5746  \tTraining Loss: 0.0001331402689830792\tValidation Loss: 0.00013552109510033572\n",
      "Epoch 5747  \tTraining Loss: 0.00013313683949361114\tValidation Loss: 0.00013551786452689106\n",
      "Epoch 5748  \tTraining Loss: 0.0001331334097953268\tValidation Loss: 0.00013551447337769156\n",
      "Epoch 5749  \tTraining Loss: 0.00013312998301702177\tValidation Loss: 0.000135511388914978\n",
      "Epoch 5750  \tTraining Loss: 0.0001331265552076041\tValidation Loss: 0.00013550821701764263\n",
      "Epoch 5751  \tTraining Loss: 0.00013312313025688388\tValidation Loss: 0.0001355047180308109\n",
      "Epoch 5752  \tTraining Loss: 0.00013311970493384183\tValidation Loss: 0.0001355015514939652\n",
      "Epoch 5753  \tTraining Loss: 0.0001331162799586207\tValidation Loss: 0.00013549818808184657\n",
      "Epoch 5754  \tTraining Loss: 0.0001331128590943638\tValidation Loss: 0.00013549516868497342\n",
      "Epoch 5755  \tTraining Loss: 0.00013310943531652462\tValidation Loss: 0.00013549184569471354\n",
      "Epoch 5756  \tTraining Loss: 0.0001331060148539603\tValidation Loss: 0.00013548844766197594\n",
      "Epoch 5757  \tTraining Loss: 0.00013310259532196627\tValidation Loss: 0.00013548542293775572\n",
      "Epoch 5758  \tTraining Loss: 0.00013309917511094134\tValidation Loss: 0.00013548217487262737\n",
      "Epoch 5759  \tTraining Loss: 0.00013309575851410576\tValidation Loss: 0.00013547871633997927\n",
      "Epoch 5760  \tTraining Loss: 0.000133092340679437\tValidation Loss: 0.00013547555526790395\n",
      "Epoch 5761  \tTraining Loss: 0.00013308892379398075\tValidation Loss: 0.00013547232556084546\n",
      "Epoch 5762  \tTraining Loss: 0.00013308551042830926\tValidation Loss: 0.00013546909655284185\n",
      "Epoch 5763  \tTraining Loss: 0.0001330820942191127\tValidation Loss: 0.00013546584607846034\n",
      "Epoch 5764  \tTraining Loss: 0.0001330786818024781\tValidation Loss: 0.00013546260607925567\n",
      "Epoch 5765  \tTraining Loss: 0.00013307527027614662\tValidation Loss: 0.00013545937777540028\n",
      "Epoch 5766  \tTraining Loss: 0.00013307185706918194\tValidation Loss: 0.0001354561476794745\n",
      "Epoch 5767  \tTraining Loss: 0.0001330684485960605\tValidation Loss: 0.00013545283955468892\n",
      "Epoch 5768  \tTraining Loss: 0.00013306503910927967\tValidation Loss: 0.00013544976401316518\n",
      "Epoch 5769  \tTraining Loss: 0.0001330616296957025\tValidation Loss: 0.00013544631592405488\n",
      "Epoch 5770  \tTraining Loss: 0.00013305822392384366\tValidation Loss: 0.00013544317488706402\n",
      "Epoch 5771  \tTraining Loss: 0.00013305481547228995\tValidation Loss: 0.00013543993927100305\n",
      "Epoch 5772  \tTraining Loss: 0.00013305141100680045\tValidation Loss: 0.00013543670926708447\n",
      "Epoch 5773  \tTraining Loss: 0.00013304800702567738\tValidation Loss: 0.00013543348975780247\n",
      "Epoch 5774  \tTraining Loss: 0.00013304460151212767\tValidation Loss: 0.00013543025086480738\n",
      "Epoch 5775  \tTraining Loss: 0.00013304120257301498\tValidation Loss: 0.0001354270019713167\n",
      "Epoch 5776  \tTraining Loss: 0.00013303780317515114\tValidation Loss: 0.00013542383446360883\n",
      "Epoch 5777  \tTraining Loss: 0.00013303440399250934\tValidation Loss: 0.00013542040352641533\n",
      "Epoch 5778  \tTraining Loss: 0.00013303100883265347\tValidation Loss: 0.0001354172472345956\n",
      "Epoch 5779  \tTraining Loss: 0.00013302761090402485\tValidation Loss: 0.0001354141253800104\n",
      "Epoch 5780  \tTraining Loss: 0.0001330242167496211\tValidation Loss: 0.00013541066832661828\n",
      "Epoch 5781  \tTraining Loss: 0.00013302082293949454\tValidation Loss: 0.000135407520881056\n",
      "Epoch 5782  \tTraining Loss: 0.0001330174278137163\tValidation Loss: 0.00013540440580124045\n",
      "Epoch 5783  \tTraining Loss: 0.0001330140379599051\tValidation Loss: 0.00013540095551026096\n",
      "Epoch 5784  \tTraining Loss: 0.00013301064587393528\tValidation Loss: 0.0001353978141223795\n",
      "Epoch 5785  \tTraining Loss: 0.00013300725451481976\tValidation Loss: 0.00013539446985307357\n",
      "Epoch 5786  \tTraining Loss: 0.00013300386728941392\tValidation Loss: 0.0001353915398004894\n",
      "Epoch 5787  \tTraining Loss: 0.00013300047740335736\tValidation Loss: 0.00013538815550565635\n",
      "Epoch 5788  \tTraining Loss: 0.00013299709071003027\tValidation Loss: 0.00013538478175604234\n",
      "Epoch 5789  \tTraining Loss: 0.00013299370455902072\tValidation Loss: 0.00013538177649119473\n",
      "Epoch 5790  \tTraining Loss: 0.00013299031760355135\tValidation Loss: 0.0001353784750705738\n",
      "Epoch 5791  \tTraining Loss: 0.00013298693520919012\tValidation Loss: 0.00013537510189353668\n",
      "Epoch 5792  \tTraining Loss: 0.0001329835506000593\tValidation Loss: 0.00013537197585916807\n",
      "Epoch 5793  \tTraining Loss: 0.00013298016742220632\tValidation Loss: 0.00013536877285298313\n",
      "Epoch 5794  \tTraining Loss: 0.0001329767875402882\tValidation Loss: 0.00013536556840743175\n",
      "Epoch 5795  \tTraining Loss: 0.00013297340500153113\tValidation Loss: 0.00013536241286847734\n",
      "Epoch 5796  \tTraining Loss: 0.00013297002671282403\tValidation Loss: 0.00013535900886722533\n",
      "Epoch 5797  \tTraining Loss: 0.0001329666480928638\tValidation Loss: 0.00013535600699669185\n",
      "Epoch 5798  \tTraining Loss: 0.0001329632688450657\tValidation Loss: 0.00013535271601248147\n",
      "Epoch 5799  \tTraining Loss: 0.00013295989398945157\tValidation Loss: 0.00013534935422302317\n",
      "Epoch 5800  \tTraining Loss: 0.0001329565170256719\tValidation Loss: 0.00013534623929472957\n",
      "Epoch 5801  \tTraining Loss: 0.00013295314187858787\tValidation Loss: 0.0001353430476368153\n",
      "Epoch 5802  \tTraining Loss: 0.00013294976930514643\tValidation Loss: 0.00013533985435550656\n",
      "Epoch 5803  \tTraining Loss: 0.00013294639437131086\tValidation Loss: 0.0001353366567894299\n",
      "Epoch 5804  \tTraining Loss: 0.00013294302376106576\tValidation Loss: 0.00013533338230855088\n",
      "Epoch 5805  \tTraining Loss: 0.00013293965308609255\tValidation Loss: 0.00013533033719919085\n",
      "Epoch 5806  \tTraining Loss: 0.00013293628155631277\tValidation Loss: 0.00013532704944392872\n",
      "Epoch 5807  \tTraining Loss: 0.00013293291395083704\tValidation Loss: 0.00013532369714904622\n",
      "Epoch 5808  \tTraining Loss: 0.0001329295447940471\tValidation Loss: 0.0001353207340803116\n",
      "Epoch 5809  \tTraining Loss: 0.00013292617771398026\tValidation Loss: 0.00013531731960525915\n",
      "Epoch 5810  \tTraining Loss: 0.00013292281212642668\tValidation Loss: 0.00013531421378640623\n",
      "Epoch 5811  \tTraining Loss: 0.00013291944526079542\tValidation Loss: 0.00013531103625412814\n",
      "Epoch 5812  \tTraining Loss: 0.00013291608183565182\tValidation Loss: 0.0001353078283556562\n",
      "Epoch 5813  \tTraining Loss: 0.0001329127186019289\tValidation Loss: 0.0001353047133850853\n",
      "Epoch 5814  \tTraining Loss: 0.0001329093552692233\tValidation Loss: 0.00013530146766602683\n",
      "Epoch 5815  \tTraining Loss: 0.00013290599479216084\tValidation Loss: 0.00013529812323069727\n",
      "Epoch 5816  \tTraining Loss: 0.00013290263379245855\tValidation Loss: 0.0001352951701197566\n",
      "Epoch 5817  \tTraining Loss: 0.00013289927372941217\tValidation Loss: 0.00013529176605919045\n",
      "Epoch 5818  \tTraining Loss: 0.00013289591589641137\tValidation Loss: 0.00013528867022255512\n",
      "Epoch 5819  \tTraining Loss: 0.0001328925569353143\tValidation Loss: 0.000135285502729678\n",
      "Epoch 5820  \tTraining Loss: 0.00013288920066955107\tValidation Loss: 0.00013528230502018318\n",
      "Epoch 5821  \tTraining Loss: 0.00013288584525413273\tValidation Loss: 0.00013527914662497491\n",
      "Epoch 5822  \tTraining Loss: 0.00013288248949777437\tValidation Loss: 0.00013527602923204225\n",
      "Epoch 5823  \tTraining Loss: 0.00013287913608358583\tValidation Loss: 0.00013527264094331432\n",
      "Epoch 5824  \tTraining Loss: 0.00013287578350932927\tValidation Loss: 0.00013526968964825483\n",
      "Epoch 5825  \tTraining Loss: 0.0001328724302420659\tValidation Loss: 0.0001352662940420279\n",
      "Epoch 5826  \tTraining Loss: 0.00013286908038925824\tValidation Loss: 0.00013526322456883051\n",
      "Epoch 5827  \tTraining Loss: 0.00013286572921176315\tValidation Loss: 0.0001352600493533195\n",
      "Epoch 5828  \tTraining Loss: 0.0001328623796012042\tValidation Loss: 0.00013525685728969542\n",
      "Epoch 5829  \tTraining Loss: 0.00013285903267235536\tValidation Loss: 0.0001352537108316235\n",
      "Epoch 5830  \tTraining Loss: 0.0001328556838869971\tValidation Loss: 0.00013525060349613805\n",
      "Epoch 5831  \tTraining Loss: 0.00013285233792955482\tValidation Loss: 0.00013524724250203388\n",
      "Epoch 5832  \tTraining Loss: 0.000132848993426047\tValidation Loss: 0.0001352442836724655\n",
      "Epoch 5833  \tTraining Loss: 0.00013284564695574923\tValidation Loss: 0.00013524101971078948\n",
      "Epoch 5834  \tTraining Loss: 0.0001328423048405562\tValidation Loss: 0.000135237720252433\n",
      "Epoch 5835  \tTraining Loss: 0.0001328389618385916\tValidation Loss: 0.00013523477675475687\n",
      "Epoch 5836  \tTraining Loss: 0.0001328356189979978\tValidation Loss: 0.0001352314111757731\n",
      "Epoch 5837  \tTraining Loss: 0.00013283228017931407\tValidation Loss: 0.00013522834158739998\n",
      "Epoch 5838  \tTraining Loss: 0.0001328289383638468\tValidation Loss: 0.00013522524768792718\n",
      "Epoch 5839  \tTraining Loss: 0.0001328256002314852\tValidation Loss: 0.00013522202446768157\n",
      "Epoch 5840  \tTraining Loss: 0.00013282226308565328\tValidation Loss: 0.0001352188601700855\n",
      "Epoch 5841  \tTraining Loss: 0.00013281892411420238\tValidation Loss: 0.00013521570429357132\n",
      "Epoch 5842  \tTraining Loss: 0.00013281558951283072\tValidation Loss: 0.00013521240464718035\n",
      "Epoch 5843  \tTraining Loss: 0.00013281225388370093\tValidation Loss: 0.00013520946795635903\n",
      "Epoch 5844  \tTraining Loss: 0.00013280891871126003\tValidation Loss: 0.00013520624051826721\n",
      "Epoch 5845  \tTraining Loss: 0.00013280558684031072\tValidation Loss: 0.00013520293947999565\n",
      "Epoch 5846  \tTraining Loss: 0.00013280225356808296\tValidation Loss: 0.00013519997020673355\n",
      "Epoch 5847  \tTraining Loss: 0.00013279892214492208\tValidation Loss: 0.00013519675437214366\n",
      "Epoch 5848  \tTraining Loss: 0.00013279559255812868\tValidation Loss: 0.0001351935986123429\n",
      "Epoch 5849  \tTraining Loss: 0.00013279226165613183\tValidation Loss: 0.00013519045512534769\n",
      "Epoch 5850  \tTraining Loss: 0.00013278893337955887\tValidation Loss: 0.0001351871652805794\n",
      "Epoch 5851  \tTraining Loss: 0.000132785606020705\tValidation Loss: 0.00013518425482344376\n",
      "Epoch 5852  \tTraining Loss: 0.00013278227847489613\tValidation Loss: 0.00013518101937244503\n",
      "Epoch 5853  \tTraining Loss: 0.00013277895278757145\tValidation Loss: 0.0001351777234896341\n",
      "Epoch 5854  \tTraining Loss: 0.00013277562840764077\tValidation Loss: 0.0001351747654807047\n",
      "Epoch 5855  \tTraining Loss: 0.00013277230322362706\tValidation Loss: 0.0001351716840382806\n",
      "Epoch 5856  \tTraining Loss: 0.0001327689815740869\tValidation Loss: 0.0001351683138669519\n",
      "Epoch 5857  \tTraining Loss: 0.00013276565906875486\tValidation Loss: 0.00013516525977989928\n",
      "Epoch 5858  \tTraining Loss: 0.0001327623362935624\tValidation Loss: 0.00013516198992740179\n",
      "Epoch 5859  \tTraining Loss: 0.0001327590180464624\tValidation Loss: 0.0001351590939575425\n",
      "Epoch 5860  \tTraining Loss: 0.00013275569725691562\tValidation Loss: 0.00013515586858476187\n",
      "Epoch 5861  \tTraining Loss: 0.0001327523786976434\tValidation Loss: 0.00013515259932880158\n",
      "Epoch 5862  \tTraining Loss: 0.00013274906232805806\tValidation Loss: 0.00013514963303106788\n",
      "Epoch 5863  \tTraining Loss: 0.000132745744021317\tValidation Loss: 0.00013514643238898187\n",
      "Epoch 5864  \tTraining Loss: 0.00013274242987685662\tValidation Loss: 0.0001351432884275653\n",
      "Epoch 5865  \tTraining Loss: 0.00013273911489549437\tValidation Loss: 0.00013514016265405403\n",
      "Epoch 5866  \tTraining Loss: 0.00013273579951446267\tValidation Loss: 0.00013513703694029066\n",
      "Epoch 5867  \tTraining Loss: 0.00013273248827056657\tValidation Loss: 0.00013513389042630426\n",
      "Epoch 5868  \tTraining Loss: 0.00013272917568930818\tValidation Loss: 0.0001351307838539953\n",
      "Epoch 5869  \tTraining Loss: 0.00013272586401806552\tValidation Loss: 0.0001351275234745409\n",
      "Epoch 5870  \tTraining Loss: 0.00013272255515867147\tValidation Loss: 0.0001351245652418021\n",
      "Epoch 5871  \tTraining Loss: 0.00013271924482378047\tValidation Loss: 0.00013512139385744626\n",
      "Epoch 5872  \tTraining Loss: 0.00013271593676579129\tValidation Loss: 0.00013511824214721806\n",
      "Epoch 5873  \tTraining Loss: 0.00013271262998567818\tValidation Loss: 0.00013511513825962006\n",
      "Epoch 5874  \tTraining Loss: 0.0001327093225051524\tValidation Loss: 0.0001351120064920716\n",
      "Epoch 5875  \tTraining Loss: 0.00013270601683515072\tValidation Loss: 0.000135108741606466\n",
      "Epoch 5876  \tTraining Loss: 0.00013270271335043886\tValidation Loss: 0.00013510586060474643\n",
      "Epoch 5877  \tTraining Loss: 0.00013269940820557485\tValidation Loss: 0.00013510272472017854\n",
      "Epoch 5878  \tTraining Loss: 0.00013269610642804684\tValidation Loss: 0.00013509940942741899\n",
      "Epoch 5879  \tTraining Loss: 0.00013269280470216944\tValidation Loss: 0.00013509637545707588\n",
      "Epoch 5880  \tTraining Loss: 0.00013268950186855262\tValidation Loss: 0.0001350933770944866\n",
      "Epoch 5881  \tTraining Loss: 0.00013268620418703254\tValidation Loss: 0.00013509004973580186\n",
      "Epoch 5882  \tTraining Loss: 0.00013268290403119784\tValidation Loss: 0.000135087025590943\n",
      "Epoch 5883  \tTraining Loss: 0.0001326796047385054\tValidation Loss: 0.0001350838021399981\n",
      "Epoch 5884  \tTraining Loss: 0.00013267630930206224\tValidation Loss: 0.00013508091602968588\n",
      "Epoch 5885  \tTraining Loss: 0.00013267301143695952\tValidation Loss: 0.00013507780288773136\n",
      "Epoch 5886  \tTraining Loss: 0.0001326697168133742\tValidation Loss: 0.00013507448170381838\n",
      "Epoch 5887  \tTraining Loss: 0.00013266642234668023\tValidation Loss: 0.000135071452025581\n",
      "Epoch 5888  \tTraining Loss: 0.0001326631275921184\tValidation Loss: 0.00013506848172576322\n",
      "Epoch 5889  \tTraining Loss: 0.00013265983576686344\tValidation Loss: 0.00013506514646552333\n",
      "Epoch 5890  \tTraining Loss: 0.00013265654396947012\tValidation Loss: 0.00013506214396190295\n",
      "Epoch 5891  \tTraining Loss: 0.00013265325200064514\tValidation Loss: 0.00013505903858650122\n",
      "Epoch 5892  \tTraining Loss: 0.0001326499624611743\tValidation Loss: 0.00013505598945013118\n",
      "Epoch 5893  \tTraining Loss: 0.00013264667466305892\tValidation Loss: 0.00013505284761034208\n",
      "Epoch 5894  \tTraining Loss: 0.0001326433833558758\tValidation Loss: 0.00013504967192077573\n",
      "Epoch 5895  \tTraining Loss: 0.00013264009077321303\tValidation Loss: 0.0001350464357040436\n",
      "Epoch 5896  \tTraining Loss: 0.00013263679779528927\tValidation Loss: 0.0001350434379777422\n",
      "Epoch 5897  \tTraining Loss: 0.0001326335044681025\tValidation Loss: 0.00013504036481460662\n",
      "Epoch 5898  \tTraining Loss: 0.00013263021624934602\tValidation Loss: 0.00013503729327497837\n",
      "Epoch 5899  \tTraining Loss: 0.00013262692464331268\tValidation Loss: 0.000135034273217462\n",
      "Epoch 5900  \tTraining Loss: 0.00013262363566687543\tValidation Loss: 0.00013503100659423514\n",
      "Epoch 5901  \tTraining Loss: 0.00013262034912829159\tValidation Loss: 0.00013502801354657582\n",
      "Epoch 5902  \tTraining Loss: 0.00013261706020091686\tValidation Loss: 0.00013502493166261262\n",
      "Epoch 5903  \tTraining Loss: 0.00013261377461298615\tValidation Loss: 0.00013502185823957872\n",
      "Epoch 5904  \tTraining Loss: 0.00013261049039353372\tValidation Loss: 0.0001350187929197378\n",
      "Epoch 5905  \tTraining Loss: 0.00013260720460504113\tValidation Loss: 0.0001350157257097977\n",
      "Epoch 5906  \tTraining Loss: 0.00013260392151272045\tValidation Loss: 0.0001350125863811328\n",
      "Epoch 5907  \tTraining Loss: 0.00013260064015283569\tValidation Loss: 0.0001350095447166848\n",
      "Epoch 5908  \tTraining Loss: 0.00013259735748700645\tValidation Loss: 0.0001350064784733881\n",
      "Epoch 5909  \tTraining Loss: 0.0001325940770105672\tValidation Loss: 0.00013500339250146898\n",
      "Epoch 5910  \tTraining Loss: 0.00013259079814564772\tValidation Loss: 0.00013500034525099693\n",
      "Epoch 5911  \tTraining Loss: 0.00013258751840105663\tValidation Loss: 0.00013499726878588155\n",
      "Epoch 5912  \tTraining Loss: 0.00013258423997473183\tValidation Loss: 0.00013499405944765295\n",
      "Epoch 5913  \tTraining Loss: 0.00013258096451076632\tValidation Loss: 0.00013499110858313534\n",
      "Epoch 5914  \tTraining Loss: 0.00013257768722038207\tValidation Loss: 0.00013498811778287452\n",
      "Epoch 5915  \tTraining Loss: 0.00013257441225201342\tValidation Loss: 0.0001349849896888899\n",
      "Epoch 5916  \tTraining Loss: 0.00013257113942044633\tValidation Loss: 0.00013498192125695833\n",
      "Epoch 5917  \tTraining Loss: 0.00013256786425871992\tValidation Loss: 0.0001349788436145602\n",
      "Epoch 5918  \tTraining Loss: 0.000132564592348504\tValidation Loss: 0.00013497565860557482\n",
      "Epoch 5919  \tTraining Loss: 0.00013256132176915657\tValidation Loss: 0.000134972696064106\n",
      "Epoch 5920  \tTraining Loss: 0.0001325580494565013\tValidation Loss: 0.00013496965207519088\n",
      "Epoch 5921  \tTraining Loss: 0.00013255478090441265\tValidation Loss: 0.00013496658058907943\n",
      "Epoch 5922  \tTraining Loss: 0.00013255151273996007\tValidation Loss: 0.0001349636003416583\n",
      "Epoch 5923  \tTraining Loss: 0.00013254824388690448\tValidation Loss: 0.0001349604871582499\n",
      "Epoch 5924  \tTraining Loss: 0.00013254497732602353\tValidation Loss: 0.0001349572790906129\n",
      "Epoch 5925  \tTraining Loss: 0.00013254171155173115\tValidation Loss: 0.00013495433245225999\n",
      "Epoch 5926  \tTraining Loss: 0.00013253844582849922\tValidation Loss: 0.00013495127828699478\n",
      "Epoch 5927  \tTraining Loss: 0.00013253518139383992\tValidation Loss: 0.00013494820801301423\n",
      "Epoch 5928  \tTraining Loss: 0.00013253191924436056\tValidation Loss: 0.0001349451811571884\n",
      "Epoch 5929  \tTraining Loss: 0.00013252865578932568\tValidation Loss: 0.00013494219435256305\n",
      "Epoch 5930  \tTraining Loss: 0.0001325253943402272\tValidation Loss: 0.00013493893717435535\n",
      "Epoch 5931  \tTraining Loss: 0.0001325221350135888\tValidation Loss: 0.000134935988255094\n",
      "Epoch 5932  \tTraining Loss: 0.00013251887377783089\tValidation Loss: 0.00013493293806406736\n",
      "Epoch 5933  \tTraining Loss: 0.0001325156151626367\tValidation Loss: 0.00013492976778120861\n",
      "Epoch 5934  \tTraining Loss: 0.00013251235851717558\tValidation Loss: 0.0001349269421829439\n",
      "Epoch 5935  \tTraining Loss: 0.00013250909987513754\tValidation Loss: 0.0001349238045537794\n",
      "Epoch 5936  \tTraining Loss: 0.00013250584445594378\tValidation Loss: 0.0001349206315788837\n",
      "Epoch 5937  \tTraining Loss: 0.00013250259012236704\tValidation Loss: 0.00013491775657195214\n",
      "Epoch 5938  \tTraining Loss: 0.00013249933455779266\tValidation Loss: 0.00013491466338307908\n",
      "Epoch 5939  \tTraining Loss: 0.0001324960818597178\tValidation Loss: 0.00013491147174451624\n",
      "Epoch 5940  \tTraining Loss: 0.00013249282951290618\tValidation Loss: 0.00013490864156220003\n",
      "Epoch 5941  \tTraining Loss: 0.0001324895768771382\tValidation Loss: 0.00013490552766794772\n",
      "Epoch 5942  \tTraining Loss: 0.00013248632588328102\tValidation Loss: 0.00013490234335422844\n",
      "Epoch 5943  \tTraining Loss: 0.0001324830762782831\tValidation Loss: 0.00013489941517652089\n",
      "Epoch 5944  \tTraining Loss: 0.00013247982626038733\tValidation Loss: 0.000134896451290188\n",
      "Epoch 5945  \tTraining Loss: 0.00013247657771305136\tValidation Loss: 0.00013489321125474647\n",
      "Epoch 5946  \tTraining Loss: 0.0001324733314822756\tValidation Loss: 0.00013489027759849452\n",
      "Epoch 5947  \tTraining Loss: 0.00013247008348325295\tValidation Loss: 0.00013488724244337558\n",
      "Epoch 5948  \tTraining Loss: 0.0001324668377000636\tValidation Loss: 0.00013488420958066098\n",
      "Epoch 5949  \tTraining Loss: 0.0001324635943912642\tValidation Loss: 0.00013488118577817008\n",
      "Epoch 5950  \tTraining Loss: 0.00013246034866055414\tValidation Loss: 0.00013487814290297296\n",
      "Epoch 5951  \tTraining Loss: 0.0001324571059642396\tValidation Loss: 0.00013487506416571236\n",
      "Epoch 5952  \tTraining Loss: 0.00013245386541859149\tValidation Loss: 0.00013487206746017196\n",
      "Epoch 5953  \tTraining Loss: 0.00013245062232452787\tValidation Loss: 0.00013486904036468571\n",
      "Epoch 5954  \tTraining Loss: 0.00013244738250205247\tValidation Loss: 0.00013486587447335697\n",
      "Epoch 5955  \tTraining Loss: 0.00013244414377418934\tValidation Loss: 0.00013486305898515553\n",
      "Epoch 5956  \tTraining Loss: 0.000132440904389535\tValidation Loss: 0.00013485995507655276\n",
      "Epoch 5957  \tTraining Loss: 0.000132437666435591\tValidation Loss: 0.00013485678000351163\n",
      "Epoch 5958  \tTraining Loss: 0.0001324344306569854\tValidation Loss: 0.00013485385972608082\n",
      "Epoch 5959  \tTraining Loss: 0.00013243119426151797\tValidation Loss: 0.00013485090381070715\n",
      "Epoch 5960  \tTraining Loss: 0.00013242795845535614\tValidation Loss: 0.00013484767166925071\n",
      "Epoch 5961  \tTraining Loss: 0.00013242472611998127\tValidation Loss: 0.0001348447452803783\n",
      "Epoch 5962  \tTraining Loss: 0.0001324214916025425\tValidation Loss: 0.00013484183953075684\n",
      "Epoch 5963  \tTraining Loss: 0.00013241825867126022\tValidation Loss: 0.00013483860212460633\n",
      "Epoch 5964  \tTraining Loss: 0.00013241502922047623\tValidation Loss: 0.00013483566540008183\n",
      "Epoch 5965  \tTraining Loss: 0.0001324117968624204\tValidation Loss: 0.00013483263663562644\n",
      "Epoch 5966  \tTraining Loss: 0.0001324085670610855\tValidation Loss: 0.00013482956757029336\n",
      "Epoch 5967  \tTraining Loss: 0.0001324053403498614\tValidation Loss: 0.00013482657848600335\n",
      "Epoch 5968  \tTraining Loss: 0.00013240211063164124\tValidation Loss: 0.00013482355896342213\n",
      "Epoch 5969  \tTraining Loss: 0.00013239888341381087\tValidation Loss: 0.00013482040100779483\n",
      "Epoch 5970  \tTraining Loss: 0.00013239565873095137\tValidation Loss: 0.00013481759699586064\n",
      "Epoch 5971  \tTraining Loss: 0.00013239243272277062\tValidation Loss: 0.00013481450681135764\n",
      "Epoch 5972  \tTraining Loss: 0.0001323892074710973\tValidation Loss: 0.00013481146743781389\n",
      "Epoch 5973  \tTraining Loss: 0.00013238598501419353\tValidation Loss: 0.00013480839757354184\n",
      "Epoch 5974  \tTraining Loss: 0.0001323827632144862\tValidation Loss: 0.00013480541304419704\n",
      "Epoch 5975  \tTraining Loss: 0.00013237953947258806\tValidation Loss: 0.00013480238193154772\n",
      "Epoch 5976  \tTraining Loss: 0.00013237632006051004\tValidation Loss: 0.0001347992473879556\n",
      "Epoch 5977  \tTraining Loss: 0.00013237310020001403\tValidation Loss: 0.00013479645387022947\n",
      "Epoch 5978  \tTraining Loss: 0.00013236987990316988\tValidation Loss: 0.00013479336757531653\n",
      "Epoch 5979  \tTraining Loss: 0.00013236666273355356\tValidation Loss: 0.00013479021276441113\n",
      "Epoch 5980  \tTraining Loss: 0.00013236344551410437\tValidation Loss: 0.0001347873668292154\n",
      "Epoch 5981  \tTraining Loss: 0.00013236022858760898\tValidation Loss: 0.0001347843092994997\n",
      "Epoch 5982  \tTraining Loss: 0.00013235701323881631\tValidation Loss: 0.00013478115289100609\n",
      "Epoch 5983  \tTraining Loss: 0.00013235379911446138\tValidation Loss: 0.00013477825380263367\n",
      "Epoch 5984  \tTraining Loss: 0.0001323505843838062\tValidation Loss: 0.00013477524747871714\n",
      "Epoch 5985  \tTraining Loss: 0.00013234737110755465\tValidation Loss: 0.00013477222563932752\n",
      "Epoch 5986  \tTraining Loss: 0.0001323441608719153\tValidation Loss: 0.00013476924607176268\n",
      "Epoch 5987  \tTraining Loss: 0.00013234094836841315\tValidation Loss: 0.00013476630726419928\n",
      "Epoch 5988  \tTraining Loss: 0.00013233773811245763\tValidation Loss: 0.00013476311598743145\n",
      "Epoch 5989  \tTraining Loss: 0.0001323345300358999\tValidation Loss: 0.0001347601964706887\n",
      "Epoch 5990  \tTraining Loss: 0.00013233131977588258\tValidation Loss: 0.000134757206550744\n",
      "Epoch 5991  \tTraining Loss: 0.00013232811257696335\tValidation Loss: 0.00013475407078527852\n",
      "Epoch 5992  \tTraining Loss: 0.00013232490666693688\tValidation Loss: 0.0001347511650270947\n",
      "Epoch 5993  \tTraining Loss: 0.000132321699811579\tValidation Loss: 0.00013474837912553207\n",
      "Epoch 5994  \tTraining Loss: 0.00013231849504766874\tValidation Loss: 0.00013474509304758285\n",
      "Epoch 5995  \tTraining Loss: 0.00013231529197466334\tValidation Loss: 0.0001347421840619722\n",
      "Epoch 5996  \tTraining Loss: 0.0001323120879923483\tValidation Loss: 0.0001347391829867968\n",
      "Epoch 5997  \tTraining Loss: 0.0001323088842258664\tValidation Loss: 0.00013473616906106925\n",
      "Epoch 5998  \tTraining Loss: 0.00013230568445389906\tValidation Loss: 0.00013473305431217016\n",
      "Epoch 5999  \tTraining Loss: 0.0001323024835260569\tValidation Loss: 0.00013473015894867988\n",
      "Epoch 6000  \tTraining Loss: 0.00013229928246756589\tValidation Loss: 0.00013472725574677086\n",
      "Epoch 6001  \tTraining Loss: 0.0001322960853176182\tValidation Loss: 0.00013472418731950173\n",
      "Epoch 6002  \tTraining Loss: 0.00013229288692040463\tValidation Loss: 0.00013472118763808516\n",
      "Epoch 6003  \tTraining Loss: 0.00013228968900186292\tValidation Loss: 0.0001347182032260919\n",
      "Epoch 6004  \tTraining Loss: 0.00013228649311890258\tValidation Loss: 0.00013471507748276458\n",
      "Epoch 6005  \tTraining Loss: 0.00013228329821848706\tValidation Loss: 0.00013471219986891842\n",
      "Epoch 6006  \tTraining Loss: 0.00013228010267136826\tValidation Loss: 0.00013470921408381616\n",
      "Epoch 6007  \tTraining Loss: 0.00013227690891660503\tValidation Loss: 0.00013470618181281858\n",
      "Epoch 6008  \tTraining Loss: 0.00013227371800644016\tValidation Loss: 0.0001347032285644384\n",
      "Epoch 6009  \tTraining Loss: 0.0001322705240243738\tValidation Loss: 0.00013470022816308784\n",
      "Epoch 6010  \tTraining Loss: 0.00013226733330839525\tValidation Loss: 0.00013469724630919388\n",
      "Epoch 6011  \tTraining Loss: 0.0001322641443406076\tValidation Loss: 0.000134694271282877\n",
      "Epoch 6012  \tTraining Loss: 0.00013226095346661047\tValidation Loss: 0.00013469129506317736\n",
      "Epoch 6013  \tTraining Loss: 0.00013225776518229444\tValidation Loss: 0.00013468825125632537\n",
      "Epoch 6014  \tTraining Loss: 0.00013225457903294423\tValidation Loss: 0.00013468529807568092\n",
      "Epoch 6015  \tTraining Loss: 0.0001322513913037117\tValidation Loss: 0.00013468232268941513\n",
      "Epoch 6016  \tTraining Loss: 0.00013224820455337414\tValidation Loss: 0.00013467920846922261\n",
      "Epoch 6017  \tTraining Loss: 0.0001322450213518312\tValidation Loss: 0.00013467634155710185\n",
      "Epoch 6018  \tTraining Loss: 0.00013224183625592988\tValidation Loss: 0.00013467336655429804\n",
      "Epoch 6019  \tTraining Loss: 0.00013223865178559519\tValidation Loss: 0.0001346705139871771\n",
      "Epoch 6020  \tTraining Loss: 0.00013223547146581018\tValidation Loss: 0.0001346673878032806\n",
      "Epoch 6021  \tTraining Loss: 0.00013223228970755648\tValidation Loss: 0.00013466443182281757\n",
      "Epoch 6022  \tTraining Loss: 0.0001322291078583804\tValidation Loss: 0.00013466146045940973\n",
      "Epoch 6023  \tTraining Loss: 0.000132225928604723\tValidation Loss: 0.00013465835201063043\n",
      "Epoch 6024  \tTraining Loss: 0.0001322227500109007\tValidation Loss: 0.0001346554909349316\n",
      "Epoch 6025  \tTraining Loss: 0.0001322195711293558\tValidation Loss: 0.0001346525220086241\n",
      "Epoch 6026  \tTraining Loss: 0.00013221639349681325\tValidation Loss: 0.00013464948995416551\n",
      "Epoch 6027  \tTraining Loss: 0.00013221321924857955\tValidation Loss: 0.00013464669110879553\n",
      "Epoch 6028  \tTraining Loss: 0.00013221004219902624\tValidation Loss: 0.00013464362236673307\n",
      "Epoch 6029  \tTraining Loss: 0.00013220686702429942\tValidation Loss: 0.00013464052411836516\n",
      "Epoch 6030  \tTraining Loss: 0.0001322036949367563\tValidation Loss: 0.00013463765140127358\n",
      "Epoch 6031  \tTraining Loss: 0.00013220052032668169\tValidation Loss: 0.0001346347003582521\n",
      "Epoch 6032  \tTraining Loss: 0.0001321973482818644\tValidation Loss: 0.0001346317963538122\n",
      "Epoch 6033  \tTraining Loss: 0.0001321941781534533\tValidation Loss: 0.00013462862819044416\n",
      "Epoch 6034  \tTraining Loss: 0.00013219100798855208\tValidation Loss: 0.00013462576558825035\n",
      "Epoch 6035  \tTraining Loss: 0.0001321878371038797\tValidation Loss: 0.000134622803214049\n",
      "Epoch 6036  \tTraining Loss: 0.00013218466922608132\tValidation Loss: 0.0001346198447302226\n",
      "Epoch 6037  \tTraining Loss: 0.00013218150210571935\tValidation Loss: 0.00013461689352492026\n",
      "Epoch 6038  \tTraining Loss: 0.00013217833342633496\tValidation Loss: 0.00013461401405478256\n",
      "Epoch 6039  \tTraining Loss: 0.000132175168999231\tValidation Loss: 0.00013461085517576144\n",
      "Epoch 6040  \tTraining Loss: 0.00013217200374989022\tValidation Loss: 0.000134607976381114\n",
      "Epoch 6041  \tTraining Loss: 0.00013216883846690127\tValidation Loss: 0.00013460503474448045\n",
      "Epoch 6042  \tTraining Loss: 0.00013216567496945688\tValidation Loss: 0.00013460194665912688\n",
      "Epoch 6043  \tTraining Loss: 0.00013216251326789115\tValidation Loss: 0.0001345991035040445\n",
      "Epoch 6044  \tTraining Loss: 0.00013215935066253164\tValidation Loss: 0.0001345961520956354\n",
      "Epoch 6045  \tTraining Loss: 0.0001321561888415662\tValidation Loss: 0.00013459313804855763\n",
      "Epoch 6046  \tTraining Loss: 0.00013215303150751918\tValidation Loss: 0.00013459035562176223\n",
      "Epoch 6047  \tTraining Loss: 0.0001321498705868432\tValidation Loss: 0.00013458730400424233\n",
      "Epoch 6048  \tTraining Loss: 0.00013214671138026465\tValidation Loss: 0.0001345843428187299\n",
      "Epoch 6049  \tTraining Loss: 0.00013214355514010772\tValidation Loss: 0.00013458125494505052\n",
      "Epoch 6050  \tTraining Loss: 0.00013214039821733253\tValidation Loss: 0.0001345784159421119\n",
      "Epoch 6051  \tTraining Loss: 0.0001321372419394167\tValidation Loss: 0.0001345755431566509\n",
      "Epoch 6052  \tTraining Loss: 0.00013213408700688478\tValidation Loss: 0.00013457239553607979\n",
      "Epoch 6053  \tTraining Loss: 0.00013213093405014053\tValidation Loss: 0.00013456955020895995\n",
      "Epoch 6054  \tTraining Loss: 0.00013212777924544953\tValidation Loss: 0.0001345666046054734\n",
      "Epoch 6055  \tTraining Loss: 0.00013212462692034304\tValidation Loss: 0.0001345636630473003\n",
      "Epoch 6056  \tTraining Loss: 0.00013212147665906266\tValidation Loss: 0.00013456072837917906\n",
      "Epoch 6057  \tTraining Loss: 0.00013211832463214717\tValidation Loss: 0.00013455786571548513\n",
      "Epoch 6058  \tTraining Loss: 0.00013211517505959817\tValidation Loss: 0.00013455472393860947\n",
      "Epoch 6059  \tTraining Loss: 0.00013211202688483752\tValidation Loss: 0.0001345518615196513\n",
      "Epoch 6060  \tTraining Loss: 0.00013210887800805494\tValidation Loss: 0.0001345489365768852\n",
      "Epoch 6061  \tTraining Loss: 0.00013210572974866136\tValidation Loss: 0.00013454598481837008\n",
      "Epoch 6062  \tTraining Loss: 0.0001321025843196417\tValidation Loss: 0.00013454292696554762\n",
      "Epoch 6063  \tTraining Loss: 0.00013209943915320663\tValidation Loss: 0.0001345401606560475\n",
      "Epoch 6064  \tTraining Loss: 0.0001320962930833597\tValidation Loss: 0.00013453717480872105\n",
      "Epoch 6065  \tTraining Loss: 0.0001320931501488875\tValidation Loss: 0.0001345342154222136\n",
      "Epoch 6066  \tTraining Loss: 0.00013209000710659173\tValidation Loss: 0.00013453128267987565\n",
      "Epoch 6067  \tTraining Loss: 0.00013208686416109957\tValidation Loss: 0.00013452835724507144\n",
      "Epoch 6068  \tTraining Loss: 0.00013208372242061778\tValidation Loss: 0.0001345252912114658\n",
      "Epoch 6069  \tTraining Loss: 0.00013208058347567437\tValidation Loss: 0.00013452254333202134\n",
      "Epoch 6070  \tTraining Loss: 0.00013207744289321375\tValidation Loss: 0.00013451954769426398\n",
      "Epoch 6071  \tTraining Loss: 0.00013207430279133124\tValidation Loss: 0.00013451648665225964\n",
      "Epoch 6072  \tTraining Loss: 0.00013207116701503883\tValidation Loss: 0.00013451365126557796\n",
      "Epoch 6073  \tTraining Loss: 0.00013206802802162573\tValidation Loss: 0.0001345107200310211\n",
      "Epoch 6074  \tTraining Loss: 0.00013206489163844926\tValidation Loss: 0.000134507871097852\n",
      "Epoch 6075  \tTraining Loss: 0.0001320617573336554\tValidation Loss: 0.00013450486572315023\n",
      "Epoch 6076  \tTraining Loss: 0.0001320586229495355\tValidation Loss: 0.00013450194820631423\n",
      "Epoch 6077  \tTraining Loss: 0.00013205548790850435\tValidation Loss: 0.0001344990124338566\n",
      "Epoch 6078  \tTraining Loss: 0.00013205235478644663\tValidation Loss: 0.0001344959681022031\n",
      "Epoch 6079  \tTraining Loss: 0.00013204922401636044\tValidation Loss: 0.00013449314093145855\n",
      "Epoch 6080  \tTraining Loss: 0.00013204609111078987\tValidation Loss: 0.00013449030798908288\n",
      "Epoch 6081  \tTraining Loss: 0.0001320429613729464\tValidation Loss: 0.0001344871899753989\n",
      "Epoch 6082  \tTraining Loss: 0.00013203983249005432\tValidation Loss: 0.00013448434884987068\n",
      "Epoch 6083  \tTraining Loss: 0.00013203670288720228\tValidation Loss: 0.0001344814450507286\n",
      "Epoch 6084  \tTraining Loss: 0.00013203357415622166\tValidation Loss: 0.000134478514180557\n",
      "Epoch 6085  \tTraining Loss: 0.00013203044807579223\tValidation Loss: 0.0001344755977000593\n",
      "Epoch 6086  \tTraining Loss: 0.00013202732235568327\tValidation Loss: 0.0001344727630334145\n",
      "Epoch 6087  \tTraining Loss: 0.00013202419565083067\tValidation Loss: 0.00013446978735635987\n",
      "Epoch 6088  \tTraining Loss: 0.00013202107186814565\tValidation Loss: 0.0001344667252813976\n",
      "Epoch 6089  \tTraining Loss: 0.00013201794825893885\tValidation Loss: 0.0001344638998890821\n",
      "Epoch 6090  \tTraining Loss: 0.0001320148246627838\tValidation Loss: 0.00013446100386404805\n",
      "Epoch 6091  \tTraining Loss: 0.00013201170210114704\tValidation Loss: 0.0001344579616761007\n",
      "Epoch 6092  \tTraining Loss: 0.00013200858265004632\tValidation Loss: 0.00013445523528540703\n",
      "Epoch 6093  \tTraining Loss: 0.00013200546138409215\tValidation Loss: 0.00013445226045167812\n",
      "Epoch 6094  \tTraining Loss: 0.00013200234056456232\tValidation Loss: 0.00013444933901341426\n",
      "Epoch 6095  \tTraining Loss: 0.0001319992235042369\tValidation Loss: 0.00013444641404819664\n",
      "Epoch 6096  \tTraining Loss: 0.00013199610513970722\tValidation Loss: 0.00013444352685664687\n",
      "Epoch 6097  \tTraining Loss: 0.00013199298768246762\tValidation Loss: 0.00013444068530684208\n",
      "Epoch 6098  \tTraining Loss: 0.00013198987143994394\tValidation Loss: 0.0001344375760528695\n",
      "Epoch 6099  \tTraining Loss: 0.0001319867572156529\tValidation Loss: 0.00013443476913231562\n",
      "Epoch 6100  \tTraining Loss: 0.00013198364127010783\tValidation Loss: 0.00013443186305714633\n",
      "Epoch 6101  \tTraining Loss: 0.00013198052709394886\tValidation Loss: 0.00013442884256432405\n",
      "Epoch 6102  \tTraining Loss: 0.00013197741585893965\tValidation Loss: 0.00013442603653396578\n",
      "Epoch 6103  \tTraining Loss: 0.00013197430249454626\tValidation Loss: 0.0001344232246912999\n",
      "Epoch 6104  \tTraining Loss: 0.00013197119140193216\tValidation Loss: 0.00013442024561396006\n",
      "Epoch 6105  \tTraining Loss: 0.00013196808116543174\tValidation Loss: 0.0001344171955281405\n",
      "Epoch 6106  \tTraining Loss: 0.00013196497246080815\tValidation Loss: 0.00013441452630703714\n",
      "Epoch 6107  \tTraining Loss: 0.00013196186224066073\tValidation Loss: 0.000134411541088791\n",
      "Epoch 6108  \tTraining Loss: 0.00013195875465039304\tValidation Loss: 0.00013440859264592109\n",
      "Epoch 6109  \tTraining Loss: 0.00013195564900422556\tValidation Loss: 0.0001344057232685678\n",
      "Epoch 6110  \tTraining Loss: 0.00013195254109956998\tValidation Loss: 0.0001344028280181549\n",
      "Epoch 6111  \tTraining Loss: 0.00013194943565591832\tValidation Loss: 0.00013439979968983177\n",
      "Epoch 6112  \tTraining Loss: 0.00013194633210839946\tValidation Loss: 0.0001343970148625972\n",
      "Epoch 6113  \tTraining Loss: 0.0001319432277261735\tValidation Loss: 0.00013439412315144689\n",
      "Epoch 6114  \tTraining Loss: 0.00013194012369267192\tValidation Loss: 0.0001343912894278971\n",
      "Epoch 6115  \tTraining Loss: 0.00013193702329039914\tValidation Loss: 0.0001343882167845192\n",
      "Epoch 6116  \tTraining Loss: 0.00013193392171471105\tValidation Loss: 0.00013438540928395354\n",
      "Epoch 6117  \tTraining Loss: 0.00013193082017773125\tValidation Loss: 0.00013438265250718064\n",
      "Epoch 6118  \tTraining Loss: 0.0001319277210037014\tValidation Loss: 0.00013437954566379543\n",
      "Epoch 6119  \tTraining Loss: 0.00013192462295309287\tValidation Loss: 0.00013437683270171313\n",
      "Epoch 6120  \tTraining Loss: 0.00013192152458371117\tValidation Loss: 0.00013437387772548612\n",
      "Epoch 6121  \tTraining Loss: 0.00013191842603041065\tValidation Loss: 0.00013437086092772714\n",
      "Epoch 6122  \tTraining Loss: 0.0001319153320427392\tValidation Loss: 0.00013436806800608546\n",
      "Epoch 6123  \tTraining Loss: 0.0001319122346923124\tValidation Loss: 0.00013436518009286345\n",
      "Epoch 6124  \tTraining Loss: 0.00013190913987842724\tValidation Loss: 0.0001343623754614914\n",
      "Epoch 6125  \tTraining Loss: 0.0001319060469936871\tValidation Loss: 0.00013435929489674636\n",
      "Epoch 6126  \tTraining Loss: 0.00013190295433757333\tValidation Loss: 0.0001343565094300046\n",
      "Epoch 6127  \tTraining Loss: 0.00013189986092531188\tValidation Loss: 0.000134353627467848\n",
      "Epoch 6128  \tTraining Loss: 0.00013189676916164965\tValidation Loss: 0.00013435063208121488\n",
      "Epoch 6129  \tTraining Loss: 0.00013189368022084854\tValidation Loss: 0.00013434796909642157\n",
      "Epoch 6130  \tTraining Loss: 0.00013189058920855197\tValidation Loss: 0.0001343450930315583\n",
      "Epoch 6131  \tTraining Loss: 0.00013188750044088425\tValidation Loss: 0.0001343421276492677\n",
      "Epoch 6132  \tTraining Loss: 0.00013188441241408278\tValidation Loss: 0.00013433909869058763\n",
      "Epoch 6133  \tTraining Loss: 0.00013188132593866178\tValidation Loss: 0.0001343363315241541\n",
      "Epoch 6134  \tTraining Loss: 0.0001318782378694541\tValidation Loss: 0.00013433345755853527\n",
      "Epoch 6135  \tTraining Loss: 0.0001318751526794721\tValidation Loss: 0.00013433054325934167\n",
      "Epoch 6136  \tTraining Loss: 0.00013187206921024745\tValidation Loss: 0.0001343276999767554\n",
      "Epoch 6137  \tTraining Loss: 0.00013186898365079563\tValidation Loss: 0.00013432482972900692\n",
      "Epoch 6138  \tTraining Loss: 0.0001318659001960646\tValidation Loss: 0.0001343218266895183\n",
      "Epoch 6139  \tTraining Loss: 0.00013186281912166072\tValidation Loss: 0.00013431906566101657\n",
      "Epoch 6140  \tTraining Loss: 0.00013185973687177374\tValidation Loss: 0.00013431627182086163\n",
      "Epoch 6141  \tTraining Loss: 0.000131856655304937\tValidation Loss: 0.00013431345788532633\n",
      "Epoch 6142  \tTraining Loss: 0.0001318535768402459\tValidation Loss: 0.00013431035642374938\n",
      "Epoch 6143  \tTraining Loss: 0.0001318504973676936\tValidation Loss: 0.00013430756841233523\n",
      "Epoch 6144  \tTraining Loss: 0.00013184741837171828\tValidation Loss: 0.00013430471718441843\n",
      "Epoch 6145  \tTraining Loss: 0.00013184434045818334\tValidation Loss: 0.00013430172300531835\n",
      "Epoch 6146  \tTraining Loss: 0.0001318412655557957\tValidation Loss: 0.00013429904322475198\n",
      "Epoch 6147  \tTraining Loss: 0.0001318381888398474\tValidation Loss: 0.00013429611484468102\n",
      "Epoch 6148  \tTraining Loss: 0.00013183511265094504\tValidation Loss: 0.00013429324059376495\n",
      "Epoch 6149  \tTraining Loss: 0.00013183203973977163\tValidation Loss: 0.000134290244670063\n",
      "Epoch 6150  \tTraining Loss: 0.00013182896609891458\tValidation Loss: 0.00013428749269651217\n",
      "Epoch 6151  \tTraining Loss: 0.00013182589314440578\tValidation Loss: 0.00013428470872594176\n",
      "Epoch 6152  \tTraining Loss: 0.00013182282089053\tValidation Loss: 0.00013428165133672128\n",
      "Epoch 6153  \tTraining Loss: 0.000131819751690813\tValidation Loss: 0.00013427901152198362\n",
      "Epoch 6154  \tTraining Loss: 0.00013181668004200726\tValidation Loss: 0.0001342760653675725\n",
      "Epoch 6155  \tTraining Loss: 0.00013181361010582843\tValidation Loss: 0.00013427320085371001\n",
      "Epoch 6156  \tTraining Loss: 0.00013181054286180016\tValidation Loss: 0.00013427028554393922\n",
      "Epoch 6157  \tTraining Loss: 0.00013180747555446883\tValidation Loss: 0.00013426747218801764\n",
      "Epoch 6158  \tTraining Loss: 0.0001318044073648067\tValidation Loss: 0.00013426460519902426\n",
      "Epoch 6159  \tTraining Loss: 0.00013180134116093915\tValidation Loss: 0.00013426163486805217\n",
      "Epoch 6160  \tTraining Loss: 0.00013179827713910767\tValidation Loss: 0.00013425887755959956\n",
      "Epoch 6161  \tTraining Loss: 0.00013179521121868358\tValidation Loss: 0.00013425611626500245\n",
      "Epoch 6162  \tTraining Loss: 0.00013179214790984926\tValidation Loss: 0.00013425307100312293\n",
      "Epoch 6163  \tTraining Loss: 0.0001317890859731267\tValidation Loss: 0.00013425031729363246\n",
      "Epoch 6164  \tTraining Loss: 0.00013178602351389908\tValidation Loss: 0.00013424746691521276\n",
      "Epoch 6165  \tTraining Loss: 0.0001317829608496344\tValidation Loss: 0.00013424460255395652\n",
      "Epoch 6166  \tTraining Loss: 0.0001317799015222017\tValidation Loss: 0.00013424183536038948\n",
      "Epoch 6167  \tTraining Loss: 0.0001317768429216834\tValidation Loss: 0.00013423893056655622\n",
      "Epoch 6168  \tTraining Loss: 0.0001317737827101052\tValidation Loss: 0.000134236077808528\n",
      "Epoch 6169  \tTraining Loss: 0.00013177072433217683\tValidation Loss: 0.00013423309969023462\n",
      "Epoch 6170  \tTraining Loss: 0.00013176766856162726\tValidation Loss: 0.0001342303638911807\n",
      "Epoch 6171  \tTraining Loss: 0.00013176461146748614\tValidation Loss: 0.00013422759640640826\n",
      "Epoch 6172  \tTraining Loss: 0.00013176155511182217\tValidation Loss: 0.00013422468965434167\n",
      "Epoch 6173  \tTraining Loss: 0.00013175850130851764\tValidation Loss: 0.00013422170399947668\n",
      "Epoch 6174  \tTraining Loss: 0.00013175544728510482\tValidation Loss: 0.00013421897006938855\n",
      "Epoch 6175  \tTraining Loss: 0.00013175239364057192\tValidation Loss: 0.000134216131589846\n",
      "Epoch 6176  \tTraining Loss: 0.00013174934022791552\tValidation Loss: 0.00013421323675275352\n",
      "Epoch 6177  \tTraining Loss: 0.0001317462913426924\tValidation Loss: 0.00013421044553943136\n",
      "Epoch 6178  \tTraining Loss: 0.000131743238845413\tValidation Loss: 0.00013420759730419556\n",
      "Epoch 6179  \tTraining Loss: 0.00013174018830328088\tValidation Loss: 0.00013420476133109004\n",
      "Epoch 6180  \tTraining Loss: 0.00013173713987560414\tValidation Loss: 0.0001342019142640633\n",
      "Epoch 6181  \tTraining Loss: 0.00013173409215309372\tValidation Loss: 0.00013419917656964803\n",
      "Epoch 6182  \tTraining Loss: 0.00013173104407735575\tValidation Loss: 0.00013419626852640372\n",
      "Epoch 6183  \tTraining Loss: 0.00013172799633570396\tValidation Loss: 0.00013419330601434985\n",
      "Epoch 6184  \tTraining Loss: 0.00013172495230051647\tValidation Loss: 0.00013419056559929056\n",
      "Epoch 6185  \tTraining Loss: 0.00013172190549284982\tValidation Loss: 0.000134187749081697\n",
      "Epoch 6186  \tTraining Loss: 0.00013171886134611546\tValidation Loss: 0.0001341849814706322\n",
      "Epoch 6187  \tTraining Loss: 0.0001317158180020408\tValidation Loss: 0.00013418196958866972\n",
      "Epoch 6188  \tTraining Loss: 0.00013171277640993778\tValidation Loss: 0.00013417922242048308\n",
      "Epoch 6189  \tTraining Loss: 0.000131709732624724\tValidation Loss: 0.000134176390065781\n",
      "Epoch 6190  \tTraining Loss: 0.0001317066916169279\tValidation Loss: 0.00013417345341395432\n",
      "Epoch 6191  \tTraining Loss: 0.00013170365270653335\tValidation Loss: 0.00013417079893299864\n",
      "Epoch 6192  \tTraining Loss: 0.0001317006124794551\tValidation Loss: 0.0001341679237269144\n",
      "Epoch 6193  \tTraining Loss: 0.00013169757294825463\tValidation Loss: 0.00013416507450461057\n",
      "Epoch 6194  \tTraining Loss: 0.00013169453547328083\tValidation Loss: 0.0001341621333055984\n",
      "Epoch 6195  \tTraining Loss: 0.00013169149909695116\tValidation Loss: 0.00013415952398257365\n",
      "Epoch 6196  \tTraining Loss: 0.0001316884617494252\tValidation Loss: 0.00013415670589945043\n",
      "Epoch 6197  \tTraining Loss: 0.000131685426312368\tValidation Loss: 0.00013415368224696788\n",
      "Epoch 6198  \tTraining Loss: 0.00013168239254464885\tValidation Loss: 0.000134150955184371\n",
      "Epoch 6199  \tTraining Loss: 0.00013167935794412247\tValidation Loss: 0.00013414813346644257\n",
      "Epoch 6200  \tTraining Loss: 0.00013167632342108514\tValidation Loss: 0.00013414531643218632\n",
      "Epoch 6201  \tTraining Loss: 0.00013167329264836774\tValidation Loss: 0.0001341424442080514\n",
      "Epoch 6202  \tTraining Loss: 0.00013167026130078645\tValidation Loss: 0.00013413965189799945\n",
      "Epoch 6203  \tTraining Loss: 0.0001316672297168403\tValidation Loss: 0.00013413684170366806\n",
      "Epoch 6204  \tTraining Loss: 0.0001316641988470472\tValidation Loss: 0.0001341338985095669\n",
      "Epoch 6205  \tTraining Loss: 0.00013166117158874645\tValidation Loss: 0.00013413126880218\n",
      "Epoch 6206  \tTraining Loss: 0.00013165814252696914\tValidation Loss: 0.00013412839041202645\n",
      "Epoch 6207  \tTraining Loss: 0.0001316551140262807\tValidation Loss: 0.00013412556703104229\n",
      "Epoch 6208  \tTraining Loss: 0.000131652087699491\tValidation Loss: 0.00013412262377331412\n",
      "Epoch 6209  \tTraining Loss: 0.00013164906232067497\tValidation Loss: 0.00013411992198399225\n",
      "Epoch 6210  \tTraining Loss: 0.0001316460363073429\tValidation Loss: 0.00013411730739089714\n",
      "Epoch 6211  \tTraining Loss: 0.00013164301150814622\tValidation Loss: 0.00013411434728691576\n",
      "Epoch 6212  \tTraining Loss: 0.00013163998918469443\tValidation Loss: 0.00013411138717815858\n",
      "Epoch 6213  \tTraining Loss: 0.00013163696585854714\tValidation Loss: 0.00013410868335172874\n",
      "Epoch 6214  \tTraining Loss: 0.00013163394319626006\tValidation Loss: 0.00013410587694267825\n",
      "Epoch 6215  \tTraining Loss: 0.00013163092149151865\tValidation Loss: 0.00013410303428068278\n",
      "Epoch 6216  \tTraining Loss: 0.00013162790328920127\tValidation Loss: 0.00013410025781076526\n",
      "Epoch 6217  \tTraining Loss: 0.0001316248814894522\tValidation Loss: 0.00013409743813874175\n",
      "Epoch 6218  \tTraining Loss: 0.00013162186255952818\tValidation Loss: 0.0001340946383865683\n",
      "Epoch 6219  \tTraining Loss: 0.00013161884503298227\tValidation Loss: 0.00013409170782892544\n",
      "Epoch 6220  \tTraining Loss: 0.00013161582856595323\tValidation Loss: 0.00013408909138767187\n",
      "Epoch 6221  \tTraining Loss: 0.00013161281099776314\tValidation Loss: 0.00013408622638580998\n",
      "Epoch 6222  \tTraining Loss: 0.00013160979486510733\tValidation Loss: 0.00013408330202738632\n",
      "Epoch 6223  \tTraining Loss: 0.00013160678124590427\tValidation Loss: 0.00013408059623102575\n",
      "Epoch 6224  \tTraining Loss: 0.00013160376594052432\tValidation Loss: 0.00013407788892749026\n",
      "Epoch 6225  \tTraining Loss: 0.00013160075269077624\tValidation Loss: 0.00013407501315830254\n",
      "Epoch 6226  \tTraining Loss: 0.00013159774028044643\tValidation Loss: 0.0001340720889502996\n",
      "Epoch 6227  \tTraining Loss: 0.0001315947293293713\tValidation Loss: 0.00013406950351077887\n",
      "Epoch 6228  \tTraining Loss: 0.0001315917169003731\tValidation Loss: 0.00013406663743257896\n",
      "Epoch 6229  \tTraining Loss: 0.00013158870674982167\tValidation Loss: 0.00013406378404582405\n",
      "Epoch 6230  \tTraining Loss: 0.00013158569891097552\tValidation Loss: 0.00013406103065107796\n",
      "Epoch 6231  \tTraining Loss: 0.00013158268947677135\tValidation Loss: 0.00013405822518870443\n",
      "Epoch 6232  \tTraining Loss: 0.0001315796802242452\tValidation Loss: 0.00013405543336176783\n",
      "Epoch 6233  \tTraining Loss: 0.00013157667419098104\tValidation Loss: 0.00013405251385805432\n",
      "Epoch 6234  \tTraining Loss: 0.0001315736684319778\tValidation Loss: 0.00013404989044063072\n",
      "Epoch 6235  \tTraining Loss: 0.00013157066256415655\tValidation Loss: 0.0001340470543713776\n",
      "Epoch 6236  \tTraining Loss: 0.0001315676566579674\tValidation Loss: 0.00013404424242540876\n",
      "Epoch 6237  \tTraining Loss: 0.00013156465397433478\tValidation Loss: 0.0001340413397218385\n",
      "Epoch 6238  \tTraining Loss: 0.00013156165080450413\tValidation Loss: 0.00013403872327342724\n",
      "Epoch 6239  \tTraining Loss: 0.00013155864793046792\tValidation Loss: 0.0001340358872957108\n",
      "Epoch 6240  \tTraining Loss: 0.00013155564604856284\tValidation Loss: 0.00013403296419737357\n",
      "Epoch 6241  \tTraining Loss: 0.00013155264647481954\tValidation Loss: 0.00013403028757856815\n",
      "Epoch 6242  \tTraining Loss: 0.00013154964537302478\tValidation Loss: 0.00013402750633963282\n",
      "Epoch 6243  \tTraining Loss: 0.00013154664560966584\tValidation Loss: 0.00013402480307693492\n",
      "Epoch 6244  \tTraining Loss: 0.00013154364832546617\tValidation Loss: 0.0001340219449624279\n",
      "Epoch 6245  \tTraining Loss: 0.00013154065083328741\tValidation Loss: 0.00013401917308324346\n",
      "Epoch 6246  \tTraining Loss: 0.0001315376530596857\tValidation Loss: 0.0001340163833868601\n",
      "Epoch 6247  \tTraining Loss: 0.0001315346559209003\tValidation Loss: 0.00013401367939829938\n",
      "Epoch 6248  \tTraining Loss: 0.00013153166286406737\tValidation Loss: 0.00013401070523670419\n",
      "Epoch 6249  \tTraining Loss: 0.00013152866750754322\tValidation Loss: 0.00013400802266929715\n",
      "Epoch 6250  \tTraining Loss: 0.00013152567312846818\tValidation Loss: 0.00013400524535701235\n",
      "Epoch 6251  \tTraining Loss: 0.00013152267941520256\tValidation Loss: 0.0001340023591776565\n",
      "Epoch 6252  \tTraining Loss: 0.00013151968931158071\tValidation Loss: 0.00013399975537704606\n",
      "Epoch 6253  \tTraining Loss: 0.0001315166963122347\tValidation Loss: 0.0001339969130248584\n",
      "Epoch 6254  \tTraining Loss: 0.0001315137055072526\tValidation Loss: 0.00013399413313936283\n",
      "Epoch 6255  \tTraining Loss: 0.0001315107159590922\tValidation Loss: 0.00013399123208867193\n",
      "Epoch 6256  \tTraining Loss: 0.00013150772761039416\tValidation Loss: 0.00013398864514970434\n",
      "Epoch 6257  \tTraining Loss: 0.00013150473843565578\tValidation Loss: 0.0001339858098051058\n",
      "Epoch 6258  \tTraining Loss: 0.0001315017503724484\tValidation Loss: 0.0001339829167782903\n",
      "Epoch 6259  \tTraining Loss: 0.0001314987649766377\tValidation Loss: 0.00013398024080563402\n",
      "Epoch 6260  \tTraining Loss: 0.0001314957779137008\tValidation Loss: 0.00013397748917683708\n",
      "Epoch 6261  \tTraining Loss: 0.000131492792395874\tValidation Loss: 0.00013397478684985928\n",
      "Epoch 6262  \tTraining Loss: 0.00013148980836785853\tValidation Loss: 0.00013397184124025967\n",
      "Epoch 6263  \tTraining Loss: 0.00013148682549325673\tValidation Loss: 0.00013396927439338887\n",
      "Epoch 6264  \tTraining Loss: 0.00013148384148873771\tValidation Loss: 0.00013396643703091087\n",
      "Epoch 6265  \tTraining Loss: 0.00013148085897031635\tValidation Loss: 0.0001339637276619495\n",
      "Epoch 6266  \tTraining Loss: 0.00013147787858891228\tValidation Loss: 0.0001339607817835492\n",
      "Epoch 6267  \tTraining Loss: 0.0001314748984523476\tValidation Loss: 0.00013395809910367096\n",
      "Epoch 6268  \tTraining Loss: 0.0001314719171766039\tValidation Loss: 0.00013395535103329548\n",
      "Epoch 6269  \tTraining Loss: 0.00013146893809972156\tValidation Loss: 0.0001339524659382895\n",
      "Epoch 6270  \tTraining Loss: 0.00013146596140555883\tValidation Loss: 0.00013394989146495878\n",
      "Epoch 6271  \tTraining Loss: 0.0001314629834941796\tValidation Loss: 0.00013394706763458997\n",
      "Epoch 6272  \tTraining Loss: 0.00013146000533904354\tValidation Loss: 0.00013394429956132306\n",
      "Epoch 6273  \tTraining Loss: 0.00013145703019898814\tValidation Loss: 0.0001339414133325995\n",
      "Epoch 6274  \tTraining Loss: 0.00013145405555187279\tValidation Loss: 0.0001339388409980416\n",
      "Epoch 6275  \tTraining Loss: 0.0001314510811140229\tValidation Loss: 0.00013393602024382997\n",
      "Epoch 6276  \tTraining Loss: 0.0001314481058769628\tValidation Loss: 0.00013393325560498993\n",
      "Epoch 6277  \tTraining Loss: 0.00013144513456988467\tValidation Loss: 0.00013393037300922362\n",
      "Epoch 6278  \tTraining Loss: 0.0001314421620728839\tValidation Loss: 0.0001339278041967141\n",
      "Epoch 6279  \tTraining Loss: 0.00013143919106750433\tValidation Loss: 0.00013392498690526194\n",
      "Epoch 6280  \tTraining Loss: 0.00013143621891583557\tValidation Loss: 0.00013392209481618105\n",
      "Epoch 6281  \tTraining Loss: 0.00013143325145760834\tValidation Loss: 0.00013391945471848403\n",
      "Epoch 6282  \tTraining Loss: 0.0001314302808627206\tValidation Loss: 0.00013391670780984937\n",
      "Epoch 6283  \tTraining Loss: 0.00013142731257185706\tValidation Loss: 0.00013391403914543703\n",
      "Epoch 6284  \tTraining Loss: 0.00013142434531837494\tValidation Loss: 0.00013391109843090626\n",
      "Epoch 6285  \tTraining Loss: 0.00013142137962887153\tValidation Loss: 0.00013390856400146068\n",
      "Epoch 6286  \tTraining Loss: 0.00013141841251202073\tValidation Loss: 0.00013390573098128754\n",
      "Epoch 6287  \tTraining Loss: 0.00013141544682883352\tValidation Loss: 0.00013390305497928452\n",
      "Epoch 6288  \tTraining Loss: 0.00013141248347713893\tValidation Loss: 0.0001339001137130496\n",
      "Epoch 6289  \tTraining Loss: 0.00013140952019370143\tValidation Loss: 0.00013389746364058058\n",
      "Epoch 6290  \tTraining Loss: 0.0001314065561836717\tValidation Loss: 0.00013389471947812325\n",
      "Epoch 6291  \tTraining Loss: 0.00013140359337101412\tValidation Loss: 0.00013389205574958711\n",
      "Epoch 6292  \tTraining Loss: 0.00013140063386805727\tValidation Loss: 0.0001338891208292141\n",
      "Epoch 6293  \tTraining Loss: 0.0001313976729943431\tValidation Loss: 0.0001338864754181664\n",
      "Epoch 6294  \tTraining Loss: 0.00013139471218309435\tValidation Loss: 0.0001338837354089296\n",
      "Epoch 6295  \tTraining Loss: 0.0001313917525578509\tValidation Loss: 0.00013388088786700725\n",
      "Epoch 6296  \tTraining Loss: 0.0001313887962860341\tValidation Loss: 0.0001338783209219696\n",
      "Epoch 6297  \tTraining Loss: 0.0001313858375805267\tValidation Loss: 0.000133875533443331\n",
      "Epoch 6298  \tTraining Loss: 0.00013138288039999567\tValidation Loss: 0.00013387277278075065\n",
      "Epoch 6299  \tTraining Loss: 0.00013137992425703236\tValidation Loss: 0.00013386992371499162\n",
      "Epoch 6300  \tTraining Loss: 0.00013137697046705567\tValidation Loss: 0.00013386735861094375\n",
      "Epoch 6301  \tTraining Loss: 0.00013137401484745355\tValidation Loss: 0.00013386457403877035\n",
      "Epoch 6302  \tTraining Loss: 0.0001313710608403903\tValidation Loss: 0.00013386181665243682\n",
      "Epoch 6303  \tTraining Loss: 0.00013136810810491657\tValidation Loss: 0.0001338589711292121\n",
      "Epoch 6304  \tTraining Loss: 0.00013136515687466572\tValidation Loss: 0.00013385640943841128\n",
      "Epoch 6305  \tTraining Loss: 0.00013136220432133786\tValidation Loss: 0.0001338536282766219\n",
      "Epoch 6306  \tTraining Loss: 0.0001313592534991526\tValidation Loss: 0.00013385076230649693\n",
      "Epoch 6307  \tTraining Loss: 0.0001313563046962568\tValidation Loss: 0.00013384814028945775\n",
      "Epoch 6308  \tTraining Loss: 0.00013135335509193672\tValidation Loss: 0.00013384548962308592\n",
      "Epoch 6309  \tTraining Loss: 0.0001313504059449632\tValidation Loss: 0.0001338426985791608\n",
      "Epoch 6310  \tTraining Loss: 0.0001313474588004721\tValidation Loss: 0.00013383994961645138\n",
      "Epoch 6311  \tTraining Loss: 0.0001313445125075095\tValidation Loss: 0.00013383724384637407\n",
      "Epoch 6312  \tTraining Loss: 0.0001313415659533881\tValidation Loss: 0.00013383451154622502\n",
      "Epoch 6313  \tTraining Loss: 0.00013133861962773348\tValidation Loss: 0.0001338318646437369\n",
      "Epoch 6314  \tTraining Loss: 0.00013133567630857972\tValidation Loss: 0.00013382894779059162\n",
      "Epoch 6315  \tTraining Loss: 0.00013133273234753131\tValidation Loss: 0.00013382632001325006\n",
      "Epoch 6316  \tTraining Loss: 0.00013132978894820813\tValidation Loss: 0.0001338235979283725\n",
      "Epoch 6317  \tTraining Loss: 0.00013132684554894444\tValidation Loss: 0.00013382093808396594\n",
      "Epoch 6318  \tTraining Loss: 0.0001313239055683422\tValidation Loss: 0.00013381804351088946\n",
      "Epoch 6319  \tTraining Loss: 0.00013132096450978466\tValidation Loss: 0.0001338154057354513\n",
      "Epoch 6320  \tTraining Loss: 0.00013131802372290085\tValidation Loss: 0.0001338127023921385\n",
      "Epoch 6321  \tTraining Loss: 0.0001313150838003714\tValidation Loss: 0.00013381005011558374\n",
      "Epoch 6322  \tTraining Loss: 0.000131312146960811\tValidation Loss: 0.00013380715593716116\n",
      "Epoch 6323  \tTraining Loss: 0.00013130920874302425\tValidation Loss: 0.00013380452088221614\n",
      "Epoch 6324  \tTraining Loss: 0.0001313062708566627\tValidation Loss: 0.00013380182058936348\n",
      "Epoch 6325  \tTraining Loss: 0.00013130333423604936\tValidation Loss: 0.00013379906008830855\n",
      "Epoch 6326  \tTraining Loss: 0.00013130040091404685\tValidation Loss: 0.0001337963882526473\n",
      "Epoch 6327  \tTraining Loss: 0.00013129746458693799\tValidation Loss: 0.0001337936642415969\n",
      "Epoch 6328  \tTraining Loss: 0.00013129453005270538\tValidation Loss: 0.0001337909540453757\n",
      "Epoch 6329  \tTraining Loss: 0.00013129159690027098\tValidation Loss: 0.00013378819404824102\n",
      "Epoch 6330  \tTraining Loss: 0.00013128866616025038\tValidation Loss: 0.0001337855248750446\n",
      "Epoch 6331  \tTraining Loss: 0.00013128573292093578\tValidation Loss: 0.00013378280424626145\n",
      "Epoch 6332  \tTraining Loss: 0.00013128280133974747\tValidation Loss: 0.00013378009762300003\n",
      "Epoch 6333  \tTraining Loss: 0.00013127987156217626\tValidation Loss: 0.0001337773413388191\n",
      "Epoch 6334  \tTraining Loss: 0.00013127694347149868\tValidation Loss: 0.00013377467560632395\n",
      "Epoch 6335  \tTraining Loss: 0.0001312740132997489\tValidation Loss: 0.0001337719584300966\n",
      "Epoch 6336  \tTraining Loss: 0.0001312710846727944\tValidation Loss: 0.00013376925525993184\n",
      "Epoch 6337  \tTraining Loss: 0.00013126815824118727\tValidation Loss: 0.0001337665025396755\n",
      "Epoch 6338  \tTraining Loss: 0.000131265232818628\tValidation Loss: 0.00013376384011856146\n",
      "Epoch 6339  \tTraining Loss: 0.00013126230570026961\tValidation Loss: 0.00013376112629367567\n",
      "Epoch 6340  \tTraining Loss: 0.0001312593800306497\tValidation Loss: 0.00013375842650019935\n",
      "Epoch 6341  \tTraining Loss: 0.0001312564569807487\tValidation Loss: 0.0001337557175452453\n",
      "Epoch 6342  \tTraining Loss: 0.00013125353402133622\tValidation Loss: 0.00013375311675372395\n",
      "Epoch 6343  \tTraining Loss: 0.00013125061039191406\tValidation Loss: 0.0001337503431250575\n",
      "Epoch 6344  \tTraining Loss: 0.00013124768752638484\tValidation Loss: 0.0001337475212759242\n",
      "Epoch 6345  \tTraining Loss: 0.00013124476792761492\tValidation Loss: 0.00013374491409571623\n",
      "Epoch 6346  \tTraining Loss: 0.0001312418467571857\tValidation Loss: 0.00013374230882316917\n",
      "Epoch 6347  \tTraining Loss: 0.00013123892671554718\tValidation Loss: 0.00013373953361508827\n",
      "Epoch 6348  \tTraining Loss: 0.00013123600721965395\tValidation Loss: 0.00013373671480071994\n",
      "Epoch 6349  \tTraining Loss: 0.0001312330902610888\tValidation Loss: 0.00013373411139776579\n",
      "Epoch 6350  \tTraining Loss: 0.0001312301720540329\tValidation Loss: 0.0001337315101642963\n",
      "Epoch 6351  \tTraining Loss: 0.00013122725504486715\tValidation Loss: 0.00013372873882942305\n",
      "Epoch 6352  \tTraining Loss: 0.0001312243388067204\tValidation Loss: 0.00013372592389993823\n",
      "Epoch 6353  \tTraining Loss: 0.00013122142456152864\tValidation Loss: 0.00013372332406380772\n",
      "Epoch 6354  \tTraining Loss: 0.00013121850931263206\tValidation Loss: 0.0001337207263969408\n",
      "Epoch 6355  \tTraining Loss: 0.00013121559531834351\tValidation Loss: 0.00013371795849854347\n",
      "Epoch 6356  \tTraining Loss: 0.0001312126823134455\tValidation Loss: 0.0001337151471168294\n",
      "Epoch 6357  \tTraining Loss: 0.000131209770794635\tValidation Loss: 0.0001337125506037421\n",
      "Epoch 6358  \tTraining Loss: 0.00013120685851377998\tValidation Loss: 0.00013370995633209297\n",
      "Epoch 6359  \tTraining Loss: 0.00013120394751317164\tValidation Loss: 0.00013370719175156775\n",
      "Epoch 6360  \tTraining Loss: 0.00013120103772312248\tValidation Loss: 0.0001337043838343347\n",
      "Epoch 6361  \tTraining Loss: 0.00013119812893881835\tValidation Loss: 0.0001337017905855897\n",
      "Epoch 6362  \tTraining Loss: 0.00013119521963755832\tValidation Loss: 0.0001336991996656539\n",
      "Epoch 6363  \tTraining Loss: 0.00013119231160872236\tValidation Loss: 0.000133696438371306\n",
      "Epoch 6364  \tTraining Loss: 0.00013118940501670264\tValidation Loss: 0.00013369363389328668\n",
      "Epoch 6365  \tTraining Loss: 0.00013118649897372298\tValidation Loss: 0.0001336910438890065\n",
      "Epoch 6366  \tTraining Loss: 0.00013118359266378054\tValidation Loss: 0.00013368845630307743\n",
      "Epoch 6367  \tTraining Loss: 0.00013118068758485597\tValidation Loss: 0.00013368569828047777\n",
      "Epoch 6368  \tTraining Loss: 0.00013117778417474763\tValidation Loss: 0.00013368289722796379\n",
      "Epoch 6369  \tTraining Loss: 0.00013117488087929124\tValidation Loss: 0.00013368031045613005\n",
      "Epoch 6370  \tTraining Loss: 0.000131171977572267\tValidation Loss: 0.00013367772619190396\n",
      "Epoch 6371  \tTraining Loss: 0.00013116907552798235\tValidation Loss: 0.0001336748612153497\n",
      "Epoch 6372  \tTraining Loss: 0.00013116617562517114\tValidation Loss: 0.00013367228064253123\n",
      "Epoch 6373  \tTraining Loss: 0.00013116327424740978\tValidation Loss: 0.00013366968139079096\n",
      "Epoch 6374  \tTraining Loss: 0.00013116037441356977\tValidation Loss: 0.000133666942457556\n",
      "Epoch 6375  \tTraining Loss: 0.00013115747527953292\tValidation Loss: 0.00013366413162979265\n",
      "Epoch 6376  \tTraining Loss: 0.00013115457825305904\tValidation Loss: 0.00013366156465135687\n",
      "Epoch 6377  \tTraining Loss: 0.0001311516798818128\tValidation Loss: 0.00013365897089915037\n",
      "Epoch 6378  \tTraining Loss: 0.0001311487829765857\tValidation Loss: 0.0001336562364322103\n",
      "Epoch 6379  \tTraining Loss: 0.00013114588694810005\tValidation Loss: 0.00013365342975099024\n",
      "Epoch 6380  \tTraining Loss: 0.00013114299272451923\tValidation Loss: 0.00013365086650895128\n",
      "Epoch 6381  \tTraining Loss: 0.0001311400973269207\tValidation Loss: 0.00013364827637849574\n",
      "Epoch 6382  \tTraining Loss: 0.0001311372033560401\tValidation Loss: 0.00013364554539038829\n",
      "Epoch 6383  \tTraining Loss: 0.00013113431041336513\tValidation Loss: 0.00013364285762068517\n",
      "Epoch 6384  \tTraining Loss: 0.0001311314190700185\tValidation Loss: 0.0001336402119389057\n",
      "Epoch 6385  \tTraining Loss: 0.00013112852662812386\tValidation Loss: 0.0001336376156451722\n",
      "Epoch 6386  \tTraining Loss: 0.00013112563555083372\tValidation Loss: 0.00013363488451015152\n",
      "Epoch 6387  \tTraining Loss: 0.00013112274557631104\tValidation Loss: 0.00013363208299065887\n",
      "Epoch 6388  \tTraining Loss: 0.00013111985707750417\tValidation Loss: 0.00013362952516776726\n",
      "Epoch 6389  \tTraining Loss: 0.0001311169676638362\tValidation Loss: 0.00013362694087899204\n",
      "Epoch 6390  \tTraining Loss: 0.00013111407949275802\tValidation Loss: 0.00013362421586541053\n",
      "Epoch 6391  \tTraining Loss: 0.00013111119253388472\tValidation Loss: 0.0001336214190337621\n",
      "Epoch 6392  \tTraining Loss: 0.00013110830689901852\tValidation Loss: 0.00013361886517781392\n",
      "Epoch 6393  \tTraining Loss: 0.0001311054204484569\tValidation Loss: 0.0001336162846086726\n",
      "Epoch 6394  \tTraining Loss: 0.00013110253519232854\tValidation Loss: 0.000133613563115561\n",
      "Epoch 6395  \tTraining Loss: 0.00013109965121762312\tValidation Loss: 0.00013361076981959773\n",
      "Epoch 6396  \tTraining Loss: 0.00013109676846588612\tValidation Loss: 0.0001336082192749336\n",
      "Epoch 6397  \tTraining Loss: 0.0001310938849738238\tValidation Loss: 0.000133605642031571\n",
      "Epoch 6398  \tTraining Loss: 0.000131091002627236\tValidation Loss: 0.00013360292381604203\n",
      "Epoch 6399  \tTraining Loss: 0.00013108812160804887\tValidation Loss: 0.00013360013390247905\n",
      "Epoch 6400  \tTraining Loss: 0.0001310852417842307\tValidation Loss: 0.00013359766276109037\n",
      "Epoch 6401  \tTraining Loss: 0.0001310823613562387\tValidation Loss: 0.0001335949434436614\n",
      "Epoch 6402  \tTraining Loss: 0.00013107948161959567\tValidation Loss: 0.0001335922823067757\n",
      "Epoch 6403  \tTraining Loss: 0.0001310766036784418\tValidation Loss: 0.0001335895067153911\n",
      "Epoch 6404  \tTraining Loss: 0.00013107372691287424\tValidation Loss: 0.0001335870409330184\n",
      "Epoch 6405  \tTraining Loss: 0.000131070849193496\tValidation Loss: 0.00013358432580912752\n",
      "Epoch 6406  \tTraining Loss: 0.00013106797246591382\tValidation Loss: 0.00013358166847134286\n",
      "Epoch 6407  \tTraining Loss: 0.00013106509742152178\tValidation Loss: 0.00013357889657245403\n",
      "Epoch 6408  \tTraining Loss: 0.00013106222371583165\tValidation Loss: 0.00013357643421459898\n",
      "Epoch 6409  \tTraining Loss: 0.00013105934871255682\tValidation Loss: 0.0001335737224165393\n",
      "Epoch 6410  \tTraining Loss: 0.00013105647498861073\tValidation Loss: 0.0001335710683990609\n",
      "Epoch 6411  \tTraining Loss: 0.00013105360281467904\tValidation Loss: 0.0001335682998877159\n",
      "Epoch 6412  \tTraining Loss: 0.00013105073219138146\tValidation Loss: 0.00013356584075739292\n",
      "Epoch 6413  \tTraining Loss: 0.00013104785988838114\tValidation Loss: 0.00013356313215595505\n",
      "Epoch 6414  \tTraining Loss: 0.00013104498916856132\tValidation Loss: 0.00013356048137326814\n",
      "Epoch 6415  \tTraining Loss: 0.0001310421198383438\tValidation Loss: 0.00013355771619158563\n",
      "Epoch 6416  \tTraining Loss: 0.00013103925232156049\tValidation Loss: 0.00013355526024919686\n",
      "Epoch 6417  \tTraining Loss: 0.00013103638270080386\tValidation Loss: 0.00013355255481708125\n",
      "Epoch 6418  \tTraining Loss: 0.00013103351498650195\tValidation Loss: 0.0001335499072493953\n",
      "Epoch 6419  \tTraining Loss: 0.00013103064856038438\tValidation Loss: 0.00013354722175204324\n",
      "Epoch 6420  \tTraining Loss: 0.00013102778421313017\tValidation Loss: 0.000133544623312187\n",
      "Epoch 6421  \tTraining Loss: 0.00013102491690750788\tValidation Loss: 0.0001335419750216032\n",
      "Epoch 6422  \tTraining Loss: 0.00013102205241470437\tValidation Loss: 0.00013353934165241535\n",
      "Epoch 6423  \tTraining Loss: 0.00013101918895164894\tValidation Loss: 0.0001335366616308475\n",
      "Epoch 6424  \tTraining Loss: 0.00013101632744883323\tValidation Loss: 0.0001335340673140989\n",
      "Epoch 6425  \tTraining Loss: 0.00013101346292897602\tValidation Loss: 0.00013353142275789894\n",
      "Epoch 6426  \tTraining Loss: 0.00013101060145074414\tValidation Loss: 0.0001335287929420137\n",
      "Epoch 6427  \tTraining Loss: 0.00013100774091063573\tValidation Loss: 0.00013352611646282153\n",
      "Epoch 6428  \tTraining Loss: 0.00013100488227632125\tValidation Loss: 0.00013352352540580575\n",
      "Epoch 6429  \tTraining Loss: 0.00013100202054593552\tValidation Loss: 0.00013352090293687957\n",
      "Epoch 6430  \tTraining Loss: 0.00013099916195176143\tValidation Loss: 0.0001335183340420247\n",
      "Epoch 6431  \tTraining Loss: 0.00013099630481256362\tValidation Loss: 0.0001335155293086351\n",
      "Epoch 6432  \tTraining Loss: 0.00013099344800691928\tValidation Loss: 0.0001335129800051629\n",
      "Epoch 6433  \tTraining Loss: 0.00013099059005872155\tValidation Loss: 0.00013351036768244025\n",
      "Epoch 6434  \tTraining Loss: 0.00013098773410066838\tValidation Loss: 0.000133507807574149\n",
      "Epoch 6435  \tTraining Loss: 0.00013098487986502581\tValidation Loss: 0.00013350500777066548\n",
      "Epoch 6436  \tTraining Loss: 0.0001309820258004781\tValidation Loss: 0.00013350246253038993\n",
      "Epoch 6437  \tTraining Loss: 0.00013097917090495288\tValidation Loss: 0.00013349985397509694\n",
      "Epoch 6438  \tTraining Loss: 0.00013097631786818112\tValidation Loss: 0.00013349729745104478\n",
      "Epoch 6439  \tTraining Loss: 0.00013097346641504284\tValidation Loss: 0.0001334945011740821\n",
      "Epoch 6440  \tTraining Loss: 0.00013097061513926795\tValidation Loss: 0.00013349195920262226\n",
      "Epoch 6441  \tTraining Loss: 0.00013096776333151262\tValidation Loss: 0.00013348943040766373\n",
      "Epoch 6442  \tTraining Loss: 0.0001309649133177813\tValidation Loss: 0.00013348673119139217\n",
      "Epoch 6443  \tTraining Loss: 0.00013096206436596976\tValidation Loss: 0.0001334839922897843\n",
      "Epoch 6444  \tTraining Loss: 0.00013095921597460007\tValidation Loss: 0.00013348146444501166\n",
      "Epoch 6445  \tTraining Loss: 0.00013095636743594037\tValidation Loss: 0.00013347894097152287\n",
      "Epoch 6446  \tTraining Loss: 0.00013095351990739177\tValidation Loss: 0.00013347624588785703\n",
      "Epoch 6447  \tTraining Loss: 0.00013095067390444926\tValidation Loss: 0.0001334735108133464\n",
      "Epoch 6448  \tTraining Loss: 0.00013094782830999898\tValidation Loss: 0.0001334709864070031\n",
      "Epoch 6449  \tTraining Loss: 0.00013094498301694104\tValidation Loss: 0.00013346846631159757\n",
      "Epoch 6450  \tTraining Loss: 0.00013094213798419736\tValidation Loss: 0.00013346577448858384\n",
      "Epoch 6451  \tTraining Loss: 0.00013093929490813567\tValidation Loss: 0.00013346304276213783\n",
      "Epoch 6452  \tTraining Loss: 0.00013093645216966088\tValidation Loss: 0.00013346059805218984\n",
      "Epoch 6453  \tTraining Loss: 0.00013093361012301585\tValidation Loss: 0.0001334579350578297\n",
      "Epoch 6454  \tTraining Loss: 0.00013093076730237224\tValidation Loss: 0.00013345530053671174\n",
      "Epoch 6455  \tTraining Loss: 0.000130927927355723\tValidation Loss: 0.00013345258309990882\n",
      "Epoch 6456  \tTraining Loss: 0.00013092508767005828\tValidation Loss: 0.00013345014363521374\n",
      "Epoch 6457  \tTraining Loss: 0.00013092224841352952\tValidation Loss: 0.0001334474847532978\n",
      "Epoch 6458  \tTraining Loss: 0.00013091940827838792\tValidation Loss: 0.00013344485392248785\n",
      "Epoch 6459  \tTraining Loss: 0.0001309165712380764\tValidation Loss: 0.00013344214008713913\n",
      "Epoch 6460  \tTraining Loss: 0.00013091373463266828\tValidation Loss: 0.00013343972290783015\n",
      "Epoch 6461  \tTraining Loss: 0.00013091089809463594\tValidation Loss: 0.00013343704833943773\n",
      "Epoch 6462  \tTraining Loss: 0.00013090806097547632\tValidation Loss: 0.00013343443516461589\n",
      "Epoch 6463  \tTraining Loss: 0.0001309052264232586\tValidation Loss: 0.00013343178571797285\n",
      "Epoch 6464  \tTraining Loss: 0.0001309023935312914\tValidation Loss: 0.00013342922125164983\n",
      "Epoch 6465  \tTraining Loss: 0.00013089955872478554\tValidation Loss: 0.00013342660750364404\n",
      "Epoch 6466  \tTraining Loss: 0.00013089672498919718\tValidation Loss: 0.00013342400916487235\n",
      "Epoch 6467  \tTraining Loss: 0.00013089389318216065\tValidation Loss: 0.0001334213655028745\n",
      "Epoch 6468  \tTraining Loss: 0.00013089106322824502\tValidation Loss: 0.00013341880534358918\n",
      "Epoch 6469  \tTraining Loss: 0.00013088823105895627\tValidation Loss: 0.0001334161954380228\n",
      "Epoch 6470  \tTraining Loss: 0.00013088540055087244\tValidation Loss: 0.00013341367740866562\n",
      "Epoch 6471  \tTraining Loss: 0.00013088257135470896\tValidation Loss: 0.00013341089088281206\n",
      "Epoch 6472  \tTraining Loss: 0.0001308797440936445\tValidation Loss: 0.00013340838826880936\n",
      "Epoch 6473  \tTraining Loss: 0.00013087691476257822\tValidation Loss: 0.00013340579261085842\n",
      "Epoch 6474  \tTraining Loss: 0.0001308740876085117\tValidation Loss: 0.00013340327992680375\n",
      "Epoch 6475  \tTraining Loss: 0.00013087126056076642\tValidation Loss: 0.00013340049763764836\n",
      "Epoch 6476  \tTraining Loss: 0.00013086843653608093\tValidation Loss: 0.00013339799866259568\n",
      "Epoch 6477  \tTraining Loss: 0.00013086560994779355\tValidation Loss: 0.0001333954831967742\n",
      "Epoch 6478  \tTraining Loss: 0.00013086278604033485\tValidation Loss: 0.0001333928274280692\n",
      "Epoch 6479  \tTraining Loss: 0.00013085996105587343\tValidation Loss: 0.0001333902100946949\n",
      "Epoch 6480  \tTraining Loss: 0.00013085713956540845\tValidation Loss: 0.00013338751288477282\n",
      "Epoch 6481  \tTraining Loss: 0.00013085431703507055\tValidation Loss: 0.00013338509320823225\n",
      "Epoch 6482  \tTraining Loss: 0.00013085149552754881\tValidation Loss: 0.0001333824542420482\n",
      "Epoch 6483  \tTraining Loss: 0.0001308486731337366\tValidation Loss: 0.00013337984338377028\n",
      "Epoch 6484  \tTraining Loss: 0.00013084585450185194\tValidation Loss: 0.00013337722718111063\n",
      "Epoch 6485  \tTraining Loss: 0.00013084303536311064\tValidation Loss: 0.00013337468313240987\n",
      "Epoch 6486  \tTraining Loss: 0.00013084021598320516\tValidation Loss: 0.00013337208241457715\n",
      "Epoch 6487  \tTraining Loss: 0.000130837396883096\tValidation Loss: 0.00013336950012557213\n",
      "Epoch 6488  \tTraining Loss: 0.0001308345806111995\tValidation Loss: 0.0001333668734076281\n",
      "Epoch 6489  \tTraining Loss: 0.0001308317649121607\tValidation Loss: 0.00013336432920960285\n",
      "Epoch 6490  \tTraining Loss: 0.00013082894772082948\tValidation Loss: 0.00013336173556584112\n",
      "Epoch 6491  \tTraining Loss: 0.00013082613193767876\tValidation Loss: 0.00013335923420668613\n",
      "Epoch 6492  \tTraining Loss: 0.00013082331824302234\tValidation Loss: 0.00013335646455751334\n",
      "Epoch 6493  \tTraining Loss: 0.0001308205052475901\tValidation Loss: 0.0001333539781695205\n",
      "Epoch 6494  \tTraining Loss: 0.00013081769082490816\tValidation Loss: 0.00013335139889043864\n",
      "Epoch 6495  \tTraining Loss: 0.00013081487846842974\tValidation Loss: 0.00013334890292532913\n",
      "Epoch 6496  \tTraining Loss: 0.00013081206671828683\tValidation Loss: 0.00013334613753610287\n",
      "Epoch 6497  \tTraining Loss: 0.00013080925706172796\tValidation Loss: 0.00013334365478709245\n",
      "Epoch 6498  \tTraining Loss: 0.00013080644536070523\tValidation Loss: 0.00013334115583823566\n",
      "Epoch 6499  \tTraining Loss: 0.00013080363623100026\tValidation Loss: 0.00013333851646485563\n",
      "Epoch 6500  \tTraining Loss: 0.00013080082627478886\tValidation Loss: 0.0001333358088010694\n",
      "Epoch 6501  \tTraining Loss: 0.00013079802011311192\tValidation Loss: 0.00013333334022317116\n",
      "Epoch 6502  \tTraining Loss: 0.0001307952113274113\tValidation Loss: 0.0001333308465269677\n",
      "Epoch 6503  \tTraining Loss: 0.0001307924049104397\tValidation Loss: 0.00013332821123405477\n",
      "Epoch 6504  \tTraining Loss: 0.00013078959728914072\tValidation Loss: 0.00013332561395907195\n",
      "Epoch 6505  \tTraining Loss: 0.00013078679389421917\tValidation Loss: 0.0001333230144301622\n",
      "Epoch 6506  \tTraining Loss: 0.00013078398912532436\tValidation Loss: 0.00013332048649017253\n",
      "Epoch 6507  \tTraining Loss: 0.00013078118446104567\tValidation Loss: 0.00013331790223289924\n",
      "Epoch 6508  \tTraining Loss: 0.00013077838019365194\tValidation Loss: 0.00013331533660471066\n",
      "Epoch 6509  \tTraining Loss: 0.00013077557895294428\tValidation Loss: 0.00013331272711902722\n",
      "Epoch 6510  \tTraining Loss: 0.00013077277773889144\tValidation Loss: 0.00013331019904120535\n",
      "Epoch 6511  \tTraining Loss: 0.00013076997519900516\tValidation Loss: 0.00013330762171464658\n",
      "Epoch 6512  \tTraining Loss: 0.0001307671744683922\tValidation Loss: 0.00013330513697442102\n",
      "Epoch 6513  \tTraining Loss: 0.00013076437529254816\tValidation Loss: 0.000133302384118434\n",
      "Epoch 6514  \tTraining Loss: 0.00013076157703790744\tValidation Loss: 0.00013329991383888035\n",
      "Epoch 6515  \tTraining Loss: 0.00013075877728112187\tValidation Loss: 0.00013329742782068282\n",
      "Epoch 6516  \tTraining Loss: 0.00013075598003980447\tValidation Loss: 0.00013329480141663079\n",
      "Epoch 6517  \tTraining Loss: 0.00013075318237927252\tValidation Loss: 0.0001332921072821439\n",
      "Epoch 6518  \tTraining Loss: 0.00013075038770861628\tValidation Loss: 0.00013328965167306027\n",
      "Epoch 6519  \tTraining Loss: 0.00013074759085817536\tValidation Loss: 0.00013328717119601963\n",
      "Epoch 6520  \tTraining Loss: 0.00013074479629943434\tValidation Loss: 0.000133284549046899\n",
      "Epoch 6521  \tTraining Loss: 0.00013074200077997243\tValidation Loss: 0.00013328185874703674\n",
      "Epoch 6522  \tTraining Loss: 0.00013073920973165242\tValidation Loss: 0.00013327948365800967\n",
      "Epoch 6523  \tTraining Loss: 0.0001307364155585421\tValidation Loss: 0.0001332768786399655\n",
      "Epoch 6524  \tTraining Loss: 0.00013073362337587608\tValidation Loss: 0.00013327429489472597\n",
      "Epoch 6525  \tTraining Loss: 0.00013073083094288768\tValidation Loss: 0.0001332717397544684\n",
      "Epoch 6526  \tTraining Loss: 0.0001307280418190977\tValidation Loss: 0.00013326914348826049\n",
      "Epoch 6527  \tTraining Loss: 0.00013072525219875306\tValidation Loss: 0.00013326662839429845\n",
      "Epoch 6528  \tTraining Loss: 0.0001307224615283676\tValidation Loss: 0.00013326406436516404\n",
      "Epoch 6529  \tTraining Loss: 0.00013071967268797483\tValidation Loss: 0.00013326159318936622\n",
      "Epoch 6530  \tTraining Loss: 0.00013071688541568314\tValidation Loss: 0.00013325885399042748\n",
      "Epoch 6531  \tTraining Loss: 0.0001307140988714491\tValidation Loss: 0.0001332563967888011\n",
      "Epoch 6532  \tTraining Loss: 0.00013071131102212027\tValidation Loss: 0.0001332539240319117\n",
      "Epoch 6533  \tTraining Loss: 0.00013070852554225996\tValidation Loss: 0.00013325131077573753\n",
      "Epoch 6534  \tTraining Loss: 0.0001307057397195219\tValidation Loss: 0.00013324863023413016\n",
      "Epoch 6535  \tTraining Loss: 0.00013070295683643014\tValidation Loss: 0.00013324618759254475\n",
      "Epoch 6536  \tTraining Loss: 0.00013070017188398397\tValidation Loss: 0.00013324372027704166\n",
      "Epoch 6537  \tTraining Loss: 0.00013069738905765394\tValidation Loss: 0.00013324111120154632\n",
      "Epoch 6538  \tTraining Loss: 0.00013069460526454724\tValidation Loss: 0.00013323843443385352\n",
      "Epoch 6539  \tTraining Loss: 0.00013069182623292396\tValidation Loss: 0.00013323607237779675\n",
      "Epoch 6540  \tTraining Loss: 0.00013068904370839185\tValidation Loss: 0.00013323348033252285\n",
      "Epoch 6541  \tTraining Loss: 0.00013068626324276074\tValidation Loss: 0.00013323090960337976\n",
      "Epoch 6542  \tTraining Loss: 0.0001306834828310403\tValidation Loss: 0.00013322844486485027\n",
      "Epoch 6543  \tTraining Loss: 0.00013068070538396466\tValidation Loss: 0.00013322571481071634\n",
      "Epoch 6544  \tTraining Loss: 0.000130677927257534\tValidation Loss: 0.00013322326721633415\n",
      "Epoch 6545  \tTraining Loss: 0.00013067514845856967\tValidation Loss: 0.00013322072724824703\n",
      "Epoch 6546  \tTraining Loss: 0.00013067237174590107\tValidation Loss: 0.000133218271438365\n",
      "Epoch 6547  \tTraining Loss: 0.00013066959481195206\tValidation Loss: 0.0001332155333931322\n",
      "Epoch 6548  \tTraining Loss: 0.0001306668181326355\tValidation Loss: 0.00013321308838287312\n",
      "Epoch 6549  \tTraining Loss: 0.00013066404005306374\tValidation Loss: 0.0001332106308705858\n",
      "Epoch 6550  \tTraining Loss: 0.0001306612640795877\tValidation Loss: 0.00013320803440591323\n",
      "Epoch 6551  \tTraining Loss: 0.00013065848748969067\tValidation Loss: 0.00013320537184059554\n",
      "Epoch 6552  \tTraining Loss: 0.00013065571481700653\tValidation Loss: 0.00013320302426647238\n",
      "Epoch 6553  \tTraining Loss: 0.00013065293944120488\tValidation Loss: 0.00013320042755779642\n",
      "Epoch 6554  \tTraining Loss: 0.00013065016591904142\tValidation Loss: 0.00013319789125649015\n",
      "Epoch 6555  \tTraining Loss: 0.00013064739178276306\tValidation Loss: 0.0001331954461573788\n",
      "Epoch 6556  \tTraining Loss: 0.0001306446218499545\tValidation Loss: 0.00013319272801492313\n",
      "Epoch 6557  \tTraining Loss: 0.0001306418502112603\tValidation Loss: 0.0001331902947864014\n",
      "Epoch 6558  \tTraining Loss: 0.0001306390788708699\tValidation Loss: 0.00013318779832082332\n",
      "Epoch 6559  \tTraining Loss: 0.00013063631098992907\tValidation Loss: 0.00013318535604203518\n",
      "Epoch 6560  \tTraining Loss: 0.00013063354450407576\tValidation Loss: 0.00013318264208827334\n",
      "Epoch 6561  \tTraining Loss: 0.0001306307791187326\tValidation Loss: 0.00013318020916081234\n",
      "Epoch 6562  \tTraining Loss: 0.00013062801284222396\tValidation Loss: 0.00013317776130622024\n",
      "Epoch 6563  \tTraining Loss: 0.0001306252479755893\tValidation Loss: 0.00013317517296825248\n",
      "Epoch 6564  \tTraining Loss: 0.00013062248302609948\tValidation Loss: 0.0001331726234142443\n",
      "Epoch 6565  \tTraining Loss: 0.0001306197206702216\tValidation Loss: 0.00013317007425293193\n",
      "Epoch 6566  \tTraining Loss: 0.0001306169586224388\tValidation Loss: 0.0001331675741585773\n",
      "Epoch 6567  \tTraining Loss: 0.0001306141955919845\tValidation Loss: 0.00013316505748512913\n",
      "Epoch 6568  \tTraining Loss: 0.00013061143334007908\tValidation Loss: 0.00013316252554814408\n",
      "Epoch 6569  \tTraining Loss: 0.00013060867354856942\tValidation Loss: 0.0001331599825662966\n",
      "Epoch 6570  \tTraining Loss: 0.0001306059144385945\tValidation Loss: 0.00013315748650961798\n",
      "Epoch 6571  \tTraining Loss: 0.00013060315434096457\tValidation Loss: 0.00013315497324057233\n",
      "Epoch 6572  \tTraining Loss: 0.00013060039506730236\tValidation Loss: 0.00013315251888407792\n",
      "Epoch 6573  \tTraining Loss: 0.00013059763719143323\tValidation Loss: 0.00013314993682350521\n",
      "Epoch 6574  \tTraining Loss: 0.0001305948793002328\tValidation Loss: 0.00013314728911122356\n",
      "Epoch 6575  \tTraining Loss: 0.0001305921237024787\tValidation Loss: 0.00013314495544783936\n",
      "Epoch 6576  \tTraining Loss: 0.00013058936629067517\tValidation Loss: 0.0001331423721241089\n",
      "Epoch 6577  \tTraining Loss: 0.0001305866107056037\tValidation Loss: 0.00013313984922239756\n",
      "Epoch 6578  \tTraining Loss: 0.00013058385461859983\tValidation Loss: 0.00013313729335903576\n",
      "Epoch 6579  \tTraining Loss: 0.00013058110355651143\tValidation Loss: 0.0001331348165885167\n",
      "Epoch 6580  \tTraining Loss: 0.00013057834780067412\tValidation Loss: 0.00013313231129720345\n",
      "Epoch 6581  \tTraining Loss: 0.00013057559517920411\tValidation Loss: 0.00013312978326712388\n",
      "Epoch 6582  \tTraining Loss: 0.0001305728425448892\tValidation Loss: 0.00013312735258282505\n",
      "Epoch 6583  \tTraining Loss: 0.00013057009182806438\tValidation Loss: 0.00013312465560088568\n",
      "Epoch 6584  \tTraining Loss: 0.00013056734149551454\tValidation Loss: 0.00013312223949147288\n",
      "Epoch 6585  \tTraining Loss: 0.00013056459085076714\tValidation Loss: 0.00013311980854296596\n",
      "Epoch 6586  \tTraining Loss: 0.0001305618411568516\tValidation Loss: 0.00013311723676657562\n",
      "Epoch 6587  \tTraining Loss: 0.00013055909183621635\tValidation Loss: 0.00013311470360333818\n",
      "Epoch 6588  \tTraining Loss: 0.0001305563444481109\tValidation Loss: 0.00013311217164008212\n",
      "Epoch 6589  \tTraining Loss: 0.00013055359814908793\tValidation Loss: 0.00013310968734269325\n",
      "Epoch 6590  \tTraining Loss: 0.00013055085022049614\tValidation Loss: 0.00013310718695334623\n",
      "Epoch 6591  \tTraining Loss: 0.00013054810358379063\tValidation Loss: 0.0001331047488664193\n",
      "Epoch 6592  \tTraining Loss: 0.0001305453587494185\tValidation Loss: 0.00013310207500776257\n",
      "Epoch 6593  \tTraining Loss: 0.00013054261504985177\tValidation Loss: 0.00013309964989718082\n",
      "Epoch 6594  \tTraining Loss: 0.00013053987029503598\tValidation Loss: 0.00013309716424158576\n",
      "Epoch 6595  \tTraining Loss: 0.000130537126725066\tValidation Loss: 0.00013309473163064113\n",
      "Epoch 6596  \tTraining Loss: 0.00013053438420388552\tValidation Loss: 0.00013309216653421005\n",
      "Epoch 6597  \tTraining Loss: 0.00013053164174557798\tValidation Loss: 0.00013308953744062824\n",
      "Epoch 6598  \tTraining Loss: 0.0001305289022111665\tValidation Loss: 0.00013308722210720442\n",
      "Epoch 6599  \tTraining Loss: 0.00013052616055370036\tValidation Loss: 0.0001330846570637302\n",
      "Epoch 6600  \tTraining Loss: 0.00013052342041458884\tValidation Loss: 0.00013308215289309404\n",
      "Epoch 6601  \tTraining Loss: 0.0001305206802350805\tValidation Loss: 0.0001330797205906504\n",
      "Epoch 6602  \tTraining Loss: 0.0001305179434034475\tValidation Loss: 0.0001330770548041758\n",
      "Epoch 6603  \tTraining Loss: 0.000130515205068963\tValidation Loss: 0.00013307463788364294\n",
      "Epoch 6604  \tTraining Loss: 0.00013051246772218954\tValidation Loss: 0.000133072238342026\n",
      "Epoch 6605  \tTraining Loss: 0.0001305097306364245\tValidation Loss: 0.00013306966593987178\n",
      "Epoch 6606  \tTraining Loss: 0.00013050699494006918\tValidation Loss: 0.00013306716412605173\n",
      "Epoch 6607  \tTraining Loss: 0.0001305042597466445\tValidation Loss: 0.00013306457415077934\n",
      "Epoch 6608  \tTraining Loss: 0.0001305015265060046\tValidation Loss: 0.00013306224927436984\n",
      "Epoch 6609  \tTraining Loss: 0.00013049879153120342\tValidation Loss: 0.00013305970841964685\n",
      "Epoch 6610  \tTraining Loss: 0.00013049605793763136\tValidation Loss: 0.00013305719643161876\n",
      "Epoch 6611  \tTraining Loss: 0.00013049332537533431\tValidation Loss: 0.00013305468432524853\n",
      "Epoch 6612  \tTraining Loss: 0.00013049059537594438\tValidation Loss: 0.00013305221786121213\n",
      "Epoch 6613  \tTraining Loss: 0.0001304878629058897\tValidation Loss: 0.000133049735673598\n",
      "Epoch 6614  \tTraining Loss: 0.00013048513226596276\tValidation Loss: 0.00013304731586433877\n",
      "Epoch 6615  \tTraining Loss: 0.00013048240251050723\tValidation Loss: 0.00013304476473327036\n",
      "Epoch 6616  \tTraining Loss: 0.0001304796731137882\tValidation Loss: 0.00013304215059447178\n",
      "Epoch 6617  \tTraining Loss: 0.00013047694603420006\tValidation Loss: 0.0001330398499863365\n",
      "Epoch 6618  \tTraining Loss: 0.00013047421756374935\tValidation Loss: 0.00013303729961291672\n",
      "Epoch 6619  \tTraining Loss: 0.0001304714900161204\tValidation Loss: 0.0001330348104960254\n",
      "Epoch 6620  \tTraining Loss: 0.00013046876276701859\tValidation Loss: 0.000133032393341406\n",
      "Epoch 6621  \tTraining Loss: 0.00013046603851449992\tValidation Loss: 0.00013302974307601373\n",
      "Epoch 6622  \tTraining Loss: 0.00013046331326824196\tValidation Loss: 0.00013302734087956503\n",
      "Epoch 6623  \tTraining Loss: 0.00013046058848207234\tValidation Loss: 0.0001330249565486823\n",
      "Epoch 6624  \tTraining Loss: 0.00013045786441471591\tValidation Loss: 0.0001330223990095457\n",
      "Epoch 6625  \tTraining Loss: 0.00013045514133472728\tValidation Loss: 0.00013301991229645315\n",
      "Epoch 6626  \tTraining Loss: 0.00013045241871155553\tValidation Loss: 0.00013301731829941507\n",
      "Epoch 6627  \tTraining Loss: 0.00013044969880666874\tValidation Loss: 0.00013301502813788783\n",
      "Epoch 6628  \tTraining Loss: 0.0001304469760497597\tValidation Loss: 0.00013301250651959887\n",
      "Epoch 6629  \tTraining Loss: 0.00013044425563089567\tValidation Loss: 0.00013301000596645422\n",
      "Epoch 6630  \tTraining Loss: 0.0001304415354534512\tValidation Loss: 0.00013300761245170159\n",
      "Epoch 6631  \tTraining Loss: 0.00013043881780205958\tValidation Loss: 0.0001330049539793195\n",
      "Epoch 6632  \tTraining Loss: 0.00013043609925307554\tValidation Loss: 0.00013300257530892906\n",
      "Epoch 6633  \tTraining Loss: 0.0001304333809104455\tValidation Loss: 0.0001330001825255865\n",
      "Epoch 6634  \tTraining Loss: 0.00013043066371779552\tValidation Loss: 0.00013299764877125326\n",
      "Epoch 6635  \tTraining Loss: 0.00013042794668845662\tValidation Loss: 0.00013299505030646294\n",
      "Epoch 6636  \tTraining Loss: 0.0001304252327816453\tValidation Loss: 0.0001329927642720195\n",
      "Epoch 6637  \tTraining Loss: 0.00013042251700404512\tValidation Loss: 0.000132990228170642\n",
      "Epoch 6638  \tTraining Loss: 0.00013041980210061227\tValidation Loss: 0.00013298775362042956\n",
      "Epoch 6639  \tTraining Loss: 0.00013041708774375518\tValidation Loss: 0.0001329853511025317\n",
      "Epoch 6640  \tTraining Loss: 0.00013041437550341263\tValidation Loss: 0.00013298271587526545\n",
      "Epoch 6641  \tTraining Loss: 0.00013041166348521578\tValidation Loss: 0.0001329803279486157\n",
      "Epoch 6642  \tTraining Loss: 0.00013040895153117177\tValidation Loss: 0.0001329779583661859\n",
      "Epoch 6643  \tTraining Loss: 0.0001304062400039772\tValidation Loss: 0.00013297541527012975\n",
      "Epoch 6644  \tTraining Loss: 0.0001304035295907714\tValidation Loss: 0.000132972943241603\n",
      "Epoch 6645  \tTraining Loss: 0.00013040081912335513\tValidation Loss: 0.00013297044232120434\n",
      "Epoch 6646  \tTraining Loss: 0.00013039811273994354\tValidation Loss: 0.000132968017855046\n",
      "Epoch 6647  \tTraining Loss: 0.00013039540211645981\tValidation Loss: 0.00013296556595073433\n",
      "Epoch 6648  \tTraining Loss: 0.00013039269457984742\tValidation Loss: 0.00013296316904955497\n",
      "Epoch 6649  \tTraining Loss: 0.00013038998740923905\tValidation Loss: 0.0001329606436266868\n",
      "Epoch 6650  \tTraining Loss: 0.00013038728113500214\tValidation Loss: 0.00013295805629954342\n",
      "Epoch 6651  \tTraining Loss: 0.0001303845761076096\tValidation Loss: 0.00013295570371336344\n",
      "Epoch 6652  \tTraining Loss: 0.00013038187066760892\tValidation Loss: 0.00013295332794037313\n",
      "Epoch 6653  \tTraining Loss: 0.00013037916586516494\tValidation Loss: 0.00013295080986236996\n",
      "Epoch 6654  \tTraining Loss: 0.00013037646125975722\tValidation Loss: 0.00013294832986725192\n",
      "Epoch 6655  \tTraining Loss: 0.0001303737592332048\tValidation Loss: 0.0001329458534409195\n",
      "Epoch 6656  \tTraining Loss: 0.00013037105742315545\tValidation Loss: 0.00013294342044900296\n",
      "Epoch 6657  \tTraining Loss: 0.0001303683547591496\tValidation Loss: 0.00013294097270561566\n",
      "Epoch 6658  \tTraining Loss: 0.0001303656532468946\tValidation Loss: 0.0001329385874173027\n",
      "Epoch 6659  \tTraining Loss: 0.0001303629530846886\tValidation Loss: 0.0001329360704227483\n",
      "Epoch 6660  \tTraining Loss: 0.00013036025279943323\tValidation Loss: 0.00013293349140698036\n",
      "Epoch 6661  \tTraining Loss: 0.00013035755553194838\tValidation Loss: 0.00013293122461818902\n",
      "Epoch 6662  \tTraining Loss: 0.0001303548555259851\tValidation Loss: 0.000132928707627249\n",
      "Epoch 6663  \tTraining Loss: 0.00013035215780916136\tValidation Loss: 0.00013292625247089896\n",
      "Epoch 6664  \tTraining Loss: 0.00013034945980083492\tValidation Loss: 0.0001329238892211787\n",
      "Epoch 6665  \tTraining Loss: 0.00013034676469352507\tValidation Loss: 0.00013292125352000142\n",
      "Epoch 6666  \tTraining Loss: 0.00013034406890701265\tValidation Loss: 0.0001329188996703722\n",
      "Epoch 6667  \tTraining Loss: 0.0001303413732890295\tValidation Loss: 0.00013291653240403575\n",
      "Epoch 6668  \tTraining Loss: 0.00013033867849869988\tValidation Loss: 0.00013291402419388752\n",
      "Epoch 6669  \tTraining Loss: 0.0001303359838968164\tValidation Loss: 0.0001329115546493481\n",
      "Epoch 6670  \tTraining Loss: 0.00013033329168242168\tValidation Loss: 0.00013290908961509967\n",
      "Epoch 6671  \tTraining Loss: 0.00013033059983178128\tValidation Loss: 0.00013290666726817915\n",
      "Epoch 6672  \tTraining Loss: 0.00013032790697285442\tValidation Loss: 0.00013290423064587606\n",
      "Epoch 6673  \tTraining Loss: 0.00013032521527682342\tValidation Loss: 0.0001329018566236559\n",
      "Epoch 6674  \tTraining Loss: 0.00013032252507777433\tValidation Loss: 0.00013289935084896256\n",
      "Epoch 6675  \tTraining Loss: 0.0001303198348392455\tValidation Loss: 0.00013289678344391976\n",
      "Epoch 6676  \tTraining Loss: 0.00013031714715924908\tValidation Loss: 0.00013289452789613294\n",
      "Epoch 6677  \tTraining Loss: 0.00013031445717544767\tValidation Loss: 0.00013289202199644472\n",
      "Epoch 6678  \tTraining Loss: 0.0001303117692869571\tValidation Loss: 0.00013288957816475077\n",
      "Epoch 6679  \tTraining Loss: 0.00013030908106660615\tValidation Loss: 0.00013288722635000118\n",
      "Epoch 6680  \tTraining Loss: 0.00013030639597225383\tValidation Loss: 0.00013288460215429718\n",
      "Epoch 6681  \tTraining Loss: 0.0001303037099586699\tValidation Loss: 0.00013288225939542153\n",
      "Epoch 6682  \tTraining Loss: 0.00013030102415181486\tValidation Loss: 0.00013287990338979702\n",
      "Epoch 6683  \tTraining Loss: 0.00013029833925680192\tValidation Loss: 0.00013287740638144748\n",
      "Epoch 6684  \tTraining Loss: 0.00013029565449047597\tValidation Loss: 0.00013287494802142146\n",
      "Epoch 6685  \tTraining Loss: 0.00013029297208321097\tValidation Loss: 0.00013287249476988828\n",
      "Epoch 6686  \tTraining Loss: 0.00013029029008395976\tValidation Loss: 0.00013287008328511724\n",
      "Epoch 6687  \tTraining Loss: 0.00013028760707391675\tValidation Loss: 0.00013286765785933962\n",
      "Epoch 6688  \tTraining Loss: 0.00013028492518347878\tValidation Loss: 0.00013286529508256927\n",
      "Epoch 6689  \tTraining Loss: 0.00013028224483863186\tValidation Loss: 0.00013286280046707636\n",
      "Epoch 6690  \tTraining Loss: 0.0001302795642861711\tValidation Loss: 0.0001328602445728355\n",
      "Epoch 6691  \tTraining Loss: 0.00013027688658094212\tValidation Loss: 0.00013285800013762024\n",
      "Epoch 6692  \tTraining Loss: 0.00013027420632129367\tValidation Loss: 0.00013285550520465266\n",
      "Epoch 6693  \tTraining Loss: 0.0001302715282970559\tValidation Loss: 0.00013285307255991085\n",
      "Epoch 6694  \tTraining Loss: 0.00013026884994639452\tValidation Loss: 0.00013285073202737977\n",
      "Epoch 6695  \tTraining Loss: 0.00013026617437202957\tValidation Loss: 0.00013284811919594188\n",
      "Epoch 6696  \tTraining Loss: 0.00013026349836659038\tValidation Loss: 0.00013284578738858232\n",
      "Epoch 6697  \tTraining Loss: 0.00013026082235885917\tValidation Loss: 0.00013284344249395305\n",
      "Epoch 6698  \tTraining Loss: 0.000130258147253355\tValidation Loss: 0.00013284095655197544\n",
      "Epoch 6699  \tTraining Loss: 0.0001302554721729068\tValidation Loss: 0.00013283850924131972\n",
      "Epoch 6700  \tTraining Loss: 0.00013025279954802504\tValidation Loss: 0.00013283606762236832\n",
      "Epoch 6701  \tTraining Loss: 0.00013025012730929635\tValidation Loss: 0.00013283366687511034\n",
      "Epoch 6702  \tTraining Loss: 0.00013024745418033551\tValidation Loss: 0.00013283125251695686\n",
      "Epoch 6703  \tTraining Loss: 0.00013024478208269609\tValidation Loss: 0.00013282890084871473\n",
      "Epoch 6704  \tTraining Loss: 0.00013024211148789347\tValidation Loss: 0.00013282641727031314\n",
      "Epoch 6705  \tTraining Loss: 0.0001302394402847087\tValidation Loss: 0.0001328238727612215\n",
      "Epoch 6706  \tTraining Loss: 0.0001302367729095625\tValidation Loss: 0.00013282163930919413\n",
      "Epoch 6707  \tTraining Loss: 0.0001302341021826844\tValidation Loss: 0.0001328191753496397\n",
      "Epoch 6708  \tTraining Loss: 0.00013023143370498736\tValidation Loss: 0.00013281673308040582\n",
      "Epoch 6709  \tTraining Loss: 0.00013022876580550148\tValidation Loss: 0.00013281439939700457\n",
      "Epoch 6710  \tTraining Loss: 0.00013022609894702597\tValidation Loss: 0.00013281180135020555\n",
      "Epoch 6711  \tTraining Loss: 0.00013022343328102494\tValidation Loss: 0.00013280948094491276\n",
      "Epoch 6712  \tTraining Loss: 0.00013022076702629465\tValidation Loss: 0.0001328071473363982\n",
      "Epoch 6713  \tTraining Loss: 0.0001302181016234761\tValidation Loss: 0.0001328046725303299\n",
      "Epoch 6714  \tTraining Loss: 0.00013021543610720624\tValidation Loss: 0.00013280231474418322\n",
      "Epoch 6715  \tTraining Loss: 0.00013021277332124297\tValidation Loss: 0.00013279973497481456\n",
      "Epoch 6716  \tTraining Loss: 0.0001302101103517661\tValidation Loss: 0.00013279740061327436\n",
      "Epoch 6717  \tTraining Loss: 0.00013020744761089032\tValidation Loss: 0.0001327950869917962\n",
      "Epoch 6718  \tTraining Loss: 0.0001302047851485118\tValidation Loss: 0.00013279259879800338\n",
      "Epoch 6719  \tTraining Loss: 0.0001302021239952301\tValidation Loss: 0.00013279018278933462\n",
      "Epoch 6720  \tTraining Loss: 0.00013019946239066073\tValidation Loss: 0.00013278776004239972\n",
      "Epoch 6721  \tTraining Loss: 0.0001301968049017111\tValidation Loss: 0.00013278536944183204\n",
      "Epoch 6722  \tTraining Loss: 0.00013019414397351576\tValidation Loss: 0.0001327829689066554\n",
      "Epoch 6723  \tTraining Loss: 0.00013019148514890688\tValidation Loss: 0.0001327806314923074\n",
      "Epoch 6724  \tTraining Loss: 0.0001301888271823191\tValidation Loss: 0.00013277816225791948\n",
      "Epoch 6725  \tTraining Loss: 0.0001301861688608932\tValidation Loss: 0.00013277573369937422\n",
      "Epoch 6726  \tTraining Loss: 0.0001301835135105741\tValidation Loss: 0.00013277331243455187\n",
      "Epoch 6727  \tTraining Loss: 0.0001301808576008379\tValidation Loss: 0.00013277093051853255\n",
      "Epoch 6728  \tTraining Loss: 0.0001301782013621437\tValidation Loss: 0.00013276853554175391\n",
      "Epoch 6729  \tTraining Loss: 0.00013017554582310154\tValidation Loss: 0.0001327662032777844\n",
      "Epoch 6730  \tTraining Loss: 0.00013017289227842038\tValidation Loss: 0.0001327637388945113\n",
      "Epoch 6731  \tTraining Loss: 0.0001301702382635626\tValidation Loss: 0.00013276123430092711\n",
      "Epoch 6732  \tTraining Loss: 0.00013016758681918896\tValidation Loss: 0.00013275899906483045\n",
      "Epoch 6733  \tTraining Loss: 0.00013016493349736239\tValidation Loss: 0.0001327565497462804\n",
      "Epoch 6734  \tTraining Loss: 0.0001301622813408206\tValidation Loss: 0.00013275412996138503\n",
      "Epoch 6735  \tTraining Loss: 0.0001301596304492395\tValidation Loss: 0.00013275181616561457\n",
      "Epoch 6736  \tTraining Loss: 0.00013015698049237377\tValidation Loss: 0.00013274923787306444\n",
      "Epoch 6737  \tTraining Loss: 0.00013015433142729067\tValidation Loss: 0.00013274693635238608\n",
      "Epoch 6738  \tTraining Loss: 0.00013015168171705784\tValidation Loss: 0.0001327446218502477\n",
      "Epoch 6739  \tTraining Loss: 0.0001301490332558475\tValidation Loss: 0.00013274216602849657\n",
      "Epoch 6740  \tTraining Loss: 0.00013014638428301308\tValidation Loss: 0.00013273982733343095\n",
      "Epoch 6741  \tTraining Loss: 0.00013014373861710624\tValidation Loss: 0.00013273726716112248\n",
      "Epoch 6742  \tTraining Loss: 0.0001301410919307965\tValidation Loss: 0.00013273497159400915\n",
      "Epoch 6743  \tTraining Loss: 0.00013013844611049234\tValidation Loss: 0.00013273265603692348\n",
      "Epoch 6744  \tTraining Loss: 0.00013013580047778535\tValidation Loss: 0.0001327302025866737\n",
      "Epoch 6745  \tTraining Loss: 0.0001301331554266853\tValidation Loss: 0.00013272778813405305\n",
      "Epoch 6746  \tTraining Loss: 0.0001301305113943886\tValidation Loss: 0.00013272538146773418\n",
      "Epoch 6747  \tTraining Loss: 0.00013012786977648075\tValidation Loss: 0.00013272301297301444\n",
      "Epoch 6748  \tTraining Loss: 0.00013012522594509792\tValidation Loss: 0.00013272063201213625\n",
      "Epoch 6749  \tTraining Loss: 0.00013012258361918787\tValidation Loss: 0.0001327183139220798\n",
      "Epoch 6750  \tTraining Loss: 0.0001301199424801544\tValidation Loss: 0.00013271586373562685\n",
      "Epoch 6751  \tTraining Loss: 0.00013011730046301212\tValidation Loss: 0.0001327134540972416\n",
      "Epoch 6752  \tTraining Loss: 0.0001301146622985936\tValidation Loss: 0.00013271105269167574\n",
      "Epoch 6753  \tTraining Loss: 0.00013011202247658024\tValidation Loss: 0.00013270870943176536\n",
      "Epoch 6754  \tTraining Loss: 0.00013010938315557443\tValidation Loss: 0.0001327063121623364\n",
      "Epoch 6755  \tTraining Loss: 0.00013010674466002882\tValidation Loss: 0.0001327040149334813\n",
      "Epoch 6756  \tTraining Loss: 0.00013010410701260817\tValidation Loss: 0.00013270155174904485\n",
      "Epoch 6757  \tTraining Loss: 0.0001301014701133899\tValidation Loss: 0.00013269914178015504\n",
      "Epoch 6758  \tTraining Loss: 0.00013009883515254495\tValidation Loss: 0.00013269677855388777\n",
      "Epoch 6759  \tTraining Loss: 0.0001300961983199024\tValidation Loss: 0.0001326944043933819\n",
      "Epoch 6760  \tTraining Loss: 0.00013009356274653624\tValidation Loss: 0.0001326920936569693\n",
      "Epoch 6761  \tTraining Loss: 0.0001300909288654895\tValidation Loss: 0.00013268965100304966\n",
      "Epoch 6762  \tTraining Loss: 0.00013008829435837754\tValidation Loss: 0.00013268714889625046\n",
      "Epoch 6763  \tTraining Loss: 0.00013008566316809444\tValidation Loss: 0.00013268495652852085\n",
      "Epoch 6764  \tTraining Loss: 0.00013008302929890772\tValidation Loss: 0.00013268251299827235\n",
      "Epoch 6765  \tTraining Loss: 0.00013008039755639796\tValidation Loss: 0.0001326801329039413\n",
      "Epoch 6766  \tTraining Loss: 0.00013007776556523074\tValidation Loss: 0.00013267784540181636\n",
      "Epoch 6767  \tTraining Loss: 0.0001300751354795829\tValidation Loss: 0.00013267528597772727\n",
      "Epoch 6768  \tTraining Loss: 0.00013007250637093792\tValidation Loss: 0.00013267300567825922\n",
      "Epoch 6769  \tTraining Loss: 0.00013006987666325972\tValidation Loss: 0.00013267071302813468\n",
      "Epoch 6770  \tTraining Loss: 0.00013006724770877449\tValidation Loss: 0.0001326682791608336\n",
      "Epoch 6771  \tTraining Loss: 0.00013006461870756637\tValidation Loss: 0.00013266596264236056\n",
      "Epoch 6772  \tTraining Loss: 0.00013006199207559865\tValidation Loss: 0.00013266352534284072\n",
      "Epoch 6773  \tTraining Loss: 0.00013005936499703872\tValidation Loss: 0.00013266103057991356\n",
      "Epoch 6774  \tTraining Loss: 0.00013005674015492135\tValidation Loss: 0.00013265884582559898\n",
      "Epoch 6775  \tTraining Loss: 0.00013005411333899932\tValidation Loss: 0.00013265643048149416\n",
      "Epoch 6776  \tTraining Loss: 0.0001300514884105105\tValidation Loss: 0.00013265403699208245\n",
      "Epoch 6777  \tTraining Loss: 0.00013004886371751327\tValidation Loss: 0.00013265175330359226\n",
      "Epoch 6778  \tTraining Loss: 0.00013004624074124674\tValidation Loss: 0.00013264920559226374\n",
      "Epoch 6779  \tTraining Loss: 0.00013004361830866042\tValidation Loss: 0.00013264693365898665\n",
      "Epoch 6780  \tTraining Loss: 0.00013004099532857506\tValidation Loss: 0.00013264464921704873\n",
      "Epoch 6781  \tTraining Loss: 0.00013003837353188383\tValidation Loss: 0.0001326422233994869\n",
      "Epoch 6782  \tTraining Loss: 0.00013003575128056998\tValidation Loss: 0.00013263991493778866\n",
      "Epoch 6783  \tTraining Loss: 0.0001300331319208861\tValidation Loss: 0.00013263738588631553\n",
      "Epoch 6784  \tTraining Loss: 0.00013003051220893794\tValidation Loss: 0.000132635099343137\n",
      "Epoch 6785  \tTraining Loss: 0.0001300278931008001\tValidation Loss: 0.00013263283522821066\n",
      "Epoch 6786  \tTraining Loss: 0.00013002527382726563\tValidation Loss: 0.0001326304159343223\n",
      "Epoch 6787  \tTraining Loss: 0.00013002265570862223\tValidation Loss: 0.0001326280277423332\n",
      "Epoch 6788  \tTraining Loss: 0.00013002003805857537\tValidation Loss: 0.00013262575160957017\n",
      "Epoch 6789  \tTraining Loss: 0.00013001742229146152\tValidation Loss: 0.0001326232120860407\n",
      "Epoch 6790  \tTraining Loss: 0.00013001480652212797\tValidation Loss: 0.00013262094813996846\n",
      "Epoch 6791  \tTraining Loss: 0.00013001219029991674\tValidation Loss: 0.00013261867177244974\n",
      "Epoch 6792  \tTraining Loss: 0.00013000957559139862\tValidation Loss: 0.00013261625394782973\n",
      "Epoch 6793  \tTraining Loss: 0.00013000696009591835\tValidation Loss: 0.00013261395348119326\n",
      "Epoch 6794  \tTraining Loss: 0.00013000434814661464\tValidation Loss: 0.0001326114326142182\n",
      "Epoch 6795  \tTraining Loss: 0.00013000173497078053\tValidation Loss: 0.00013260917438523447\n",
      "Epoch 6796  \tTraining Loss: 0.00012999912263935521\tValidation Loss: 0.00013260689667989034\n",
      "Epoch 6797  \tTraining Loss: 0.00012999651062855068\tValidation Loss: 0.0001326044810233956\n",
      "Epoch 6798  \tTraining Loss: 0.00012999389894513194\tValidation Loss: 0.00013260210426706718\n",
      "Epoch 6799  \tTraining Loss: 0.00012999128854617346\tValidation Loss: 0.00013259983664899033\n",
      "Epoch 6800  \tTraining Loss: 0.00012998867967962775\tValidation Loss: 0.00013259730542388554\n",
      "Epoch 6801  \tTraining Loss: 0.00012998607072841332\tValidation Loss: 0.00013259504933566527\n",
      "Epoch 6802  \tTraining Loss: 0.00012998346125373304\tValidation Loss: 0.00013259278087821098\n",
      "Epoch 6803  \tTraining Loss: 0.00012998085358159386\tValidation Loss: 0.0001325903709011341\n",
      "Epoch 6804  \tTraining Loss: 0.0001299782448471632\tValidation Loss: 0.0001325880990162041\n",
      "Epoch 6805  \tTraining Loss: 0.00012997564001210603\tValidation Loss: 0.00013258556471548992\n",
      "Epoch 6806  \tTraining Loss: 0.000129973033931424\tValidation Loss: 0.00013258330986913626\n",
      "Epoch 6807  \tTraining Loss: 0.00012997042793709207\tValidation Loss: 0.00013258104358678105\n",
      "Epoch 6808  \tTraining Loss: 0.00012996782304888513\tValidation Loss: 0.0001325786362551685\n",
      "Epoch 6809  \tTraining Loss: 0.0001299652179739355\tValidation Loss: 0.00013257626752409213\n",
      "Epoch 6810  \tTraining Loss: 0.00012996261494244962\tValidation Loss: 0.0001325739089689862\n",
      "Epoch 6811  \tTraining Loss: 0.00012996001301475018\tValidation Loss: 0.0001325715847044571\n",
      "Epoch 6812  \tTraining Loss: 0.00012995741001928406\tValidation Loss: 0.00013256932850911842\n",
      "Epoch 6813  \tTraining Loss: 0.0001299548079922293\tValidation Loss: 0.00013256690523929656\n",
      "Epoch 6814  \tTraining Loss: 0.0001299522070089159\tValidation Loss: 0.0001325645566303294\n",
      "Epoch 6815  \tTraining Loss: 0.00012994960549973165\tValidation Loss: 0.00013256230347494442\n",
      "Epoch 6816  \tTraining Loss: 0.0001299470072084402\tValidation Loss: 0.00013255977940695445\n",
      "Epoch 6817  \tTraining Loss: 0.0001299444080866844\tValidation Loss: 0.00013255753343959593\n",
      "Epoch 6818  \tTraining Loss: 0.0001299418088494555\tValidation Loss: 0.00013255527566658198\n",
      "Epoch 6819  \tTraining Loss: 0.0001299392109090706\tValidation Loss: 0.00013255287657096615\n",
      "Epoch 6820  \tTraining Loss: 0.00012993661240694553\tValidation Loss: 0.00013255051587421427\n",
      "Epoch 6821  \tTraining Loss: 0.000129934016778397\tValidation Loss: 0.0001325481656450096\n",
      "Epoch 6822  \tTraining Loss: 0.00012993142112129204\tValidation Loss: 0.0001325458489880856\n",
      "Epoch 6823  \tTraining Loss: 0.00012992882543696741\tValidation Loss: 0.0001325436006461926\n",
      "Epoch 6824  \tTraining Loss: 0.00012992622997388788\tValidation Loss: 0.00013254120584310725\n",
      "Epoch 6825  \tTraining Loss: 0.00012992363551611117\tValidation Loss: 0.00013253884323387096\n",
      "Epoch 6826  \tTraining Loss: 0.00012992104168938105\tValidation Loss: 0.00013253659375052522\n",
      "Epoch 6827  \tTraining Loss: 0.00012991844942405702\tValidation Loss: 0.0001325340812536848\n",
      "Epoch 6828  \tTraining Loss: 0.00012991585748080802\tValidation Loss: 0.00013253184345137566\n",
      "Epoch 6829  \tTraining Loss: 0.00012991326496378702\tValidation Loss: 0.00013252959364666318\n",
      "Epoch 6830  \tTraining Loss: 0.00012991067393164024\tValidation Loss: 0.0001325272023867687\n",
      "Epoch 6831  \tTraining Loss: 0.0001299080821373563\tValidation Loss: 0.00013252492866070716\n",
      "Epoch 6832  \tTraining Loss: 0.0001299054936084604\tValidation Loss: 0.00013252243536909808\n",
      "Epoch 6833  \tTraining Loss: 0.000129902904279474\tValidation Loss: 0.0001325202033385147\n",
      "Epoch 6834  \tTraining Loss: 0.00012990031565892796\tValidation Loss: 0.00013251795218289134\n",
      "Epoch 6835  \tTraining Loss: 0.00012989772723151933\tValidation Loss: 0.00013251556308873966\n",
      "Epoch 6836  \tTraining Loss: 0.00012989513896007735\tValidation Loss: 0.0001325132128163887\n",
      "Epoch 6837  \tTraining Loss: 0.00012989255269809158\tValidation Loss: 0.00013251097223941272\n",
      "Epoch 6838  \tTraining Loss: 0.00012988996650959076\tValidation Loss: 0.00013250846824202102\n",
      "Epoch 6839  \tTraining Loss: 0.00012988738199544038\tValidation Loss: 0.00013250631766189574\n",
      "Epoch 6840  \tTraining Loss: 0.00012988479590334587\tValidation Loss: 0.0001325039245133032\n",
      "Epoch 6841  \tTraining Loss: 0.00012988221163050322\tValidation Loss: 0.00013250159728304066\n",
      "Epoch 6842  \tTraining Loss: 0.00012987962711043363\tValidation Loss: 0.0001324993635808571\n",
      "Epoch 6843  \tTraining Loss: 0.00012987704452534967\tValidation Loss: 0.0001324968585430741\n",
      "Epoch 6844  \tTraining Loss: 0.00012987446283122694\tValidation Loss: 0.00013249471017642\n",
      "Epoch 6845  \tTraining Loss: 0.0001298718806835401\tValidation Loss: 0.0001324923195954699\n",
      "Epoch 6846  \tTraining Loss: 0.000129869298869055\tValidation Loss: 0.0001324899953274344\n",
      "Epoch 6847  \tTraining Loss: 0.00012986671750097467\tValidation Loss: 0.00013248774380382038\n",
      "Epoch 6848  \tTraining Loss: 0.00012986413820102402\tValidation Loss: 0.00013248536210135498\n",
      "Epoch 6849  \tTraining Loss: 0.00012986155801278905\tValidation Loss: 0.00013248294428555045\n",
      "Epoch 6850  \tTraining Loss: 0.0001298589809439927\tValidation Loss: 0.00013248079132612638\n",
      "Epoch 6851  \tTraining Loss: 0.00012985640166621797\tValidation Loss: 0.00013247842515745265\n",
      "Epoch 6852  \tTraining Loss: 0.00012985382354817622\tValidation Loss: 0.0001324761678816386\n",
      "Epoch 6853  \tTraining Loss: 0.00012985124697023222\tValidation Loss: 0.00013247378768484742\n",
      "Epoch 6854  \tTraining Loss: 0.00012984866958197202\tValidation Loss: 0.00013247135123935246\n",
      "Epoch 6855  \tTraining Loss: 0.00012984609600676423\tValidation Loss: 0.00013246922278131574\n",
      "Epoch 6856  \tTraining Loss: 0.00012984351965972252\tValidation Loss: 0.00013246684242888953\n",
      "Epoch 6857  \tTraining Loss: 0.00012984094506117728\tValidation Loss: 0.00013246452713350175\n",
      "Epoch 6858  \tTraining Loss: 0.00012983837060045369\tValidation Loss: 0.00013246230503395235\n",
      "Epoch 6859  \tTraining Loss: 0.00012983579761108058\tValidation Loss: 0.00013245990926179186\n",
      "Epoch 6860  \tTraining Loss: 0.0001298332245973402\tValidation Loss: 0.00013245757502188313\n",
      "Epoch 6861  \tTraining Loss: 0.00012983065403852518\tValidation Loss: 0.00013245528184076442\n",
      "Epoch 6862  \tTraining Loss: 0.0001298280814208372\tValidation Loss: 0.00013245298023758416\n",
      "Epoch 6863  \tTraining Loss: 0.00012982551024272884\tValidation Loss: 0.00013245074221328502\n",
      "Epoch 6864  \tTraining Loss: 0.00012982294034392204\tValidation Loss: 0.00013244837192772117\n",
      "Epoch 6865  \tTraining Loss: 0.0001298203692598025\tValidation Loss: 0.00013244612143966793\n",
      "Epoch 6866  \tTraining Loss: 0.00012981780250753656\tValidation Loss: 0.00013244365272552458\n",
      "Epoch 6867  \tTraining Loss: 0.0001298152333405304\tValidation Loss: 0.00013244144415385705\n",
      "Epoch 6868  \tTraining Loss: 0.0001298126654446749\tValidation Loss: 0.0001324392167654269\n",
      "Epoch 6869  \tTraining Loss: 0.00012981009804868002\tValidation Loss: 0.00013243685139276132\n",
      "Epoch 6870  \tTraining Loss: 0.00012980753053923406\tValidation Loss: 0.00013243452471242527\n",
      "Epoch 6871  \tTraining Loss: 0.00012980496530841991\tValidation Loss: 0.0001324322107271305\n",
      "Epoch 6872  \tTraining Loss: 0.00012980240068566064\tValidation Loss: 0.00013242992741079295\n",
      "Epoch 6873  \tTraining Loss: 0.00012979983560714445\tValidation Loss: 0.00013242771397993584\n",
      "Epoch 6874  \tTraining Loss: 0.00012979727083741794\tValidation Loss: 0.00013242535359571774\n",
      "Epoch 6875  \tTraining Loss: 0.00012979470719854362\tValidation Loss: 0.00013242302532511898\n",
      "Epoch 6876  \tTraining Loss: 0.0001297921440892119\tValidation Loss: 0.0001324208110336508\n",
      "Epoch 6877  \tTraining Loss: 0.00012978958211988046\tValidation Loss: 0.00013241833394258081\n",
      "Epoch 6878  \tTraining Loss: 0.0001297870212121255\tValidation Loss: 0.00013241620981145068\n",
      "Epoch 6879  \tTraining Loss: 0.00012978445950354971\tValidation Loss: 0.00013241384283425078\n",
      "Epoch 6880  \tTraining Loss: 0.00012978189783326586\tValidation Loss: 0.00013241158402187156\n",
      "Epoch 6881  \tTraining Loss: 0.00012977933218724134\tValidation Loss: 0.00013240937465692224\n",
      "Epoch 6882  \tTraining Loss: 0.00012977676900688146\tValidation Loss: 0.0001324070298034052\n",
      "Epoch 6883  \tTraining Loss: 0.00012977420498335313\tValidation Loss: 0.00013240472653861042\n",
      "Epoch 6884  \tTraining Loss: 0.0001297716440223113\tValidation Loss: 0.00013240245365454275\n",
      "Epoch 6885  \tTraining Loss: 0.00012976908037843328\tValidation Loss: 0.0001324001750401731\n",
      "Epoch 6886  \tTraining Loss: 0.00012976651885283276\tValidation Loss: 0.0001323979597947965\n",
      "Epoch 6887  \tTraining Loss: 0.00012976395803721987\tValidation Loss: 0.00013239561193416257\n",
      "Epoch 6888  \tTraining Loss: 0.00012976139676248167\tValidation Loss: 0.00013239338384073588\n",
      "Epoch 6889  \tTraining Loss: 0.0001297588388917203\tValidation Loss: 0.00013239093782421789\n",
      "Epoch 6890  \tTraining Loss: 0.00012975627910304644\tValidation Loss: 0.0001323887512451215\n",
      "Epoch 6891  \tTraining Loss: 0.0001297537212920261\tValidation Loss: 0.00013238654598104495\n",
      "Epoch 6892  \tTraining Loss: 0.00012975116264637418\tValidation Loss: 0.00013238420271630415\n",
      "Epoch 6893  \tTraining Loss: 0.00012974860531624565\tValidation Loss: 0.00013238197772806227\n",
      "Epoch 6894  \tTraining Loss: 0.00012974604888108494\tValidation Loss: 0.00013237963177049482\n",
      "Epoch 6895  \tTraining Loss: 0.00012974349304743966\tValidation Loss: 0.00013237723098504414\n",
      "Epoch 6896  \tTraining Loss: 0.00012974093913360488\tValidation Loss: 0.00013237513747869488\n",
      "Epoch 6897  \tTraining Loss: 0.00012973838304476996\tValidation Loss: 0.0001323727915345681\n",
      "Epoch 6898  \tTraining Loss: 0.00012973582921454907\tValidation Loss: 0.00013237059094987964\n",
      "Epoch 6899  \tTraining Loss: 0.0001297332750668553\tValidation Loss: 0.0001323682308167841\n",
      "Epoch 6900  \tTraining Loss: 0.00012973072241258583\tValidation Loss: 0.0001323660282459219\n",
      "Epoch 6901  \tTraining Loss: 0.00012972817096065084\tValidation Loss: 0.0001323635934135003\n",
      "Epoch 6902  \tTraining Loss: 0.00012972561960924918\tValidation Loss: 0.00013236138917439052\n",
      "Epoch 6903  \tTraining Loss: 0.0001297230680674485\tValidation Loss: 0.00013235921381253368\n",
      "Epoch 6904  \tTraining Loss: 0.00012972051747734225\tValidation Loss: 0.00013235686100461331\n",
      "Epoch 6905  \tTraining Loss: 0.00012971796739708839\tValidation Loss: 0.0001323546629275885\n",
      "Epoch 6906  \tTraining Loss: 0.00012971541812936688\tValidation Loss: 0.00013235230723119092\n",
      "Epoch 6907  \tTraining Loss: 0.00012971287051214794\tValidation Loss: 0.00013234993336280976\n",
      "Epoch 6908  \tTraining Loss: 0.00012971032250600582\tValidation Loss: 0.00013234783014865297\n",
      "Epoch 6909  \tTraining Loss: 0.00012970777538010565\tValidation Loss: 0.0001323455108411217\n",
      "Epoch 6910  \tTraining Loss: 0.00012970522745728993\tValidation Loss: 0.00013234332232435627\n",
      "Epoch 6911  \tTraining Loss: 0.0001297026821384384\tValidation Loss: 0.0001323409666748638\n",
      "Epoch 6912  \tTraining Loss: 0.00012970013569239673\tValidation Loss: 0.00013233869248657193\n",
      "Epoch 6913  \tTraining Loss: 0.00012969759217711506\tValidation Loss: 0.00013233639724151577\n",
      "Epoch 6914  \tTraining Loss: 0.00012969504851004674\tValidation Loss: 0.0001323341672311753\n",
      "Epoch 6915  \tTraining Loss: 0.0001296925035231784\tValidation Loss: 0.00013233197181363878\n",
      "Epoch 6916  \tTraining Loss: 0.00012968996105581539\tValidation Loss: 0.00013232964438903572\n",
      "Epoch 6917  \tTraining Loss: 0.00012968741720282157\tValidation Loss: 0.00013232743617187508\n",
      "Epoch 6918  \tTraining Loss: 0.00012968487676192283\tValidation Loss: 0.00013232510723560308\n",
      "Epoch 6919  \tTraining Loss: 0.0001296823351545987\tValidation Loss: 0.00013232274537051318\n",
      "Epoch 6920  \tTraining Loss: 0.00012967979561353242\tValidation Loss: 0.00013232064632478092\n",
      "Epoch 6921  \tTraining Loss: 0.00012967725484697786\tValidation Loss: 0.00013231833483860867\n",
      "Epoch 6922  \tTraining Loss: 0.00012967471487712696\tValidation Loss: 0.00013231613280393338\n",
      "Epoch 6923  \tTraining Loss: 0.0001296721766386421\tValidation Loss: 0.00013231380786205523\n",
      "Epoch 6924  \tTraining Loss: 0.00012966963719027714\tValidation Loss: 0.00013231152437601388\n",
      "Epoch 6925  \tTraining Loss: 0.00012966710165703365\tValidation Loss: 0.00013230925653456643\n",
      "Epoch 6926  \tTraining Loss: 0.00012966456424919778\tValidation Loss: 0.00013230703803884938\n",
      "Epoch 6927  \tTraining Loss: 0.00012966202784321404\tValidation Loss: 0.00013230484672556474\n",
      "Epoch 6928  \tTraining Loss: 0.00012965949169064728\tValidation Loss: 0.00013230252683186385\n",
      "Epoch 6929  \tTraining Loss: 0.000129656956023394\tValidation Loss: 0.00013230032649649313\n",
      "Epoch 6930  \tTraining Loss: 0.00012965442200262513\tValidation Loss: 0.000132298005649011\n",
      "Epoch 6931  \tTraining Loss: 0.00012965188802989693\tValidation Loss: 0.0001322956308461895\n",
      "Epoch 6932  \tTraining Loss: 0.00012964935602282493\tValidation Loss: 0.0001322935626423847\n",
      "Epoch 6933  \tTraining Loss: 0.00012964682199577513\tValidation Loss: 0.00013229124174220978\n",
      "Epoch 6934  \tTraining Loss: 0.00012964429010447952\tValidation Loss: 0.00013228906703063805\n",
      "Epoch 6935  \tTraining Loss: 0.00012964175805101682\tValidation Loss: 0.00013228675363367292\n",
      "Epoch 6936  \tTraining Loss: 0.00012963922703289948\tValidation Loss: 0.0001322844741817681\n",
      "Epoch 6937  \tTraining Loss: 0.00012963669757397366\tValidation Loss: 0.00013228221424457768\n",
      "Epoch 6938  \tTraining Loss: 0.00012963416843755739\tValidation Loss: 0.00013227998183775242\n",
      "Epoch 6939  \tTraining Loss: 0.0001296316386836231\tValidation Loss: 0.00013227782145461712\n",
      "Epoch 6940  \tTraining Loss: 0.00012962910990426763\tValidation Loss: 0.0001322754919316479\n",
      "Epoch 6941  \tTraining Loss: 0.00012962658184795687\tValidation Loss: 0.00013227331887009553\n",
      "Epoch 6942  \tTraining Loss: 0.00012962405429331632\tValidation Loss: 0.00013227098787763975\n",
      "Epoch 6943  \tTraining Loss: 0.00012962152839798315\tValidation Loss: 0.00013226864018177695\n",
      "Epoch 6944  \tTraining Loss: 0.00012961900256524026\tValidation Loss: 0.00013226658333164365\n",
      "Epoch 6945  \tTraining Loss: 0.00012961647702150356\tValidation Loss: 0.00013226426614946964\n",
      "Epoch 6946  \tTraining Loss: 0.000129613951480151\tValidation Loss: 0.00013226209887226063\n",
      "Epoch 6947  \tTraining Loss: 0.00012961142731716255\tValidation Loss: 0.0001322597715295079\n",
      "Epoch 6948  \tTraining Loss: 0.00012960890311940253\tValidation Loss: 0.00013225760292231233\n",
      "Epoch 6949  \tTraining Loss: 0.00012960638066219897\tValidation Loss: 0.0001322551811226597\n",
      "Epoch 6950  \tTraining Loss: 0.0001296038592259117\tValidation Loss: 0.00013225303244741502\n",
      "Epoch 6951  \tTraining Loss: 0.0001296013359766756\tValidation Loss: 0.0001322508732264162\n",
      "Epoch 6952  \tTraining Loss: 0.00012959881551266234\tValidation Loss: 0.00013224857278054759\n",
      "Epoch 6953  \tTraining Loss: 0.0001295962935169709\tValidation Loss: 0.00013224641153796065\n",
      "Epoch 6954  \tTraining Loss: 0.0001295937742093831\tValidation Loss: 0.00013224408422072003\n",
      "Epoch 6955  \tTraining Loss: 0.0001295912541995621\tValidation Loss: 0.00013224174385660097\n",
      "Epoch 6956  \tTraining Loss: 0.0001295887366178268\tValidation Loss: 0.00013223967254512488\n",
      "Epoch 6957  \tTraining Loss: 0.00012958621793014927\tValidation Loss: 0.00013223738573388505\n",
      "Epoch 6958  \tTraining Loss: 0.00012958369936450536\tValidation Loss: 0.0001322352081030328\n",
      "Epoch 6959  \tTraining Loss: 0.00012958118308080155\tValidation Loss: 0.00013223290739831632\n",
      "Epoch 6960  \tTraining Loss: 0.0001295786651604216\tValidation Loss: 0.000132230749595463\n",
      "Epoch 6961  \tTraining Loss: 0.00012957615041585805\tValidation Loss: 0.00013222833123450198\n",
      "Epoch 6962  \tTraining Loss: 0.00012957363571137944\tValidation Loss: 0.0001322261891558286\n",
      "Epoch 6963  \tTraining Loss: 0.00012957112056729693\tValidation Loss: 0.00013222404704000607\n",
      "Epoch 6964  \tTraining Loss: 0.00012956860620823323\tValidation Loss: 0.0001322217460281186\n",
      "Epoch 6965  \tTraining Loss: 0.00012956609183448683\tValidation Loss: 0.00013221956691649746\n",
      "Epoch 6966  \tTraining Loss: 0.0001295635795499244\tValidation Loss: 0.00013221726770679958\n",
      "Epoch 6967  \tTraining Loss: 0.00012956106568413442\tValidation Loss: 0.00013221511244220966\n",
      "Epoch 6968  \tTraining Loss: 0.0001295585559166023\tValidation Loss: 0.00013221269690412814\n",
      "Epoch 6969  \tTraining Loss: 0.00012955604437734605\tValidation Loss: 0.0001322105576465397\n",
      "Epoch 6970  \tTraining Loss: 0.00012955353355642896\tValidation Loss: 0.00013220840825959627\n",
      "Epoch 6971  \tTraining Loss: 0.00012955102320843113\tValidation Loss: 0.00013220611789515677\n",
      "Epoch 6972  \tTraining Loss: 0.00012954851327111414\tValidation Loss: 0.00013220394565376375\n",
      "Epoch 6973  \tTraining Loss: 0.0001295460049303786\tValidation Loss: 0.0001322016523041318\n",
      "Epoch 6974  \tTraining Loss: 0.00012954349581121164\tValidation Loss: 0.0001321993856304181\n",
      "Epoch 6975  \tTraining Loss: 0.00012954099075346436\tValidation Loss: 0.00013219719150464753\n",
      "Epoch 6976  \tTraining Loss: 0.00012953848141915067\tValidation Loss: 0.00013219497562953167\n",
      "Epoch 6977  \tTraining Loss: 0.00012953597599652808\tValidation Loss: 0.00013219281610474297\n",
      "Epoch 6978  \tTraining Loss: 0.00012953346946937888\tValidation Loss: 0.0001321905277245246\n",
      "Epoch 6979  \tTraining Loss: 0.00012953096399695156\tValidation Loss: 0.00013218835971213173\n",
      "Epoch 6980  \tTraining Loss: 0.00012952845956325668\tValidation Loss: 0.00013218607125585325\n",
      "Epoch 6981  \tTraining Loss: 0.00012952595539127007\tValidation Loss: 0.0001321838097562554\n",
      "Epoch 6982  \tTraining Loss: 0.00012952345372101745\tValidation Loss: 0.00013218162053730317\n",
      "Epoch 6983  \tTraining Loss: 0.0001295209485936605\tValidation Loss: 0.00013217946789164072\n",
      "Epoch 6984  \tTraining Loss: 0.00012951844780451644\tValidation Loss: 0.0001321771838305137\n",
      "Epoch 6985  \tTraining Loss: 0.00012951594453168046\tValidation Loss: 0.00013217496103266392\n",
      "Epoch 6986  \tTraining Loss: 0.00012951344403325064\tValidation Loss: 0.00013217280535964985\n",
      "Epoch 6987  \tTraining Loss: 0.00012951094348756176\tValidation Loss: 0.00013217042831184365\n",
      "Epoch 6988  \tTraining Loss: 0.0001295084442970568\tValidation Loss: 0.00013216836479525954\n",
      "Epoch 6989  \tTraining Loss: 0.0001295059450988636\tValidation Loss: 0.00013216609535729497\n",
      "Epoch 6990  \tTraining Loss: 0.0001295034450949691\tValidation Loss: 0.00013216393648417103\n",
      "Epoch 6991  \tTraining Loss: 0.0001295009481871336\tValidation Loss: 0.00013216165498784705\n",
      "Epoch 6992  \tTraining Loss: 0.00012949844906293714\tValidation Loss: 0.0001321594366801967\n",
      "Epoch 6993  \tTraining Loss: 0.00012949595299795605\tValidation Loss: 0.00013215728597305517\n",
      "Epoch 6994  \tTraining Loss: 0.0001294934567207106\tValidation Loss: 0.0001321549141485416\n",
      "Epoch 6995  \tTraining Loss: 0.0001294909615409894\tValidation Loss: 0.00013215285552105547\n",
      "Epoch 6996  \tTraining Loss: 0.00012948846619927526\tValidation Loss: 0.0001321505909887091\n",
      "Epoch 6997  \tTraining Loss: 0.00012948597062125813\tValidation Loss: 0.00013214843695654325\n",
      "Epoch 6998  \tTraining Loss: 0.0001294834775647313\tValidation Loss: 0.00013214616027498768\n",
      "Epoch 6999  \tTraining Loss: 0.0001294809825868355\tValidation Loss: 0.00013214402701983052\n",
      "Epoch 7000  \tTraining Loss: 0.00012947849109152653\tValidation Loss: 0.00013214172796161214\n",
      "Epoch 7001  \tTraining Loss: 0.00012947599872010755\tValidation Loss: 0.00013213941801829617\n",
      "Epoch 7002  \tTraining Loss: 0.00012947350777273953\tValidation Loss: 0.00013213737543318083\n",
      "Epoch 7003  \tTraining Loss: 0.00012947101621174062\tValidation Loss: 0.00013213511774409832\n",
      "Epoch 7004  \tTraining Loss: 0.0001294685250698487\tValidation Loss: 0.00013213296938302023\n",
      "Epoch 7005  \tTraining Loss: 0.00012946603583556418\tValidation Loss: 0.00013213069795287198\n",
      "Epoch 7006  \tTraining Loss: 0.00012946354516694128\tValidation Loss: 0.00013212854771462278\n",
      "Epoch 7007  \tTraining Loss: 0.0001294610580888929\tValidation Loss: 0.00013212618273475836\n",
      "Epoch 7008  \tTraining Loss: 0.00012945856966642896\tValidation Loss: 0.00013212407330342454\n",
      "Epoch 7009  \tTraining Loss: 0.00012945608252342038\tValidation Loss: 0.00013212194612686695\n",
      "Epoch 7010  \tTraining Loss: 0.00012945359500707887\tValidation Loss: 0.00013211968135818484\n",
      "Epoch 7011  \tTraining Loss: 0.00012945110833926992\tValidation Loss: 0.0001321175350174621\n",
      "Epoch 7012  \tTraining Loss: 0.0001294486229011299\tValidation Loss: 0.00013211526772096666\n",
      "Epoch 7013  \tTraining Loss: 0.00012944613665551508\tValidation Loss: 0.00013211312218085547\n",
      "Epoch 7014  \tTraining Loss: 0.00012944365363341183\tValidation Loss: 0.00013211076221542574\n",
      "Epoch 7015  \tTraining Loss: 0.0001294411691647781\tValidation Loss: 0.00013210873788395687\n",
      "Epoch 7016  \tTraining Loss: 0.00012943868632050835\tValidation Loss: 0.00013210646220776135\n",
      "Epoch 7017  \tTraining Loss: 0.0001294362024181935\tValidation Loss: 0.00013210425923064449\n",
      "Epoch 7018  \tTraining Loss: 0.0001294337203874455\tValidation Loss: 0.00013210212902680906\n",
      "Epoch 7019  \tTraining Loss: 0.00012943123867322306\tValidation Loss: 0.00013209986858279688\n",
      "Epoch 7020  \tTraining Loss: 0.00012942875685831954\tValidation Loss: 0.00013209772868687326\n",
      "Epoch 7021  \tTraining Loss: 0.00012942627780741415\tValidation Loss: 0.000132095374100345\n",
      "Epoch 7022  \tTraining Loss: 0.00012942379746459575\tValidation Loss: 0.00013209335468597472\n",
      "Epoch 7023  \tTraining Loss: 0.00012942131855130727\tValidation Loss: 0.00013209108374202873\n",
      "Epoch 7024  \tTraining Loss: 0.00012941883884409912\tValidation Loss: 0.00013208896593314376\n",
      "Epoch 7025  \tTraining Loss: 0.0001294163610550841\tValidation Loss: 0.00013208668706951048\n",
      "Epoch 7026  \tTraining Loss: 0.00012941388290937478\tValidation Loss: 0.0001320844883760585\n",
      "Epoch 7027  \tTraining Loss: 0.00012941140572807054\tValidation Loss: 0.0001320823645407576\n",
      "Epoch 7028  \tTraining Loss: 0.0001294089305234333\tValidation Loss: 0.00013208001689853416\n",
      "Epoch 7029  \tTraining Loss: 0.00012940645438936979\tValidation Loss: 0.00013207800301297102\n",
      "Epoch 7030  \tTraining Loss: 0.00012940397933824136\tValidation Loss: 0.00013207573710879948\n",
      "Epoch 7031  \tTraining Loss: 0.000129401503925715\tValidation Loss: 0.00013207362426872143\n",
      "Epoch 7032  \tTraining Loss: 0.0001293990299934204\tValidation Loss: 0.0001320713501234001\n",
      "Epoch 7033  \tTraining Loss: 0.00012939655607346632\tValidation Loss: 0.0001320692366063011\n",
      "Epoch 7034  \tTraining Loss: 0.00012939408312904254\tValidation Loss: 0.00013206687021385824\n",
      "Epoch 7035  \tTraining Loss: 0.00012939161218383902\tValidation Loss: 0.00013206477515213474\n",
      "Epoch 7036  \tTraining Loss: 0.00012938913939469782\tValidation Loss: 0.0001320626922445449\n",
      "Epoch 7037  \tTraining Loss: 0.00012938666855634304\tValidation Loss: 0.0001320604219173803\n",
      "Epoch 7038  \tTraining Loss: 0.00012938419751816233\tValidation Loss: 0.00013205831238887237\n",
      "Epoch 7039  \tTraining Loss: 0.0001293817273721751\tValidation Loss: 0.000132056042948818\n",
      "Epoch 7040  \tTraining Loss: 0.00012937925779329642\tValidation Loss: 0.00013205393457423872\n",
      "Epoch 7041  \tTraining Loss: 0.00012937678872543615\tValidation Loss: 0.00013205157332575316\n",
      "Epoch 7042  \tTraining Loss: 0.00012937432197266007\tValidation Loss: 0.00013204956361670422\n",
      "Epoch 7043  \tTraining Loss: 0.0001293718531765477\tValidation Loss: 0.0001320473096875909\n",
      "Epoch 7044  \tTraining Loss: 0.00012936938624114549\tValidation Loss: 0.00013204520586376667\n",
      "Epoch 7045  \tTraining Loss: 0.00012936691943192538\tValidation Loss: 0.00013204296256176003\n",
      "Epoch 7046  \tTraining Loss: 0.00012936445297209617\tValidation Loss: 0.00013204075301782403\n",
      "Epoch 7047  \tTraining Loss: 0.00012936198793659216\tValidation Loss: 0.00013203866108522602\n",
      "Epoch 7048  \tTraining Loss: 0.00012935952261135163\tValidation Loss: 0.0001320363068821566\n",
      "Epoch 7049  \tTraining Loss: 0.00012935706019345892\tValidation Loss: 0.00013203430282428106\n",
      "Epoch 7050  \tTraining Loss: 0.00012935459511091978\tValidation Loss: 0.00013203205397767544\n",
      "Epoch 7051  \tTraining Loss: 0.00012935213255268495\tValidation Loss: 0.0001320299551085125\n",
      "Epoch 7052  \tTraining Loss: 0.00012934966943101356\tValidation Loss: 0.0001320277165466243\n",
      "Epoch 7053  \tTraining Loss: 0.0001293472072279879\tValidation Loss: 0.00013202559212588698\n",
      "Epoch 7054  \tTraining Loss: 0.000129344746316152\tValidation Loss: 0.00013202335120060402\n",
      "Epoch 7055  \tTraining Loss: 0.00012934228455065708\tValidation Loss: 0.00013202105887911148\n",
      "Epoch 7056  \tTraining Loss: 0.00012933982675109065\tValidation Loss: 0.00013201907077100688\n",
      "Epoch 7057  \tTraining Loss: 0.00012933736527491368\tValidation Loss: 0.00013201682849860348\n",
      "Epoch 7058  \tTraining Loss: 0.00012933490714899483\tValidation Loss: 0.00013201473515598927\n",
      "Epoch 7059  \tTraining Loss: 0.0001293324476690026\tValidation Loss: 0.0001320125016211617\n",
      "Epoch 7060  \tTraining Loss: 0.00012932998988241168\tValidation Loss: 0.0001320103819814523\n",
      "Epoch 7061  \tTraining Loss: 0.0001293275326124716\tValidation Loss: 0.00013200814575111948\n",
      "Epoch 7062  \tTraining Loss: 0.00012932507518065298\tValidation Loss: 0.00013200593877649676\n",
      "Epoch 7063  \tTraining Loss: 0.0001293226213217452\tValidation Loss: 0.00013200380138374048\n",
      "Epoch 7064  \tTraining Loss: 0.00012932016358453466\tValidation Loss: 0.00013200172388082299\n",
      "Epoch 7065  \tTraining Loss: 0.00012931770979124262\tValidation Loss: 0.00013199946833120687\n",
      "Epoch 7066  \tTraining Loss: 0.00012931525409084024\tValidation Loss: 0.00013199737576885504\n",
      "Epoch 7067  \tTraining Loss: 0.00012931280084386355\tValidation Loss: 0.00013199512329245438\n",
      "Epoch 7068  \tTraining Loss: 0.00012931034688267137\tValidation Loss: 0.00013199295219936422\n",
      "Epoch 7069  \tTraining Loss: 0.00012930789399896226\tValidation Loss: 0.00013199076297664618\n",
      "Epoch 7070  \tTraining Loss: 0.00012930544376882808\tValidation Loss: 0.00013198863308550558\n",
      "Epoch 7071  \tTraining Loss: 0.00012930299050236327\tValidation Loss: 0.00013198656163826913\n",
      "Epoch 7072  \tTraining Loss: 0.0001293005402225625\tValidation Loss: 0.00013198431140267502\n",
      "Epoch 7073  \tTraining Loss: 0.0001292980890236225\tValidation Loss: 0.00013198222390378248\n",
      "Epoch 7074  \tTraining Loss: 0.00012929563929458884\tValidation Loss: 0.00013197997616127806\n",
      "Epoch 7075  \tTraining Loss: 0.0001292931896257376\tValidation Loss: 0.00013197789041500827\n",
      "Epoch 7076  \tTraining Loss: 0.00012929074074546353\tValidation Loss: 0.00013197564475795755\n",
      "Epoch 7077  \tTraining Loss: 0.00012928829344290433\tValidation Loss: 0.00013197346829423452\n",
      "Epoch 7078  \tTraining Loss: 0.00012928584623667249\tValidation Loss: 0.00013197134337623672\n",
      "Epoch 7079  \tTraining Loss: 0.00012928339843426158\tValidation Loss: 0.00013196916793759144\n",
      "Epoch 7080  \tTraining Loss: 0.00012928095204331762\tValidation Loss: 0.00013196709962779484\n",
      "Epoch 7081  \tTraining Loss: 0.00012927850571105274\tValidation Loss: 0.0001319648592747078\n",
      "Epoch 7082  \tTraining Loss: 0.00012927606059689377\tValidation Loss: 0.00013196277910551446\n",
      "Epoch 7083  \tTraining Loss: 0.00012927361516034777\tValidation Loss: 0.00013196053826274644\n",
      "Epoch 7084  \tTraining Loss: 0.0001292711722050577\tValidation Loss: 0.00013195836660779138\n",
      "Epoch 7085  \tTraining Loss: 0.00012926872869058922\tValidation Loss: 0.00013195624606931853\n",
      "Epoch 7086  \tTraining Loss: 0.00012926628517957142\tValidation Loss: 0.0001319541556580797\n",
      "Epoch 7087  \tTraining Loss: 0.0001292638427282914\tValidation Loss: 0.00013195193781224897\n",
      "Epoch 7088  \tTraining Loss: 0.00012926139998419521\tValidation Loss: 0.000131949839852112\n",
      "Epoch 7089  \tTraining Loss: 0.00012925895949515752\tValidation Loss: 0.00013194762159183913\n",
      "Epoch 7090  \tTraining Loss: 0.0001292565171983053\tValidation Loss: 0.0001319454446060478\n",
      "Epoch 7091  \tTraining Loss: 0.00012925407891323107\tValidation Loss: 0.00013194329000299615\n",
      "Epoch 7092  \tTraining Loss: 0.00012925163900541503\tValidation Loss: 0.00013194117644524508\n",
      "Epoch 7093  \tTraining Loss: 0.0001292491999246294\tValidation Loss: 0.00013193909166714365\n",
      "Epoch 7094  \tTraining Loss: 0.00012924676100742628\tValidation Loss: 0.00013193687896095337\n",
      "Epoch 7095  \tTraining Loss: 0.0001292443226781167\tValidation Loss: 0.00013193478579871554\n",
      "Epoch 7096  \tTraining Loss: 0.00012924188572283881\tValidation Loss: 0.00013193257220605004\n",
      "Epoch 7097  \tTraining Loss: 0.00012923944782194298\tValidation Loss: 0.00013193050319962617\n",
      "Epoch 7098  \tTraining Loss: 0.00012923701286229682\tValidation Loss: 0.00013192817436203416\n",
      "Epoch 7099  \tTraining Loss: 0.0001292345775623111\tValidation Loss: 0.00013192619981240806\n",
      "Epoch 7100  \tTraining Loss: 0.00012923214221662372\tValidation Loss: 0.00013192398084581125\n",
      "Epoch 7101  \tTraining Loss: 0.00012922970713849884\tValidation Loss: 0.0001319219135630433\n",
      "Epoch 7102  \tTraining Loss: 0.0001292272731651694\tValidation Loss: 0.00013191968344242612\n",
      "Epoch 7103  \tTraining Loss: 0.0001292248395778004\tValidation Loss: 0.0001319176153790722\n",
      "Epoch 7104  \tTraining Loss: 0.00012922240655036113\tValidation Loss: 0.00013191540954101872\n",
      "Epoch 7105  \tTraining Loss: 0.00012921997417169082\tValidation Loss: 0.00013191314562856644\n",
      "Epoch 7106  \tTraining Loss: 0.0001292175440467481\tValidation Loss: 0.00013191118841576952\n",
      "Epoch 7107  \tTraining Loss: 0.0001292151118852892\tValidation Loss: 0.00013190897677634055\n",
      "Epoch 7108  \tTraining Loss: 0.00012921268149692795\tValidation Loss: 0.00013190691541374545\n",
      "Epoch 7109  \tTraining Loss: 0.00012921025075999517\tValidation Loss: 0.000131904690475818\n",
      "Epoch 7110  \tTraining Loss: 0.0001292078218353922\tValidation Loss: 0.00013190262733243934\n",
      "Epoch 7111  \tTraining Loss: 0.0001292053921711837\tValidation Loss: 0.0001319004261581545\n",
      "Epoch 7112  \tTraining Loss: 0.00012920296399083082\tValidation Loss: 0.0001318983396751511\n",
      "Epoch 7113  \tTraining Loss: 0.0001292005369248717\tValidation Loss: 0.00013189604484682466\n",
      "Epoch 7114  \tTraining Loss: 0.00012919810970482586\tValidation Loss: 0.00013189406006517134\n",
      "Epoch 7115  \tTraining Loss: 0.00012919568336293693\tValidation Loss: 0.00013189187118966888\n",
      "Epoch 7116  \tTraining Loss: 0.00012919325589777242\tValidation Loss: 0.0001318897932467568\n",
      "Epoch 7117  \tTraining Loss: 0.00012919083174114907\tValidation Loss: 0.00013188759300315723\n",
      "Epoch 7118  \tTraining Loss: 0.0001291884051842116\tValidation Loss: 0.00013188553705984984\n",
      "Epoch 7119  \tTraining Loss: 0.00012918598184703025\tValidation Loss: 0.0001318833130859676\n",
      "Epoch 7120  \tTraining Loss: 0.00012918355752941502\tValidation Loss: 0.00013188108290650532\n",
      "Epoch 7121  \tTraining Loss: 0.0001291811352673572\tValidation Loss: 0.00013187911509026656\n",
      "Epoch 7122  \tTraining Loss: 0.00012917871231274896\tValidation Loss: 0.00013187693338884387\n",
      "Epoch 7123  \tTraining Loss: 0.0001291762893619478\tValidation Loss: 0.00013187488408533468\n",
      "Epoch 7124  \tTraining Loss: 0.00012917386836715778\tValidation Loss: 0.00013187266375037828\n",
      "Epoch 7125  \tTraining Loss: 0.0001291714468523657\tValidation Loss: 0.00013187060920387213\n",
      "Epoch 7126  \tTraining Loss: 0.00012916902627852105\tValidation Loss: 0.00013186839388736388\n",
      "Epoch 7127  \tTraining Loss: 0.000129166606358706\tValidation Loss: 0.00013186634169947223\n",
      "Epoch 7128  \tTraining Loss: 0.0001291641874322714\tValidation Loss: 0.00013186403690452396\n",
      "Epoch 7129  \tTraining Loss: 0.00012916176962457772\tValidation Loss: 0.0001318620821614353\n",
      "Epoch 7130  \tTraining Loss: 0.00012915935024519827\tValidation Loss: 0.0001318599052042298\n",
      "Epoch 7131  \tTraining Loss: 0.00012915693252648424\tValidation Loss: 0.00013185783201971705\n",
      "Epoch 7132  \tTraining Loss: 0.0001291545156496606\tValidation Loss: 0.0001318556402376057\n",
      "Epoch 7133  \tTraining Loss: 0.00012915209810543203\tValidation Loss: 0.00013185357019377533\n",
      "Epoch 7134  \tTraining Loss: 0.00012914968314375083\tValidation Loss: 0.00013185138044480386\n",
      "Epoch 7135  \tTraining Loss: 0.000129147266629189\tValidation Loss: 0.00013184916320659181\n",
      "Epoch 7136  \tTraining Loss: 0.00012914485391002218\tValidation Loss: 0.00013184720019773357\n",
      "Epoch 7137  \tTraining Loss: 0.00012914243822015404\tValidation Loss: 0.00013184502701604894\n",
      "Epoch 7138  \tTraining Loss: 0.0001291400246357881\tValidation Loss: 0.00013184296361346259\n",
      "Epoch 7139  \tTraining Loss: 0.00012913761127287835\tValidation Loss: 0.00013184077761667758\n",
      "Epoch 7140  \tTraining Loss: 0.00012913519811835666\tValidation Loss: 0.00013183871262272253\n",
      "Epoch 7141  \tTraining Loss: 0.00012913278656405953\tValidation Loss: 0.00013183652764068288\n",
      "Epoch 7142  \tTraining Loss: 0.00012913037417140166\tValidation Loss: 0.000131834396182946\n",
      "Epoch 7143  \tTraining Loss: 0.00012912796519169375\tValidation Loss: 0.00013183228290364205\n",
      "Epoch 7144  \tTraining Loss: 0.0001291255537964625\tValidation Loss: 0.0001318302526515019\n",
      "Epoch 7145  \tTraining Loss: 0.00012912314378661275\tValidation Loss: 0.00013182805050626346\n",
      "Epoch 7146  \tTraining Loss: 0.00012912073451127882\tValidation Loss: 0.0001318260095675963\n",
      "Epoch 7147  \tTraining Loss: 0.00012911832520641733\tValidation Loss: 0.00013182380676370124\n",
      "Epoch 7148  \tTraining Loss: 0.00012911591745695143\tValidation Loss: 0.0001318217674073323\n",
      "Epoch 7149  \tTraining Loss: 0.0001291135093418362\tValidation Loss: 0.00013181958978996243\n",
      "Epoch 7150  \tTraining Loss: 0.00012911110244678408\tValidation Loss: 0.00013181743592767555\n",
      "Epoch 7151  \tTraining Loss: 0.0001291086975457602\tValidation Loss: 0.00013181535202043165\n",
      "Epoch 7152  \tTraining Loss: 0.0001291062897632711\tValidation Loss: 0.0001318133065207517\n",
      "Epoch 7153  \tTraining Loss: 0.0001291038854542055\tValidation Loss: 0.0001318111304681535\n",
      "Epoch 7154  \tTraining Loss: 0.00012910147900049295\tValidation Loss: 0.00013180904726427822\n",
      "Epoch 7155  \tTraining Loss: 0.00012909907516431195\tValidation Loss: 0.000131806834155327\n",
      "Epoch 7156  \tTraining Loss: 0.00012909667091652637\tValidation Loss: 0.0001318047962439929\n",
      "Epoch 7157  \tTraining Loss: 0.00012909426688204604\tValidation Loss: 0.00013180268110672352\n",
      "Epoch 7158  \tTraining Loss: 0.00012909186498061035\tValidation Loss: 0.00013180048538106098\n",
      "Epoch 7159  \tTraining Loss: 0.00012908946244342864\tValidation Loss: 0.00013179845362927415\n",
      "Epoch 7160  \tTraining Loss: 0.00012908706088011026\tValidation Loss: 0.0001317963589072235\n",
      "Epoch 7161  \tTraining Loss: 0.00012908465867884057\tValidation Loss: 0.0001317942560166128\n",
      "Epoch 7162  \tTraining Loss: 0.00012908225831191188\tValidation Loss: 0.0001317921233767633\n",
      "Epoch 7163  \tTraining Loss: 0.0001290798579968737\tValidation Loss: 0.0001317900157077606\n",
      "Epoch 7164  \tTraining Loss: 0.00012907745788222572\tValidation Loss: 0.00013178788833112138\n",
      "Epoch 7165  \tTraining Loss: 0.00012907505950229963\tValidation Loss: 0.00013178569231980762\n",
      "Epoch 7166  \tTraining Loss: 0.00012907266137524083\tValidation Loss: 0.00013178368552045318\n",
      "Epoch 7167  \tTraining Loss: 0.0001290702635179945\tValidation Loss: 0.000131781567922903\n",
      "Epoch 7168  \tTraining Loss: 0.00012906786565811696\tValidation Loss: 0.00013177946531837956\n",
      "Epoch 7169  \tTraining Loss: 0.00012906546872741337\tValidation Loss: 0.0001317772598173032\n",
      "Epoch 7170  \tTraining Loss: 0.0001290630729105619\tValidation Loss: 0.00013177523035008923\n",
      "Epoch 7171  \tTraining Loss: 0.0001290606761719165\tValidation Loss: 0.0001317731235888591\n",
      "Epoch 7172  \tTraining Loss: 0.0001290582817190935\tValidation Loss: 0.00013177102748160243\n",
      "Epoch 7173  \tTraining Loss: 0.00012905588666989757\tValidation Loss: 0.00013176884188757305\n",
      "Epoch 7174  \tTraining Loss: 0.00012905349385761652\tValidation Loss: 0.00013176681371061396\n",
      "Epoch 7175  \tTraining Loss: 0.00012905109965607255\tValidation Loss: 0.00013176472709881432\n",
      "Epoch 7176  \tTraining Loss: 0.00012904870622639688\tValidation Loss: 0.00013176260959375913\n",
      "Epoch 7177  \tTraining Loss: 0.00012904631421529905\tValidation Loss: 0.00013176051186826687\n",
      "Epoch 7178  \tTraining Loss: 0.00012904392144492927\tValidation Loss: 0.00013175841682168299\n",
      "Epoch 7179  \tTraining Loss: 0.00012904153072689913\tValidation Loss: 0.00013175629455919192\n",
      "Epoch 7180  \tTraining Loss: 0.00012903913955911961\tValidation Loss: 0.00013175419828613137\n",
      "Epoch 7181  \tTraining Loss: 0.00012903674978157227\tValidation Loss: 0.00013175199191780773\n",
      "Epoch 7182  \tTraining Loss: 0.00012903436120358146\tValidation Loss: 0.00013174999298298145\n",
      "Epoch 7183  \tTraining Loss: 0.0001290319706794709\tValidation Loss: 0.0001317478903637759\n",
      "Epoch 7184  \tTraining Loss: 0.00012902958284966536\tValidation Loss: 0.00013174571812249103\n",
      "Epoch 7185  \tTraining Loss: 0.0001290271942304016\tValidation Loss: 0.00013174370119670137\n",
      "Epoch 7186  \tTraining Loss: 0.0001290248069002547\tValidation Loss: 0.0001317415991119019\n",
      "Epoch 7187  \tTraining Loss: 0.00012902241982859332\tValidation Loss: 0.00013173951147705002\n",
      "Epoch 7188  \tTraining Loss: 0.00012902003293450135\tValidation Loss: 0.00013173740192461077\n",
      "Epoch 7189  \tTraining Loss: 0.00012901764844291928\tValidation Loss: 0.00013173522359650214\n",
      "Epoch 7190  \tTraining Loss: 0.0001290152628711163\tValidation Loss: 0.00013173323290626636\n",
      "Epoch 7191  \tTraining Loss: 0.00012901287822246355\tValidation Loss: 0.00013173113102666736\n",
      "Epoch 7192  \tTraining Loss: 0.00012901049359273443\tValidation Loss: 0.00013172904424485536\n",
      "Epoch 7193  \tTraining Loss: 0.0001290081097062188\tValidation Loss: 0.00013172693509503377\n",
      "Epoch 7194  \tTraining Loss: 0.00012900572687229127\tValidation Loss: 0.00013172484793061998\n",
      "Epoch 7195  \tTraining Loss: 0.00012900334349687606\tValidation Loss: 0.00013172274044742014\n",
      "Epoch 7196  \tTraining Loss: 0.00012900096244163064\tValidation Loss: 0.00013172056528287638\n",
      "Epoch 7197  \tTraining Loss: 0.0001289985814143861\tValidation Loss: 0.00013171857804506663\n",
      "Epoch 7198  \tTraining Loss: 0.00012899620028799713\tValidation Loss: 0.0001317164799816284\n",
      "Epoch 7199  \tTraining Loss: 0.00012899381980221837\tValidation Loss: 0.00013171439734879057\n",
      "Epoch 7200  \tTraining Loss: 0.0001289914394758327\tValidation Loss: 0.00013171221129973026\n",
      "Epoch 7201  \tTraining Loss: 0.00012898906112502568\tValidation Loss: 0.00013171020196542425\n",
      "Epoch 7202  \tTraining Loss: 0.00012898668128462134\tValidation Loss: 0.00013170813838924168\n",
      "Epoch 7203  \tTraining Loss: 0.00012898430359207526\tValidation Loss: 0.00013170603596418767\n",
      "Epoch 7204  \tTraining Loss: 0.00012898192587181494\tValidation Loss: 0.0001317038677306838\n",
      "Epoch 7205  \tTraining Loss: 0.0001289795496574996\tValidation Loss: 0.00013170186286739124\n",
      "Epoch 7206  \tTraining Loss: 0.00012897717297030475\tValidation Loss: 0.00013169979670458884\n",
      "Epoch 7207  \tTraining Loss: 0.00012897479609148593\tValidation Loss: 0.00013169769900875765\n",
      "Epoch 7208  \tTraining Loss: 0.00012897242150116498\tValidation Loss: 0.00013169562129450435\n",
      "Epoch 7209  \tTraining Loss: 0.0001289700457136073\tValidation Loss: 0.0001316935460856609\n",
      "Epoch 7210  \tTraining Loss: 0.00012896767154115573\tValidation Loss: 0.0001316914433093939\n",
      "Epoch 7211  \tTraining Loss: 0.00012896529778267301\tValidation Loss: 0.00013168936686489132\n",
      "Epoch 7212  \tTraining Loss: 0.0001289629241475513\tValidation Loss: 0.00013168718064927913\n",
      "Epoch 7213  \tTraining Loss: 0.0001289605533403093\tValidation Loss: 0.00013168520105355023\n",
      "Epoch 7214  \tTraining Loss: 0.00012895817961655552\tValidation Loss: 0.000131683141380589\n",
      "Epoch 7215  \tTraining Loss: 0.00012895580836559935\tValidation Loss: 0.00013168104345651505\n",
      "Epoch 7216  \tTraining Loss: 0.00012895343710072948\tValidation Loss: 0.00013167896963241267\n",
      "Epoch 7217  \tTraining Loss: 0.00012895106611698632\tValidation Loss: 0.00013167679392601462\n",
      "Epoch 7218  \tTraining Loss: 0.00012894869695031185\tValidation Loss: 0.00013167479581110037\n",
      "Epoch 7219  \tTraining Loss: 0.00012894632636892557\tValidation Loss: 0.00013167274336809892\n",
      "Epoch 7220  \tTraining Loss: 0.00012894395828063054\tValidation Loss: 0.00013167056236730536\n",
      "Epoch 7221  \tTraining Loss: 0.00012894159081406833\tValidation Loss: 0.00013166858927606947\n",
      "Epoch 7222  \tTraining Loss: 0.00012893922206461882\tValidation Loss: 0.00013166651177636015\n",
      "Epoch 7223  \tTraining Loss: 0.00012893685501777513\tValidation Loss: 0.0001316644458879105\n",
      "Epoch 7224  \tTraining Loss: 0.00012893448741580486\tValidation Loss: 0.0001316623566998697\n",
      "Epoch 7225  \tTraining Loss: 0.00012893212205639478\tValidation Loss: 0.00013166028952873293\n",
      "Epoch 7226  \tTraining Loss: 0.00012892975550453885\tValidation Loss: 0.00013165822528960977\n",
      "Epoch 7227  \tTraining Loss: 0.00012892739056762987\tValidation Loss: 0.00013165613337369722\n",
      "Epoch 7228  \tTraining Loss: 0.00012892502632388313\tValidation Loss: 0.00013165397856837978\n",
      "Epoch 7229  \tTraining Loss: 0.0001289226626827555\tValidation Loss: 0.0001316519869111385\n",
      "Epoch 7230  \tTraining Loss: 0.00012892029936876904\tValidation Loss: 0.00013164993499207623\n",
      "Epoch 7231  \tTraining Loss: 0.00012891793550203872\tValidation Loss: 0.00013164787547086427\n",
      "Epoch 7232  \tTraining Loss: 0.00012891557345862107\tValidation Loss: 0.0001316457859575428\n",
      "Epoch 7233  \tTraining Loss: 0.00012891321143632058\tValidation Loss: 0.00013164372259799105\n",
      "Epoch 7234  \tTraining Loss: 0.00012891084955788155\tValidation Loss: 0.0001316416388773101\n",
      "Epoch 7235  \tTraining Loss: 0.00012890848942432234\tValidation Loss: 0.00013163949690352366\n",
      "Epoch 7236  \tTraining Loss: 0.00012890612887177862\tValidation Loss: 0.0001316374238379392\n",
      "Epoch 7237  \tTraining Loss: 0.00012890377047241588\tValidation Loss: 0.0001316354486080995\n",
      "Epoch 7238  \tTraining Loss: 0.0001289014104916384\tValidation Loss: 0.0001316334053613287\n",
      "Epoch 7239  \tTraining Loss: 0.00012889905150520966\tValidation Loss: 0.00013163132884081496\n",
      "Epoch 7240  \tTraining Loss: 0.00012889669373195798\tValidation Loss: 0.00013162927210354984\n",
      "Epoch 7241  \tTraining Loss: 0.0001288943352836095\tValidation Loss: 0.0001316272173885606\n",
      "Epoch 7242  \tTraining Loss: 0.00012889197893798944\tValidation Loss: 0.00013162513454176132\n",
      "Epoch 7243  \tTraining Loss: 0.0001288896220101952\tValidation Loss: 0.0001316230783197691\n",
      "Epoch 7244  \tTraining Loss: 0.00012888726600939886\tValidation Loss: 0.00013162091255646287\n",
      "Epoch 7245  \tTraining Loss: 0.0001288849121825638\tValidation Loss: 0.000131618952577416\n",
      "Epoch 7246  \tTraining Loss: 0.00012888255589434784\tValidation Loss: 0.00013161688870229\n",
      "Epoch 7247  \tTraining Loss: 0.00012888020228657993\tValidation Loss: 0.00013161483712943414\n",
      "Epoch 7248  \tTraining Loss: 0.0001288778476027567\tValidation Loss: 0.00013161278621605973\n",
      "Epoch 7249  \tTraining Loss: 0.000128875494604321\tValidation Loss: 0.0001316107069168599\n",
      "Epoch 7250  \tTraining Loss: 0.00012887314185698265\tValidation Loss: 0.00013160865444429053\n",
      "Epoch 7251  \tTraining Loss: 0.00012887078900856313\tValidation Loss: 0.00013160658145164292\n",
      "Epoch 7252  \tTraining Loss: 0.00012886843811310108\tValidation Loss: 0.00013160444284917382\n",
      "Epoch 7253  \tTraining Loss: 0.0001288660874999711\tValidation Loss: 0.00013160240844221227\n",
      "Epoch 7254  \tTraining Loss: 0.00012886373706585817\tValidation Loss: 0.00013160041835279192\n",
      "Epoch 7255  \tTraining Loss: 0.00012886138661054542\tValidation Loss: 0.0001315983867976092\n",
      "Epoch 7256  \tTraining Loss: 0.00012885903662508076\tValidation Loss: 0.00013159632071263353\n",
      "Epoch 7257  \tTraining Loss: 0.000128856688145637\tValidation Loss: 0.00013159427527939248\n",
      "Epoch 7258  \tTraining Loss: 0.00012885433882552032\tValidation Loss: 0.0001315922318539141\n",
      "Epoch 7259  \tTraining Loss: 0.00012885199145011933\tValidation Loss: 0.00013159015997948396\n",
      "Epoch 7260  \tTraining Loss: 0.0001288496438048796\tValidation Loss: 0.0001315881147653744\n",
      "Epoch 7261  \tTraining Loss: 0.00012884729730793762\tValidation Loss: 0.00013158596004338968\n",
      "Epoch 7262  \tTraining Loss: 0.00012884495220485495\tValidation Loss: 0.00013158401055089508\n",
      "Epoch 7263  \tTraining Loss: 0.00012884260490046026\tValidation Loss: 0.0001315819811382515\n",
      "Epoch 7264  \tTraining Loss: 0.00012884026033457217\tValidation Loss: 0.0001315799129998119\n",
      "Epoch 7265  \tTraining Loss: 0.0001288379152017787\tValidation Loss: 0.00013157786959686762\n",
      "Epoch 7266  \tTraining Loss: 0.00012883557078001754\tValidation Loss: 0.00013157580491363107\n",
      "Epoch 7267  \tTraining Loss: 0.00012883322745260373\tValidation Loss: 0.000131573763647106\n",
      "Epoch 7268  \tTraining Loss: 0.00012883088348729992\tValidation Loss: 0.00013157172540881286\n",
      "Epoch 7269  \tTraining Loss: 0.00012882854214542468\tValidation Loss: 0.00013156957076886414\n",
      "Epoch 7270  \tTraining Loss: 0.00012882620065081916\tValidation Loss: 0.00013156762505029938\n",
      "Epoch 7271  \tTraining Loss: 0.0001288238583593313\tValidation Loss: 0.00013156557579844697\n",
      "Epoch 7272  \tTraining Loss: 0.00012882151754376058\tValidation Loss: 0.000131563539447149\n",
      "Epoch 7273  \tTraining Loss: 0.00012881917635431813\tValidation Loss: 0.00013156139805190758\n",
      "Epoch 7274  \tTraining Loss: 0.00012881683759476134\tValidation Loss: 0.00013155943482713988\n",
      "Epoch 7275  \tTraining Loss: 0.0001288144971120223\tValidation Loss: 0.0001315574166624231\n",
      "Epoch 7276  \tTraining Loss: 0.00012881215853066005\tValidation Loss: 0.00013155535879271312\n",
      "Epoch 7277  \tTraining Loss: 0.000128809820408152\tValidation Loss: 0.00013155323767054386\n",
      "Epoch 7278  \tTraining Loss: 0.00012880748316844887\tValidation Loss: 0.0001315512763280133\n",
      "Epoch 7279  \tTraining Loss: 0.00012880514607377277\tValidation Loss: 0.00013154925522926888\n",
      "Epoch 7280  \tTraining Loss: 0.0001288028084674094\tValidation Loss: 0.00013154722610237316\n",
      "Epoch 7281  \tTraining Loss: 0.0001288004727068558\tValidation Loss: 0.0001315451662805624\n",
      "Epoch 7282  \tTraining Loss: 0.0001287981369010085\tValidation Loss: 0.00013154313313179354\n",
      "Epoch 7283  \tTraining Loss: 0.00012879580125641486\tValidation Loss: 0.00013154107893394607\n",
      "Epoch 7284  \tTraining Loss: 0.0001287934672507444\tValidation Loss: 0.00013153904838044287\n",
      "Epoch 7285  \tTraining Loss: 0.00012879113233850133\tValidation Loss: 0.00013153693260910224\n",
      "Epoch 7286  \tTraining Loss: 0.0001287888006114055\tValidation Loss: 0.0001315349700075768\n",
      "Epoch 7287  \tTraining Loss: 0.0001287864668991441\tValidation Loss: 0.00013153295210388278\n",
      "Epoch 7288  \tTraining Loss: 0.00012878413399471523\tValidation Loss: 0.0001315309025987696\n",
      "Epoch 7289  \tTraining Loss: 0.00012878180251976918\tValidation Loss: 0.00013152887452198177\n",
      "Epoch 7290  \tTraining Loss: 0.00012877947024809324\tValidation Loss: 0.0001315268488537633\n",
      "Epoch 7291  \tTraining Loss: 0.00012877713990396027\tValidation Loss: 0.00013152479490447378\n",
      "Epoch 7292  \tTraining Loss: 0.00012877480929594437\tValidation Loss: 0.00013152276846503986\n",
      "Epoch 7293  \tTraining Loss: 0.00012877247905386478\tValidation Loss: 0.00013152072095353774\n",
      "Epoch 7294  \tTraining Loss: 0.00012877015073490904\tValidation Loss: 0.000131518527513436\n",
      "Epoch 7295  \tTraining Loss: 0.00012876782224891377\tValidation Loss: 0.0001315166736939276\n",
      "Epoch 7296  \tTraining Loss: 0.00012876549397757378\tValidation Loss: 0.0001315146509136257\n",
      "Epoch 7297  \tTraining Loss: 0.00012876316584293137\tValidation Loss: 0.00013151263344012706\n",
      "Epoch 7298  \tTraining Loss: 0.00012876083837862426\tValidation Loss: 0.0001315105903566948\n",
      "Epoch 7299  \tTraining Loss: 0.00012875851206287122\tValidation Loss: 0.0001315085695925846\n",
      "Epoch 7300  \tTraining Loss: 0.00012875618507376767\tValidation Loss: 0.00013150655109460385\n",
      "Epoch 7301  \tTraining Loss: 0.00012875386015912254\tValidation Loss: 0.00013150450389936834\n",
      "Epoch 7302  \tTraining Loss: 0.000128751534723934\tValidation Loss: 0.00013150239633685112\n",
      "Epoch 7303  \tTraining Loss: 0.00012874921124010445\tValidation Loss: 0.00013150044820527007\n",
      "Epoch 7304  \tTraining Loss: 0.0001287468870075509\tValidation Loss: 0.00013149844137727852\n",
      "Epoch 7305  \tTraining Loss: 0.0001287445626694659\tValidation Loss: 0.00013149640232913953\n",
      "Epoch 7306  \tTraining Loss: 0.00012874224056908516\tValidation Loss: 0.00013149438470325958\n",
      "Epoch 7307  \tTraining Loss: 0.00012873991726905385\tValidation Loss: 0.00013149236932917546\n",
      "Epoch 7308  \tTraining Loss: 0.00012873759551113663\tValidation Loss: 0.0001314903254392465\n",
      "Epoch 7309  \tTraining Loss: 0.0001287352742601039\tValidation Loss: 0.00013148830923422774\n",
      "Epoch 7310  \tTraining Loss: 0.0001287329525920112\tValidation Loss: 0.00013148627172915173\n",
      "Epoch 7311  \tTraining Loss: 0.0001287306336653035\tValidation Loss: 0.00013148417059273562\n",
      "Epoch 7312  \tTraining Loss: 0.00012872831384001958\tValidation Loss: 0.00013148225264894133\n",
      "Epoch 7313  \tTraining Loss: 0.00012872599424330013\tValidation Loss: 0.00013148022372487213\n",
      "Epoch 7314  \tTraining Loss: 0.00012872367550614534\tValidation Loss: 0.0001314782120001934\n",
      "Epoch 7315  \tTraining Loss: 0.00012872135654103798\tValidation Loss: 0.0001314761766355917\n",
      "Epoch 7316  \tTraining Loss: 0.0001287190396376034\tValidation Loss: 0.00013147416481311606\n",
      "Epoch 7317  \tTraining Loss: 0.00012871672161917237\tValidation Loss: 0.00013147207398877536\n",
      "Epoch 7318  \tTraining Loss: 0.00012871440545259426\tValidation Loss: 0.00013147011047753086\n",
      "Epoch 7319  \tTraining Loss: 0.00012871208921066426\tValidation Loss: 0.00013146802939397224\n",
      "Epoch 7320  \tTraining Loss: 0.0001287097742123959\tValidation Loss: 0.0001314660954405498\n",
      "Epoch 7321  \tTraining Loss: 0.00012870745936706943\tValidation Loss: 0.00013146410107290048\n",
      "Epoch 7322  \tTraining Loss: 0.00012870514391694543\tValidation Loss: 0.00013146209796720477\n",
      "Epoch 7323  \tTraining Loss: 0.00012870283020702494\tValidation Loss: 0.00013146006341201714\n",
      "Epoch 7324  \tTraining Loss: 0.0001287005166475797\tValidation Loss: 0.00013145805589648917\n",
      "Epoch 7325  \tTraining Loss: 0.00012869820301380852\tValidation Loss: 0.00013145602667261348\n",
      "Epoch 7326  \tTraining Loss: 0.00012869589126237346\tValidation Loss: 0.00013145402152614407\n",
      "Epoch 7327  \tTraining Loss: 0.0001286935784612736\tValidation Loss: 0.00013145201904104953\n",
      "Epoch 7328  \tTraining Loss: 0.00012869126773499683\tValidation Loss: 0.00013144990073112934\n",
      "Epoch 7329  \tTraining Loss: 0.00012868895789845984\tValidation Loss: 0.0001314479898968175\n",
      "Epoch 7330  \tTraining Loss: 0.00012868664627613589\tValidation Loss: 0.00013144597530020767\n",
      "Epoch 7331  \tTraining Loss: 0.0001286843370465347\tValidation Loss: 0.00013144397442597916\n",
      "Epoch 7332  \tTraining Loss: 0.00012868202685666422\tValidation Loss: 0.0001314419739930909\n",
      "Epoch 7333  \tTraining Loss: 0.00012867971839180772\tValidation Loss: 0.00013143994433968903\n",
      "Epoch 7334  \tTraining Loss: 0.00012867741004966417\tValidation Loss: 0.0001314379426986439\n",
      "Epoch 7335  \tTraining Loss: 0.00012867510164443093\tValidation Loss: 0.0001314359195050309\n",
      "Epoch 7336  \tTraining Loss: 0.000128672795116469\tValidation Loss: 0.00013143392060663563\n",
      "Epoch 7337  \tTraining Loss: 0.0001286704882853973\tValidation Loss: 0.00013143183740808826\n",
      "Epoch 7338  \tTraining Loss: 0.00012866818296895124\tValidation Loss: 0.0001314299048723377\n",
      "Epoch 7339  \tTraining Loss: 0.00012866587674005195\tValidation Loss: 0.0001314279182109138\n",
      "Epoch 7340  \tTraining Loss: 0.00012866357096031185\tValidation Loss: 0.00013142589939492763\n",
      "Epoch 7341  \tTraining Loss: 0.00012866126698353252\tValidation Loss: 0.00013142390267312562\n",
      "Epoch 7342  \tTraining Loss: 0.0001286589620085074\tValidation Loss: 0.00013142190814153832\n",
      "Epoch 7343  \tTraining Loss: 0.00012865665880361674\tValidation Loss: 0.00013141980288572288\n",
      "Epoch 7344  \tTraining Loss: 0.00012865435595693604\tValidation Loss: 0.00013141788186312735\n",
      "Epoch 7345  \tTraining Loss: 0.0001286520526379005\tValidation Loss: 0.0001314157943290018\n",
      "Epoch 7346  \tTraining Loss: 0.00012864975263844863\tValidation Loss: 0.0001314138985754936\n",
      "Epoch 7347  \tTraining Loss: 0.00012864744957990888\tValidation Loss: 0.0001314119213338158\n",
      "Epoch 7348  \tTraining Loss: 0.000128645148971255\tValidation Loss: 0.00013140990375531484\n",
      "Epoch 7349  \tTraining Loss: 0.00012864284835740572\tValidation Loss: 0.00013140791159820572\n",
      "Epoch 7350  \tTraining Loss: 0.00012864054781682596\tValidation Loss: 0.0001314058968035222\n",
      "Epoch 7351  \tTraining Loss: 0.0001286382490209546\tValidation Loss: 0.0001314039062644771\n",
      "Epoch 7352  \tTraining Loss: 0.00012863594923745954\tValidation Loss: 0.00013140191827917827\n",
      "Epoch 7353  \tTraining Loss: 0.00012863365117610508\tValidation Loss: 0.00013139990126532605\n",
      "Epoch 7354  \tTraining Loss: 0.00012863135350333393\tValidation Loss: 0.00013139782586571558\n",
      "Epoch 7355  \tTraining Loss: 0.0001286290565607511\tValidation Loss: 0.00013139590740571174\n",
      "Epoch 7356  \tTraining Loss: 0.00012862675987052822\tValidation Loss: 0.0001313939314207024\n",
      "Epoch 7357  \tTraining Loss: 0.0001286244626532934\tValidation Loss: 0.000131391947604101\n",
      "Epoch 7358  \tTraining Loss: 0.00012862216720350327\tValidation Loss: 0.00013138993252449115\n",
      "Epoch 7359  \tTraining Loss: 0.0001286198718235624\tValidation Loss: 0.00013138794533565175\n",
      "Epoch 7360  \tTraining Loss: 0.00012861757640840178\tValidation Loss: 0.00013138593620828573\n",
      "Epoch 7361  \tTraining Loss: 0.00012861528285177634\tValidation Loss: 0.0001313839517507049\n",
      "Epoch 7362  \tTraining Loss: 0.00012861298824798324\tValidation Loss: 0.00013138196989385877\n",
      "Epoch 7363  \tTraining Loss: 0.0001286106957876052\tValidation Loss: 0.00013137987266381162\n",
      "Epoch 7364  \tTraining Loss: 0.00012860840404525046\tValidation Loss: 0.00013137798217051374\n",
      "Epoch 7365  \tTraining Loss: 0.00012860611055910654\tValidation Loss: 0.00013137598781695365\n",
      "Epoch 7366  \tTraining Loss: 0.0001286038195419151\tValidation Loss: 0.0001313740077342185\n",
      "Epoch 7367  \tTraining Loss: 0.0001286015275177166\tValidation Loss: 0.00013137202797637934\n",
      "Epoch 7368  \tTraining Loss: 0.00012859923715037415\tValidation Loss: 0.00013137001862651416\n",
      "Epoch 7369  \tTraining Loss: 0.00012859694702529255\tValidation Loss: 0.00013136803778545072\n",
      "Epoch 7370  \tTraining Loss: 0.00012859465668272542\tValidation Loss: 0.0001313660349393667\n",
      "Epoch 7371  \tTraining Loss: 0.00012859236844180042\tValidation Loss: 0.00013136397479997577\n",
      "Epoch 7372  \tTraining Loss: 0.00012859007989600926\tValidation Loss: 0.0001313619873051072\n",
      "Epoch 7373  \tTraining Loss: 0.00012858779248394424\tValidation Loss: 0.00013136009058731813\n",
      "Epoch 7374  \tTraining Loss: 0.0001285855044430559\tValidation Loss: 0.00013135812889207162\n",
      "Epoch 7375  \tTraining Loss: 0.00012858321670069492\tValidation Loss: 0.0001313561325183429\n",
      "Epoch 7376  \tTraining Loss: 0.0001285809309036723\tValidation Loss: 0.00013135415769325505\n",
      "Epoch 7377  \tTraining Loss: 0.00012857864402650721\tValidation Loss: 0.00013135218434456134\n",
      "Epoch 7378  \tTraining Loss: 0.00012857635876944155\tValidation Loss: 0.00013135018140896456\n",
      "Epoch 7379  \tTraining Loss: 0.00012857407383011136\tValidation Loss: 0.00013134820695014467\n",
      "Epoch 7380  \tTraining Loss: 0.00012857178859527668\tValidation Loss: 0.00013134621016018934\n",
      "Epoch 7381  \tTraining Loss: 0.00012856950608453624\tValidation Loss: 0.0001313441521152953\n",
      "Epoch 7382  \tTraining Loss: 0.00012856722249807138\tValidation Loss: 0.00013134227438728669\n",
      "Epoch 7383  \tTraining Loss: 0.00012856493925746297\tValidation Loss: 0.0001313402855477932\n",
      "Epoch 7384  \tTraining Loss: 0.00012856265685169858\tValidation Loss: 0.0001313383150499926\n",
      "Epoch 7385  \tTraining Loss: 0.0001285603741773858\tValidation Loss: 0.00013133632004867996\n",
      "Epoch 7386  \tTraining Loss: 0.0001285580936216734\tValidation Loss: 0.000131334349502475\n",
      "Epoch 7387  \tTraining Loss: 0.00012855581188327969\tValidation Loss: 0.0001313323814397764\n",
      "Epoch 7388  \tTraining Loss: 0.00012855353164335732\tValidation Loss: 0.00013133038410663406\n",
      "Epoch 7389  \tTraining Loss: 0.00012855125195446879\tValidation Loss: 0.0001313284155730725\n",
      "Epoch 7390  \tTraining Loss: 0.0001285489724743971\tValidation Loss: 0.0001313263641141258\n",
      "Epoch 7391  \tTraining Loss: 0.00012854669495350048\tValidation Loss: 0.00013132446100158927\n",
      "Epoch 7392  \tTraining Loss: 0.00012854441568986184\tValidation Loss: 0.00013132250511959731\n",
      "Epoch 7393  \tTraining Loss: 0.0001285421376180556\tValidation Loss: 0.0001313205165622759\n",
      "Epoch 7394  \tTraining Loss: 0.00012853986062943837\tValidation Loss: 0.00013131855088023195\n",
      "Epoch 7395  \tTraining Loss: 0.00012853758298822312\tValidation Loss: 0.0001313165871950604\n",
      "Epoch 7396  \tTraining Loss: 0.0001285353073525718\tValidation Loss: 0.00013131459409429491\n",
      "Epoch 7397  \tTraining Loss: 0.00012853303124584961\tValidation Loss: 0.00013131262998931117\n",
      "Epoch 7398  \tTraining Loss: 0.00012853075560685247\tValidation Loss: 0.00013131064345230252\n",
      "Epoch 7399  \tTraining Loss: 0.0001285284819658107\tValidation Loss: 0.00013130859640418665\n",
      "Epoch 7400  \tTraining Loss: 0.00012852620756278752\tValidation Loss: 0.0001313067290012486\n",
      "Epoch 7401  \tTraining Loss: 0.0001285239338898311\tValidation Loss: 0.00013130475048132233\n",
      "Epoch 7402  \tTraining Loss: 0.00012852166033734753\tValidation Loss: 0.0001313027906399883\n",
      "Epoch 7403  \tTraining Loss: 0.0001285193872237073\tValidation Loss: 0.00013130072392054614\n",
      "Epoch 7404  \tTraining Loss: 0.00012851711583283405\tValidation Loss: 0.00013129883846967327\n",
      "Epoch 7405  \tTraining Loss: 0.00012851484304789968\tValidation Loss: 0.00013129689704899865\n",
      "Epoch 7406  \tTraining Loss: 0.00012851257236076713\tValidation Loss: 0.00013129491448423222\n",
      "Epoch 7407  \tTraining Loss: 0.00012851030149134952\tValidation Loss: 0.00013129295881772357\n",
      "Epoch 7408  \tTraining Loss: 0.0001285080313858669\tValidation Loss: 0.0001312908942417899\n",
      "Epoch 7409  \tTraining Loss: 0.00012850576318641557\tValidation Loss: 0.00013128903101551428\n",
      "Epoch 7410  \tTraining Loss: 0.00012850349251529252\tValidation Loss: 0.00013128708837550397\n",
      "Epoch 7411  \tTraining Loss: 0.0001285012243602325\tValidation Loss: 0.00013128510560185352\n",
      "Epoch 7412  \tTraining Loss: 0.00012849895603276394\tValidation Loss: 0.00013128314961334734\n",
      "Epoch 7413  \tTraining Loss: 0.00012849668787942356\tValidation Loss: 0.0001312811704272746\n",
      "Epoch 7414  \tTraining Loss: 0.000128494421390403\tValidation Loss: 0.00013127921649808415\n",
      "Epoch 7415  \tTraining Loss: 0.0001284921539366613\tValidation Loss: 0.00013127726495558792\n",
      "Epoch 7416  \tTraining Loss: 0.00012848988819104152\tValidation Loss: 0.0001312752837520227\n",
      "Epoch 7417  \tTraining Loss: 0.00012848762298093715\tValidation Loss: 0.0001312732465072003\n",
      "Epoch 7418  \tTraining Loss: 0.0001284853581167214\tValidation Loss: 0.0001312713631358175\n",
      "Epoch 7419  \tTraining Loss: 0.00012848309375398958\tValidation Loss: 0.000131269423684338\n",
      "Epoch 7420  \tTraining Loss: 0.0001284808288118592\tValidation Loss: 0.00013126747629453158\n",
      "Epoch 7421  \tTraining Loss: 0.00012847856555131586\tValidation Loss: 0.00013126549699756838\n",
      "Epoch 7422  \tTraining Loss: 0.000128476302504798\tValidation Loss: 0.00013126354652081917\n",
      "Epoch 7423  \tTraining Loss: 0.00012847403921973573\tValidation Loss: 0.000131261573284511\n",
      "Epoch 7424  \tTraining Loss: 0.00012847177801048523\tValidation Loss: 0.00013125962555179848\n",
      "Epoch 7425  \tTraining Loss: 0.00012846951562784942\tValidation Loss: 0.00013125768014693\n",
      "Epoch 7426  \tTraining Loss: 0.00012846725508547903\tValidation Loss: 0.00013125562005805751\n",
      "Epoch 7427  \tTraining Loss: 0.00012846499581405343\tValidation Loss: 0.00013125376536110044\n",
      "Epoch 7428  \tTraining Loss: 0.00012846273433214306\tValidation Loss: 0.00013125180656961652\n",
      "Epoch 7429  \tTraining Loss: 0.00012846047567791123\tValidation Loss: 0.00013124986303413862\n",
      "Epoch 7430  \tTraining Loss: 0.00012845821582141884\tValidation Loss: 0.00013124791959744645\n",
      "Epoch 7431  \tTraining Loss: 0.00012845595739727832\tValidation Loss: 0.00013124594588702097\n",
      "Epoch 7432  \tTraining Loss: 0.00012845369963776184\tValidation Loss: 0.0001312440016018727\n",
      "Epoch 7433  \tTraining Loss: 0.00012845144132288958\tValidation Loss: 0.00013124206009959398\n",
      "Epoch 7434  \tTraining Loss: 0.0001284491849435335\tValidation Loss: 0.00013124008893141992\n",
      "Epoch 7435  \tTraining Loss: 0.00012844692830378695\tValidation Loss: 0.0001312380624819463\n",
      "Epoch 7436  \tTraining Loss: 0.00012844467305978902\tValidation Loss: 0.00013123618907580814\n",
      "Epoch 7437  \tTraining Loss: 0.00012844241756866122\tValidation Loss: 0.00013123426004632555\n",
      "Epoch 7438  \tTraining Loss: 0.00012844016172763634\tValidation Loss: 0.00013123232305954052\n",
      "Epoch 7439  \tTraining Loss: 0.00012843790777984933\tValidation Loss: 0.0001312303539807776\n",
      "Epoch 7440  \tTraining Loss: 0.00012843565361278506\tValidation Loss: 0.00013122841400561107\n",
      "Epoch 7441  \tTraining Loss: 0.00012843339962130578\tValidation Loss: 0.0001312264510215249\n",
      "Epoch 7442  \tTraining Loss: 0.00012843114730219163\tValidation Loss: 0.00013122443144350269\n",
      "Epoch 7443  \tTraining Loss: 0.00012842889426938832\tValidation Loss: 0.00013122257105964487\n",
      "Epoch 7444  \tTraining Loss: 0.00012842664247694917\tValidation Loss: 0.00013122062209850716\n",
      "Epoch 7445  \tTraining Loss: 0.00012842439159402546\tValidation Loss: 0.00013121860636874122\n",
      "Epoch 7446  \tTraining Loss: 0.00012842214066056165\tValidation Loss: 0.00013121674093399853\n",
      "Epoch 7447  \tTraining Loss: 0.0001284198904168717\tValidation Loss: 0.00013121481900654603\n",
      "Epoch 7448  \tTraining Loss: 0.00012841763960195422\tValidation Loss: 0.0001312128884821129\n",
      "Epoch 7449  \tTraining Loss: 0.00012841539048625176\tValidation Loss: 0.00013121092537078416\n",
      "Epoch 7450  \tTraining Loss: 0.00012841314154428353\tValidation Loss: 0.00013120899125276299\n",
      "Epoch 7451  \tTraining Loss: 0.00012841089238381354\tValidation Loss: 0.00013120703384173317\n",
      "Epoch 7452  \tTraining Loss: 0.00012840864527983084\tValidation Loss: 0.00013120510221379137\n",
      "Epoch 7453  \tTraining Loss: 0.00012840639700406451\tValidation Loss: 0.0001312031727245347\n",
      "Epoch 7454  \tTraining Loss: 0.00012840415061988347\tValidation Loss: 0.00013120112880847932\n",
      "Epoch 7455  \tTraining Loss: 0.0001284019053823343\tValidation Loss: 0.00013119928967879652\n",
      "Epoch 7456  \tTraining Loss: 0.00012839965797760276\tValidation Loss: 0.00013119734632584186\n",
      "Epoch 7457  \tTraining Loss: 0.0001283974134295438\tValidation Loss: 0.00013119541867092602\n",
      "Epoch 7458  \tTraining Loss: 0.00012839516765757902\tValidation Loss: 0.00013119349100746563\n",
      "Epoch 7459  \tTraining Loss: 0.00012839292328641107\tValidation Loss: 0.00013119153276086143\n",
      "Epoch 7460  \tTraining Loss: 0.00012839067963232975\tValidation Loss: 0.00013118960436070537\n",
      "Epoch 7461  \tTraining Loss: 0.00012838843538840022\tValidation Loss: 0.0001311876786294385\n",
      "Epoch 7462  \tTraining Loss: 0.00012838619303454877\tValidation Loss: 0.0001311857229217998\n",
      "Epoch 7463  \tTraining Loss: 0.0001283839504297074\tValidation Loss: 0.00013118371296439064\n",
      "Epoch 7464  \tTraining Loss: 0.0001283817092460575\tValidation Loss: 0.00013118185468380872\n",
      "Epoch 7465  \tTraining Loss: 0.0001283794678641488\tValidation Loss: 0.00013117994142800485\n",
      "Epoch 7466  \tTraining Loss: 0.000128377226071183\tValidation Loss: 0.00013117802016177662\n",
      "Epoch 7467  \tTraining Loss: 0.00012837498609928282\tValidation Loss: 0.00013117606651177015\n",
      "Epoch 7468  \tTraining Loss: 0.0001283727460417495\tValidation Loss: 0.00013117414239334328\n",
      "Epoch 7469  \tTraining Loss: 0.00012837050600219955\tValidation Loss: 0.00013117219489340993\n",
      "Epoch 7470  \tTraining Loss: 0.0001283682677932537\tValidation Loss: 0.00013117027353260616\n",
      "Epoch 7471  \tTraining Loss: 0.00012836602852122614\tValidation Loss: 0.0001311683542897806\n",
      "Epoch 7472  \tTraining Loss: 0.00012836379083143413\tValidation Loss: 0.0001311664046975952\n",
      "Epoch 7473  \tTraining Loss: 0.0001283615539723909\tValidation Loss: 0.00013116440106469263\n",
      "Epoch 7474  \tTraining Loss: 0.0001283593170272503\tValidation Loss: 0.00013116257451772041\n",
      "Epoch 7475  \tTraining Loss: 0.0001283570808856215\tValidation Loss: 0.00013116063685976142\n",
      "Epoch 7476  \tTraining Loss: 0.00012835484439413557\tValidation Loss: 0.00013115871922971976\n",
      "Epoch 7477  \tTraining Loss: 0.00012835260872693104\tValidation Loss: 0.0001311567759482441\n",
      "Epoch 7478  \tTraining Loss: 0.00012835037411664858\tValidation Loss: 0.00013115485843870448\n",
      "Epoch 7479  \tTraining Loss: 0.0001283481388301356\tValidation Loss: 0.00013115294301988288\n",
      "Epoch 7480  \tTraining Loss: 0.000128345905495231\tValidation Loss: 0.00013115099728993612\n",
      "Epoch 7481  \tTraining Loss: 0.0001283436717638121\tValidation Loss: 0.0001311490817415944\n",
      "Epoch 7482  \tTraining Loss: 0.00012834143851029966\tValidation Loss: 0.0001311470588991249\n",
      "Epoch 7483  \tTraining Loss: 0.00012833920786020048\tValidation Loss: 0.00013114523637624358\n",
      "Epoch 7484  \tTraining Loss: 0.00012833697437971564\tValidation Loss: 0.00013114333516178842\n",
      "Epoch 7485  \tTraining Loss: 0.00012833474339083977\tValidation Loss: 0.00013114139337971157\n",
      "Epoch 7486  \tTraining Loss: 0.0001283325122511058\tValidation Loss: 0.0001311394796883254\n",
      "Epoch 7487  \tTraining Loss: 0.00012833028119357655\tValidation Loss: 0.00013113754189688804\n",
      "Epoch 7488  \tTraining Loss: 0.0001283280519118432\tValidation Loss: 0.00013113563043577945\n",
      "Epoch 7489  \tTraining Loss: 0.0001283258215898372\tValidation Loss: 0.000131133721059968\n",
      "Epoch 7490  \tTraining Loss: 0.00012832359286322565\tValidation Loss: 0.00013113178119736636\n",
      "Epoch 7491  \tTraining Loss: 0.00012832136445166848\tValidation Loss: 0.00013112987157271747\n",
      "Epoch 7492  \tTraining Loss: 0.0001283191361330649\tValidation Loss: 0.0001311278807437397\n",
      "Epoch 7493  \tTraining Loss: 0.0001283169101787053\tValidation Loss: 0.00013112603343656663\n",
      "Epoch 7494  \tTraining Loss: 0.00012831468219575418\tValidation Loss: 0.00013112413579911627\n",
      "Epoch 7495  \tTraining Loss: 0.00012831245543967553\tValidation Loss: 0.0001311222043179617\n",
      "Epoch 7496  \tTraining Loss: 0.0001283102297548695\tValidation Loss: 0.00013112029715702295\n",
      "Epoch 7497  \tTraining Loss: 0.00012830800344076256\tValidation Loss: 0.0001311183089618276\n",
      "Epoch 7498  \tTraining Loss: 0.00012830577922477124\tValidation Loss: 0.00013111644761760232\n",
      "Epoch 7499  \tTraining Loss: 0.00012830355421242803\tValidation Loss: 0.0001311145580154212\n",
      "Epoch 7500  \tTraining Loss: 0.00012830132973177806\tValidation Loss: 0.00013111263288241725\n",
      "Epoch 7501  \tTraining Loss: 0.00012829910673415134\tValidation Loss: 0.00013111073201792713\n",
      "Epoch 7502  \tTraining Loss: 0.00012829688352938054\tValidation Loss: 0.00013110874865296767\n",
      "Epoch 7503  \tTraining Loss: 0.00012829466198158342\tValidation Loss: 0.00013110690758540665\n",
      "Epoch 7504  \tTraining Loss: 0.00012829243926494667\tValidation Loss: 0.00013110501596940344\n",
      "Epoch 7505  \tTraining Loss: 0.00012829021713349001\tValidation Loss: 0.00013110309012527227\n",
      "Epoch 7506  \tTraining Loss: 0.00012828799669927114\tValidation Loss: 0.00013110118855458886\n",
      "Epoch 7507  \tTraining Loss: 0.0001282857752736544\tValidation Loss: 0.00013109928837742705\n",
      "Epoch 7508  \tTraining Loss: 0.0001282835554820844\tValidation Loss: 0.00013109735744987148\n",
      "Epoch 7509  \tTraining Loss: 0.00012828133592193025\tValidation Loss: 0.00013109545713475357\n",
      "Epoch 7510  \tTraining Loss: 0.00012827911603329468\tValidation Loss: 0.00013109353284312988\n",
      "Epoch 7511  \tTraining Loss: 0.00012827689844223012\tValidation Loss: 0.0001310915519813943\n",
      "Epoch 7512  \tTraining Loss: 0.00012827468063230236\tValidation Loss: 0.00013108974629682808\n",
      "Epoch 7513  \tTraining Loss: 0.0001282724626057383\tValidation Loss: 0.00013108782927647626\n",
      "Epoch 7514  \tTraining Loss: 0.0001282702456117409\tValidation Loss: 0.00013108593287352305\n",
      "Epoch 7515  \tTraining Loss: 0.0001282680281474344\tValidation Loss: 0.00013108403670397044\n",
      "Epoch 7516  \tTraining Loss: 0.00012826581264127192\tValidation Loss: 0.0001310821094903998\n",
      "Epoch 7517  \tTraining Loss: 0.00012826359667101152\tValidation Loss: 0.00013108021310641733\n",
      "Epoch 7518  \tTraining Loss: 0.00012826138104884624\tValidation Loss: 0.0001310782927603269\n",
      "Epoch 7519  \tTraining Loss: 0.00012825916693930202\tValidation Loss: 0.000131076399225178\n",
      "Epoch 7520  \tTraining Loss: 0.00012825695191221286\tValidation Loss: 0.0001310745075275265\n",
      "Epoch 7521  \tTraining Loss: 0.00012825473891328688\tValidation Loss: 0.00013107250206614529\n",
      "Epoch 7522  \tTraining Loss: 0.00012825252673219362\tValidation Loss: 0.0001310707000417002\n",
      "Epoch 7523  \tTraining Loss: 0.00012825031273125428\tValidation Loss: 0.00013106879346598692\n",
      "Epoch 7524  \tTraining Loss: 0.00012824810119970814\tValidation Loss: 0.00013106690368040356\n",
      "Epoch 7525  \tTraining Loss: 0.00012824588862190198\tValidation Loss: 0.0001310650136106135\n",
      "Epoch 7526  \tTraining Loss: 0.00012824367759221853\tValidation Loss: 0.00013106309218624733\n",
      "Epoch 7527  \tTraining Loss: 0.00012824146695820582\tValidation Loss: 0.0001310612016359026\n",
      "Epoch 7528  \tTraining Loss: 0.00012823925587624957\tValidation Loss: 0.0001310593134546578\n",
      "Epoch 7529  \tTraining Loss: 0.00012823704679622313\tValidation Loss: 0.000131057394518762\n",
      "Epoch 7530  \tTraining Loss: 0.00012823483715897305\tValidation Loss: 0.0001310555065241058\n",
      "Epoch 7531  \tTraining Loss: 0.00012823262841710784\tValidation Loss: 0.00013105351159566764\n",
      "Epoch 7532  \tTraining Loss: 0.00012823042155033686\tValidation Loss: 0.0001310517159337694\n",
      "Epoch 7533  \tTraining Loss: 0.00012822821221191886\tValidation Loss: 0.00013104984179944563\n",
      "Epoch 7534  \tTraining Loss: 0.00012822600538303216\tValidation Loss: 0.00013104792663871516\n",
      "Epoch 7535  \tTraining Loss: 0.0001282237983532248\tValidation Loss: 0.00013104604035520781\n",
      "Epoch 7536  \tTraining Loss: 0.00012822159141431656\tValidation Loss: 0.00013104412929172002\n",
      "Epoch 7537  \tTraining Loss: 0.00012821938625133864\tValidation Loss: 0.00013104224525409215\n",
      "Epoch 7538  \tTraining Loss: 0.00012821718003472722\tValidation Loss: 0.00013104036307208611\n",
      "Epoch 7539  \tTraining Loss: 0.00012821497538226124\tValidation Loss: 0.0001310384498323765\n",
      "Epoch 7540  \tTraining Loss: 0.0001282127710891963\tValidation Loss: 0.0001310365675847617\n",
      "Epoch 7541  \tTraining Loss: 0.00012821056691731278\tValidation Loss: 0.00013103460497409285\n",
      "Epoch 7542  \tTraining Loss: 0.00012820836490661108\tValidation Loss: 0.0001310327835969593\n",
      "Epoch 7543  \tTraining Loss: 0.0001282061610406496\tValidation Loss: 0.00013103091310453884\n",
      "Epoch 7544  \tTraining Loss: 0.00012820395827303513\tValidation Loss: 0.00013102900819932927\n",
      "Epoch 7545  \tTraining Loss: 0.00012820175671416622\tValidation Loss: 0.00013102712834472542\n",
      "Epoch 7546  \tTraining Loss: 0.00012819955439879194\tValidation Loss: 0.00013102524981917403\n",
      "Epoch 7547  \tTraining Loss: 0.00012819735392332886\tValidation Loss: 0.00013102334015431254\n",
      "Epoch 7548  \tTraining Loss: 0.0001281951532489513\tValidation Loss: 0.00013102146175925374\n",
      "Epoch 7549  \tTraining Loss: 0.00012819295263991252\tValidation Loss: 0.00013101955884931346\n",
      "Epoch 7550  \tTraining Loss: 0.00012819075382336438\tValidation Loss: 0.00013101768306738766\n",
      "Epoch 7551  \tTraining Loss: 0.00012818855454353612\tValidation Loss: 0.0001310157266218884\n",
      "Epoch 7552  \tTraining Loss: 0.00012818635686337294\tValidation Loss: 0.00013101391075670394\n",
      "Epoch 7553  \tTraining Loss: 0.00012818415834675305\tValidation Loss: 0.00013101204592165085\n",
      "Epoch 7554  \tTraining Loss: 0.00012818195996422058\tValidation Loss: 0.00013101014651652922\n",
      "Epoch 7555  \tTraining Loss: 0.00012817976374655083\tValidation Loss: 0.00013100827225930072\n",
      "Epoch 7556  \tTraining Loss: 0.00012817756629101212\tValidation Loss: 0.00013100639926654883\n",
      "Epoch 7557  \tTraining Loss: 0.0001281753701995191\tValidation Loss: 0.00013100449501202358\n",
      "Epoch 7558  \tTraining Loss: 0.00012817317485980628\tValidation Loss: 0.00013100262215887795\n",
      "Epoch 7559  \tTraining Loss: 0.0001281709788846139\tValidation Loss: 0.0001310007514756442\n",
      "Epoch 7560  \tTraining Loss: 0.0001281687847164454\tValidation Loss: 0.00013099884959915804\n",
      "Epoch 7561  \tTraining Loss: 0.0001281665908587517\tValidation Loss: 0.00013099689703230358\n",
      "Epoch 7562  \tTraining Loss: 0.00012816439730536198\tValidation Loss: 0.0001309950912534354\n",
      "Epoch 7563  \tTraining Loss: 0.00012816220425178305\tValidation Loss: 0.00013099323275677328\n",
      "Epoch 7564  \tTraining Loss: 0.00012816001065031317\tValidation Loss: 0.0001309913659962838\n",
      "Epoch 7565  \tTraining Loss: 0.00012815781869606193\tValidation Loss: 0.00013098946576235832\n",
      "Epoch 7566  \tTraining Loss: 0.00012815562696787589\tValidation Loss: 0.0001309875965766473\n",
      "Epoch 7567  \tTraining Loss: 0.00012815343486738756\tValidation Loss: 0.0001309857295342765\n",
      "Epoch 7568  \tTraining Loss: 0.00012815124482360297\tValidation Loss: 0.00013098383133666936\n",
      "Epoch 7569  \tTraining Loss: 0.000128149054097573\tValidation Loss: 0.00013098196476990777\n",
      "Epoch 7570  \tTraining Loss: 0.00012814686389529126\tValidation Loss: 0.00013098007335045223\n",
      "Epoch 7571  \tTraining Loss: 0.00012814467548746703\tValidation Loss: 0.0001309781275094708\n",
      "Epoch 7572  \tTraining Loss: 0.00012814248657990696\tValidation Loss: 0.00013097635442033093\n",
      "Epoch 7573  \tTraining Loss: 0.00012814029817626369\tValidation Loss: 0.00013097446981578977\n",
      "Epoch 7574  \tTraining Loss: 0.00012813811004373318\tValidation Loss: 0.00013097260690535137\n",
      "Epoch 7575  \tTraining Loss: 0.00012813592204542358\tValidation Loss: 0.00013097071698970242\n",
      "Epoch 7576  \tTraining Loss: 0.00012813373580232662\tValidation Loss: 0.00013096885421823198\n",
      "Epoch 7577  \tTraining Loss: 0.00012813154850699142\tValidation Loss: 0.00013096699306287062\n",
      "Epoch 7578  \tTraining Loss: 0.00012812936276017958\tValidation Loss: 0.00013096510044491923\n",
      "Epoch 7579  \tTraining Loss: 0.00012812717738955022\tValidation Loss: 0.00013096323950821645\n",
      "Epoch 7580  \tTraining Loss: 0.00012812499156532146\tValidation Loss: 0.00013096138056572322\n",
      "Epoch 7581  \tTraining Loss: 0.00012812280808677901\tValidation Loss: 0.00013095940860775824\n",
      "Epoch 7582  \tTraining Loss: 0.00012812062470401986\tValidation Loss: 0.00013095763900994946\n",
      "Epoch 7583  \tTraining Loss: 0.00012811844012093355\tValidation Loss: 0.00013095576459015273\n",
      "Epoch 7584  \tTraining Loss: 0.00012811625746898242\tValidation Loss: 0.00013095390800818408\n",
      "Epoch 7585  \tTraining Loss: 0.00012811407402590837\tValidation Loss: 0.00013095205090960898\n",
      "Epoch 7586  \tTraining Loss: 0.0001281118923596149\tValidation Loss: 0.00013095016176530995\n",
      "Epoch 7587  \tTraining Loss: 0.00012810971060870228\tValidation Loss: 0.00013094830445857164\n",
      "Epoch 7588  \tTraining Loss: 0.00012810752877759622\tValidation Loss: 0.00013094642213434395\n",
      "Epoch 7589  \tTraining Loss: 0.00012810534889156155\tValidation Loss: 0.0001309445675793604\n",
      "Epoch 7590  \tTraining Loss: 0.00012810316785288235\tValidation Loss: 0.00013094271449283552\n",
      "Epoch 7591  \tTraining Loss: 0.00012810098850811588\tValidation Loss: 0.00013094074829750584\n",
      "Epoch 7592  \tTraining Loss: 0.00012809881060125685\tValidation Loss: 0.00013093898416998895\n",
      "Epoch 7593  \tTraining Loss: 0.00012809663034206683\tValidation Loss: 0.0001309371422961453\n",
      "Epoch 7594  \tTraining Loss: 0.00012809445273889543\tValidation Loss: 0.00013093525900625586\n",
      "Epoch 7595  \tTraining Loss: 0.00012809227458512415\tValidation Loss: 0.00013093340566887253\n",
      "Epoch 7596  \tTraining Loss: 0.0001280900968167679\tValidation Loss: 0.00013093152674213917\n",
      "Epoch 7597  \tTraining Loss: 0.0001280879205462474\tValidation Loss: 0.00013092967575708818\n",
      "Epoch 7598  \tTraining Loss: 0.00012808574334518744\tValidation Loss: 0.00013092782635774002\n",
      "Epoch 7599  \tTraining Loss: 0.0001280835678080444\tValidation Loss: 0.000130925945198678\n",
      "Epoch 7600  \tTraining Loss: 0.00012808139240875083\tValidation Loss: 0.00013092409599276165\n",
      "Epoch 7601  \tTraining Loss: 0.00012807921677722965\tValidation Loss: 0.0001309221402983941\n",
      "Epoch 7602  \tTraining Loss: 0.00012807704451289802\tValidation Loss: 0.00013092038240366895\n",
      "Epoch 7603  \tTraining Loss: 0.00012807486902035914\tValidation Loss: 0.00013091854632386486\n",
      "Epoch 7604  \tTraining Loss: 0.00012807269563664886\tValidation Loss: 0.00013091666854455193\n",
      "Epoch 7605  \tTraining Loss: 0.0001280705228386454\tValidation Loss: 0.00013091482077754688\n",
      "Epoch 7606  \tTraining Loss: 0.00012806834948802856\tValidation Loss: 0.00013091297452895922\n",
      "Epoch 7607  \tTraining Loss: 0.00012806617799662238\tValidation Loss: 0.00013091109661745966\n",
      "Epoch 7608  \tTraining Loss: 0.00012806400620449346\tValidation Loss: 0.0001309092510139333\n",
      "Epoch 7609  \tTraining Loss: 0.00012806183452530458\tValidation Loss: 0.0001309073800726108\n",
      "Epoch 7610  \tTraining Loss: 0.00012805966460624919\tValidation Loss: 0.00013090553717108708\n",
      "Epoch 7611  \tTraining Loss: 0.00012805749362138094\tValidation Loss: 0.00013090369558874115\n",
      "Epoch 7612  \tTraining Loss: 0.00012805532467707797\tValidation Loss: 0.00013090174105841713\n",
      "Epoch 7613  \tTraining Loss: 0.00012805315646051917\tValidation Loss: 0.00013089998817423728\n",
      "Epoch 7614  \tTraining Loss: 0.00012805098625197386\tValidation Loss: 0.00013089813024492512\n",
      "Epoch 7615  \tTraining Loss: 0.0001280488189188729\tValidation Loss: 0.0001308962906185942\n",
      "Epoch 7616  \tTraining Loss: 0.00012804665031435853\tValidation Loss: 0.0001308944503170439\n",
      "Epoch 7617  \tTraining Loss: 0.00012804448300608257\tValidation Loss: 0.0001308925775937581\n",
      "Epoch 7618  \tTraining Loss: 0.00012804231656637617\tValidation Loss: 0.00013089073717959714\n",
      "Epoch 7619  \tTraining Loss: 0.00012804014941642354\tValidation Loss: 0.0001308888987151138\n",
      "Epoch 7620  \tTraining Loss: 0.00012803798397834236\tValidation Loss: 0.00013088702842132077\n",
      "Epoch 7621  \tTraining Loss: 0.00012803581853207784\tValidation Loss: 0.00013088519050054437\n",
      "Epoch 7622  \tTraining Loss: 0.00012803365314148135\tValidation Loss: 0.00013088324627659914\n",
      "Epoch 7623  \tTraining Loss: 0.00012803149066648945\tValidation Loss: 0.00013088149946786717\n",
      "Epoch 7624  \tTraining Loss: 0.00012802932518628438\tValidation Loss: 0.00013087967458614975\n",
      "Epoch 7625  \tTraining Loss: 0.00012802716185725371\tValidation Loss: 0.00013087780781512366\n",
      "Epoch 7626  \tTraining Loss: 0.00012802499902345955\tValidation Loss: 0.00013087597142207543\n",
      "Epoch 7627  \tTraining Loss: 0.00012802283567663254\tValidation Loss: 0.00013087413645870358\n",
      "Epoch 7628  \tTraining Loss: 0.0001280206742208606\tValidation Loss: 0.00013087226958532488\n",
      "Epoch 7629  \tTraining Loss: 0.00012801851239330994\tValidation Loss: 0.0001308704353642359\n",
      "Epoch 7630  \tTraining Loss: 0.0001280163507335649\tValidation Loss: 0.00013086857549525954\n",
      "Epoch 7631  \tTraining Loss: 0.00012801419078220852\tValidation Loss: 0.00013086674397986753\n",
      "Epoch 7632  \tTraining Loss: 0.00012801202978538208\tValidation Loss: 0.00013086491367749836\n",
      "Epoch 7633  \tTraining Loss: 0.00012800987090561588\tValidation Loss: 0.00013086297061899887\n",
      "Epoch 7634  \tTraining Loss: 0.000128007712586326\tValidation Loss: 0.0001308612288190423\n",
      "Epoch 7635  \tTraining Loss: 0.00012800555236377708\tValidation Loss: 0.00013085938185670434\n",
      "Epoch 7636  \tTraining Loss: 0.00012800339499988912\tValidation Loss: 0.00013085755356106757\n",
      "Epoch 7637  \tTraining Loss: 0.0001280012363679933\tValidation Loss: 0.00013085572449975246\n",
      "Epoch 7638  \tTraining Loss: 0.00012799907903028165\tValidation Loss: 0.0001308538627673852\n",
      "Epoch 7639  \tTraining Loss: 0.0001279969225585528\tValidation Loss: 0.00013085203368762418\n",
      "Epoch 7640  \tTraining Loss: 0.00012799476537201558\tValidation Loss: 0.00013085020645877782\n",
      "Epoch 7641  \tTraining Loss: 0.00012799260988713738\tValidation Loss: 0.0001308483471482842\n",
      "Epoch 7642  \tTraining Loss: 0.00012799045440775952\tValidation Loss: 0.00013084652055263686\n",
      "Epoch 7643  \tTraining Loss: 0.00012798829892684716\tValidation Loss: 0.0001308445877872627\n",
      "Epoch 7644  \tTraining Loss: 0.0001279861464423307\tValidation Loss: 0.00013084285202655512\n",
      "Epoch 7645  \tTraining Loss: 0.00012798399090905378\tValidation Loss: 0.00013084103828106872\n",
      "Epoch 7646  \tTraining Loss: 0.00012798183750211054\tValidation Loss: 0.00013083918244169567\n",
      "Epoch 7647  \tTraining Loss: 0.00012797968463575582\tValidation Loss: 0.00013083735733744056\n",
      "Epoch 7648  \tTraining Loss: 0.0001279775312278296\tValidation Loss: 0.00013083553356862417\n",
      "Epoch 7649  \tTraining Loss: 0.00012797537967721898\tValidation Loss: 0.00013083367763977194\n",
      "Epoch 7650  \tTraining Loss: 0.00012797322781572684\tValidation Loss: 0.00013083185470695566\n",
      "Epoch 7651  \tTraining Loss: 0.00012797107604599383\tValidation Loss: 0.00013083000581513675\n",
      "Epoch 7652  \tTraining Loss: 0.00012796892606295424\tValidation Loss: 0.0001308281855916308\n",
      "Epoch 7653  \tTraining Loss: 0.00012796677498966703\tValidation Loss: 0.0001308263664750316\n",
      "Epoch 7654  \tTraining Loss: 0.00012796462586240213\tValidation Loss: 0.0001308244347903043\n",
      "Epoch 7655  \tTraining Loss: 0.00012796247762477786\tValidation Loss: 0.0001308227039849889\n",
      "Epoch 7656  \tTraining Loss: 0.0001279603272999607\tValidation Loss: 0.00013082089569885758\n",
      "Epoch 7657  \tTraining Loss: 0.000127958179652506\tValidation Loss: 0.00013081904537599328\n",
      "Epoch 7658  \tTraining Loss: 0.00012795603139737307\tValidation Loss: 0.00013081722606797264\n",
      "Epoch 7659  \tTraining Loss: 0.00012795388353771913\tValidation Loss: 0.00013081538024651746\n",
      "Epoch 7660  \tTraining Loss: 0.000127951737173899\tValidation Loss: 0.00013081356331143504\n",
      "Epoch 7661  \tTraining Loss: 0.00012794958986379472\tValidation Loss: 0.0001308117476450253\n",
      "Epoch 7662  \tTraining Loss: 0.00012794744418620702\tValidation Loss: 0.00013080989945385963\n",
      "Epoch 7663  \tTraining Loss: 0.0001279452986893642\tValidation Loss: 0.0001308080842376115\n",
      "Epoch 7664  \tTraining Loss: 0.0001279431528255078\tValidation Loss: 0.000130806190695633\n",
      "Epoch 7665  \tTraining Loss: 0.00012794101018340963\tValidation Loss: 0.00013080443266935492\n",
      "Epoch 7666  \tTraining Loss: 0.000127938865106078\tValidation Loss: 0.0001308026286450293\n",
      "Epoch 7667  \tTraining Loss: 0.00012793671941386723\tValidation Loss: 0.00013080080213576879\n",
      "Epoch 7668  \tTraining Loss: 0.00012793457015323133\tValidation Loss: 0.0001307989552967136\n",
      "Epoch 7669  \tTraining Loss: 0.0001279324204282119\tValidation Loss: 0.0001307971266597099\n",
      "Epoch 7670  \tTraining Loss: 0.00012793027263609345\tValidation Loss: 0.00013079527119630127\n",
      "Epoch 7671  \tTraining Loss: 0.00012792812458300025\tValidation Loss: 0.00013079345161360662\n",
      "Epoch 7672  \tTraining Loss: 0.00012792597671975738\tValidation Loss: 0.0001307916069968042\n",
      "Epoch 7673  \tTraining Loss: 0.00012792383044862035\tValidation Loss: 0.00013078979166956305\n",
      "Epoch 7674  \tTraining Loss: 0.00012792168317014166\tValidation Loss: 0.00013078797758182787\n",
      "Epoch 7675  \tTraining Loss: 0.00012791953813860352\tValidation Loss: 0.00013078605121948078\n",
      "Epoch 7676  \tTraining Loss: 0.0001279173935106046\tValidation Loss: 0.00013078432517462114\n",
      "Epoch 7677  \tTraining Loss: 0.00012791524737104946\tValidation Loss: 0.00013078249385403373\n",
      "Epoch 7678  \tTraining Loss: 0.00012791310345808523\tValidation Loss: 0.00013078068176594745\n",
      "Epoch 7679  \tTraining Loss: 0.00012791095857126556\tValidation Loss: 0.0001307788687658788\n",
      "Epoch 7680  \tTraining Loss: 0.00012790881548509014\tValidation Loss: 0.0001307770225958204\n",
      "Epoch 7681  \tTraining Loss: 0.0001279066723246243\tValidation Loss: 0.0001307752096221645\n",
      "Epoch 7682  \tTraining Loss: 0.00012790452913640733\tValidation Loss: 0.00013077337033605763\n",
      "Epoch 7683  \tTraining Loss: 0.0001279023877579505\tValidation Loss: 0.00013077156005645874\n",
      "Epoch 7684  \tTraining Loss: 0.00012790024525549445\tValidation Loss: 0.00013076975081980803\n",
      "Epoch 7685  \tTraining Loss: 0.00012789810471268315\tValidation Loss: 0.00013076782936193598\n",
      "Epoch 7686  \tTraining Loss: 0.00012789596513680204\tValidation Loss: 0.0001307661080707749\n",
      "Epoch 7687  \tTraining Loss: 0.0001278938236358435\tValidation Loss: 0.00013076428147060587\n",
      "Epoch 7688  \tTraining Loss: 0.0001278916846081929\tValidation Loss: 0.00013076247430872829\n",
      "Epoch 7689  \tTraining Loss: 0.00012788954447793982\tValidation Loss: 0.00013076066622079517\n",
      "Epoch 7690  \tTraining Loss: 0.00012788740601633758\tValidation Loss: 0.00013075882486932505\n",
      "Epoch 7691  \tTraining Loss: 0.00012788526773577469\tValidation Loss: 0.00013075701690209773\n",
      "Epoch 7692  \tTraining Loss: 0.00012788312916120202\tValidation Loss: 0.00013075518249165198\n",
      "Epoch 7693  \tTraining Loss: 0.00012788099266112163\tValidation Loss: 0.00013075337725556538\n",
      "Epoch 7694  \tTraining Loss: 0.0001278788548999538\tValidation Loss: 0.00013075157302513377\n",
      "Epoch 7695  \tTraining Loss: 0.00012787671880508007\tValidation Loss: 0.00013074965666720533\n",
      "Epoch 7696  \tTraining Loss: 0.00012787458425861874\tValidation Loss: 0.000130747940323967\n",
      "Epoch 7697  \tTraining Loss: 0.00012787244734867892\tValidation Loss: 0.00013074611861617504\n",
      "Epoch 7698  \tTraining Loss: 0.00012787031319397418\tValidation Loss: 0.00013074431653079514\n",
      "Epoch 7699  \tTraining Loss: 0.00012786817779172297\tValidation Loss: 0.0001307425134836596\n",
      "Epoch 7700  \tTraining Loss: 0.00012786604391064622\tValidation Loss: 0.0001307406770612627\n",
      "Epoch 7701  \tTraining Loss: 0.00012786391049945096\tValidation Loss: 0.0001307388741981292\n",
      "Epoch 7702  \tTraining Loss: 0.0001278617765375611\tValidation Loss: 0.0001307370729584928\n",
      "Epoch 7703  \tTraining Loss: 0.0001278596446447055\tValidation Loss: 0.00013073523891887772\n",
      "Epoch 7704  \tTraining Loss: 0.00012785751207886325\tValidation Loss: 0.00013073343850147193\n",
      "Epoch 7705  \tTraining Loss: 0.0001278553800521353\tValidation Loss: 0.00013073161128017446\n",
      "Epoch 7706  \tTraining Loss: 0.0001278532498970477\tValidation Loss: 0.00013072973434161925\n",
      "Epoch 7707  \tTraining Loss: 0.00012785111888927007\tValidation Loss: 0.00013072802483353308\n",
      "Epoch 7708  \tTraining Loss: 0.00012784898873973024\tValidation Loss: 0.00013072620344025234\n",
      "Epoch 7709  \tTraining Loss: 0.00012784685853036486\tValidation Loss: 0.00013072440612683502\n",
      "Epoch 7710  \tTraining Loss: 0.00012784472881381223\tValidation Loss: 0.0001307225800027193\n",
      "Epoch 7711  \tTraining Loss: 0.000127842600413885\tValidation Loss: 0.0001307207829513947\n",
      "Epoch 7712  \tTraining Loss: 0.00012784047112764452\tValidation Loss: 0.00013071898691457572\n",
      "Epoch 7713  \tTraining Loss: 0.0001278383437407603\tValidation Loss: 0.0001307171578110124\n",
      "Epoch 7714  \tTraining Loss: 0.0001278362160596739\tValidation Loss: 0.0001307153624850846\n",
      "Epoch 7715  \tTraining Loss: 0.00012783408853934394\tValidation Loss: 0.0001307135402206408\n",
      "Epoch 7716  \tTraining Loss: 0.0001278319630971928\tValidation Loss: 0.00013071166861027401\n",
      "Epoch 7717  \tTraining Loss: 0.00012782983694345493\tValidation Loss: 0.00013070996407170827\n",
      "Epoch 7718  \tTraining Loss: 0.00012782771129653687\tValidation Loss: 0.0001307081476230358\n",
      "Epoch 7719  \tTraining Loss: 0.00012782558595790278\tValidation Loss: 0.0001307063554552665\n",
      "Epoch 7720  \tTraining Loss: 0.0001278234607382752\tValidation Loss: 0.00013070453434288896\n",
      "Epoch 7721  \tTraining Loss: 0.00012782133720759344\tValidation Loss: 0.00013070274246862435\n",
      "Epoch 7722  \tTraining Loss: 0.00012781921260090695\tValidation Loss: 0.00013070095156648674\n",
      "Epoch 7723  \tTraining Loss: 0.00012781708970477734\tValidation Loss: 0.00013069912748299537\n",
      "Epoch 7724  \tTraining Loss: 0.00012781496688880767\tValidation Loss: 0.00013069733735123813\n",
      "Epoch 7725  \tTraining Loss: 0.00012781284385283411\tValidation Loss: 0.00013069552013526855\n",
      "Epoch 7726  \tTraining Loss: 0.00012781072311505352\tValidation Loss: 0.00013069365393356868\n",
      "Epoch 7727  \tTraining Loss: 0.00012780860179264955\tValidation Loss: 0.00013069195443531932\n",
      "Epoch 7728  \tTraining Loss: 0.00012780648062349364\tValidation Loss: 0.0001306901429917539\n",
      "Epoch 7729  \tTraining Loss: 0.00012780436014499377\tValidation Loss: 0.00013068835602397664\n",
      "Epoch 7730  \tTraining Loss: 0.00012780223939716043\tValidation Loss: 0.00013068653997058344\n",
      "Epoch 7731  \tTraining Loss: 0.0001278001207258297\tValidation Loss: 0.00013068475331661728\n",
      "Epoch 7732  \tTraining Loss: 0.0001277980007813654\tValidation Loss: 0.00013068296758771033\n",
      "Epoch 7733  \tTraining Loss: 0.00012779588235091045\tValidation Loss: 0.000130681148558711\n",
      "Epoch 7734  \tTraining Loss: 0.00012779376439069586\tValidation Loss: 0.00013067936365327286\n",
      "Epoch 7735  \tTraining Loss: 0.00012779164586757935\tValidation Loss: 0.000130677580074346\n",
      "Epoch 7736  \tTraining Loss: 0.00012778952953745653\tValidation Loss: 0.00013067568490480165\n",
      "Epoch 7737  \tTraining Loss: 0.00012778741364006258\tValidation Loss: 0.00013067398925122362\n",
      "Epoch 7738  \tTraining Loss: 0.00012778529643432768\tValidation Loss: 0.0001306721879901606\n",
      "Epoch 7739  \tTraining Loss: 0.00012778318095101908\tValidation Loss: 0.00013067040717483337\n",
      "Epoch 7740  \tTraining Loss: 0.00012778106472503066\tValidation Loss: 0.000130668625240883\n",
      "Epoch 7741  \tTraining Loss: 0.00012777895050814343\tValidation Loss: 0.00013066680944439938\n",
      "Epoch 7742  \tTraining Loss: 0.00012777683576609423\tValidation Loss: 0.00013066502796536372\n",
      "Epoch 7743  \tTraining Loss: 0.000127774721392992\tValidation Loss: 0.00013066321926116176\n",
      "Epoch 7744  \tTraining Loss: 0.0001277726084345204\tValidation Loss: 0.00013066144058417075\n",
      "Epoch 7745  \tTraining Loss: 0.0001277704945284772\tValidation Loss: 0.00013065966265668233\n",
      "Epoch 7746  \tTraining Loss: 0.000127768382465009\tValidation Loss: 0.00013065785104519407\n",
      "Epoch 7747  \tTraining Loss: 0.00012776627082281594\tValidation Loss: 0.000130655995466707\n",
      "Epoch 7748  \tTraining Loss: 0.0001277641592402913\tValidation Loss: 0.00013065427756634503\n",
      "Epoch 7749  \tTraining Loss: 0.00012776204818658245\tValidation Loss: 0.0001306525112859415\n",
      "Epoch 7750  \tTraining Loss: 0.00012775993656932968\tValidation Loss: 0.00013065073626812948\n",
      "Epoch 7751  \tTraining Loss: 0.00012775782676244325\tValidation Loss: 0.00013064892565107562\n",
      "Epoch 7752  \tTraining Loss: 0.00012775571687105325\tValidation Loss: 0.0001306471490805099\n",
      "Epoch 7753  \tTraining Loss: 0.00012775360690258882\tValidation Loss: 0.00013064534508188658\n",
      "Epoch 7754  \tTraining Loss: 0.00012775149879619836\tValidation Loss: 0.00013064357132542472\n",
      "Epoch 7755  \tTraining Loss: 0.00012774938951534345\tValidation Loss: 0.0001306417983427597\n",
      "Epoch 7756  \tTraining Loss: 0.00012774728185096582\tValidation Loss: 0.00013063999161961503\n",
      "Epoch 7757  \tTraining Loss: 0.0001277451748545874\tValidation Loss: 0.00013063814134750315\n",
      "Epoch 7758  \tTraining Loss: 0.00012774306786879783\tValidation Loss: 0.00013063642831936002\n",
      "Epoch 7759  \tTraining Loss: 0.0001277409616623794\tValidation Loss: 0.00013063466717469152\n",
      "Epoch 7760  \tTraining Loss: 0.00012773885466247904\tValidation Loss: 0.00013063289727997698\n",
      "Epoch 7761  \tTraining Loss: 0.0001277367492443196\tValidation Loss: 0.00013063109168160684\n",
      "Epoch 7762  \tTraining Loss: 0.00012773464419670583\tValidation Loss: 0.00013062932031037292\n",
      "Epoch 7763  \tTraining Loss: 0.00012773253862979994\tValidation Loss: 0.00013062755022787164\n",
      "Epoch 7764  \tTraining Loss: 0.00012773043514908807\tValidation Loss: 0.00013062574665668962\n",
      "Epoch 7765  \tTraining Loss: 0.0001277283309381329\tValidation Loss: 0.0001306239778729099\n",
      "Epoch 7766  \tTraining Loss: 0.00012772622727976195\tValidation Loss: 0.0001306221813979194\n",
      "Epoch 7767  \tTraining Loss: 0.00012772412506123734\tValidation Loss: 0.00013062033747914762\n",
      "Epoch 7768  \tTraining Loss: 0.0001277220228159697\tValidation Loss: 0.0001306186587437148\n",
      "Epoch 7769  \tTraining Loss: 0.00012771992098819433\tValidation Loss: 0.00013061686788805245\n",
      "Epoch 7770  \tTraining Loss: 0.0001277178191200839\tValidation Loss: 0.00013061510234884703\n",
      "Epoch 7771  \tTraining Loss: 0.00012771571768551638\tValidation Loss: 0.0001306133070924735\n",
      "Epoch 7772  \tTraining Loss: 0.00012771361762767036\tValidation Loss: 0.00013061154193149855\n",
      "Epoch 7773  \tTraining Loss: 0.00012771151663492402\tValidation Loss: 0.0001306097774731204\n",
      "Epoch 7774  \tTraining Loss: 0.0001277094174873742\tValidation Loss: 0.00013060797918713391\n",
      "Epoch 7775  \tTraining Loss: 0.0001277073181343434\tValidation Loss: 0.00013060621576220093\n",
      "Epoch 7776  \tTraining Loss: 0.00012770521881243221\tValidation Loss: 0.00013060442443561798\n",
      "Epoch 7777  \tTraining Loss: 0.00012770312123906404\tValidation Loss: 0.00013060266351740528\n",
      "Epoch 7778  \tTraining Loss: 0.00012770102327272945\tValidation Loss: 0.00013060082553308607\n",
      "Epoch 7779  \tTraining Loss: 0.0001276989267514324\tValidation Loss: 0.00013059911764737062\n",
      "Epoch 7780  \tTraining Loss: 0.00012769682929839605\tValidation Loss: 0.0001305973664765625\n",
      "Epoch 7781  \tTraining Loss: 0.00012769473217781274\tValidation Loss: 0.00013059557793421\n",
      "Epoch 7782  \tTraining Loss: 0.00012769263695168242\tValidation Loss: 0.0001305938180600997\n",
      "Epoch 7783  \tTraining Loss: 0.0001276905406722664\tValidation Loss: 0.00013059207755244544\n",
      "Epoch 7784  \tTraining Loss: 0.00012768844607395004\tValidation Loss: 0.00013059029300788798\n",
      "Epoch 7785  \tTraining Loss: 0.00012768635184811334\tValidation Loss: 0.00013058851975459763\n",
      "Epoch 7786  \tTraining Loss: 0.0001276842571351451\tValidation Loss: 0.00013058677559094043\n",
      "Epoch 7787  \tTraining Loss: 0.00012768216445368605\tValidation Loss: 0.00013058499154745772\n",
      "Epoch 7788  \tTraining Loss: 0.00012768007141191045\tValidation Loss: 0.00013058316156156936\n",
      "Epoch 7789  \tTraining Loss: 0.00012767797943442671\tValidation Loss: 0.0001305814664875962\n",
      "Epoch 7790  \tTraining Loss: 0.0001276758872546212\tValidation Loss: 0.0001305797237449142\n",
      "Epoch 7791  \tTraining Loss: 0.0001276737947560057\tValidation Loss: 0.00013057797197578726\n",
      "Epoch 7792  \tTraining Loss: 0.0001276717042681344\tValidation Loss: 0.00013057616484369005\n",
      "Epoch 7793  \tTraining Loss: 0.00012766961335280676\tValidation Loss: 0.00013057442167063344\n",
      "Epoch 7794  \tTraining Loss: 0.00012766752260038547\tValidation Loss: 0.0001305726449489477\n",
      "Epoch 7795  \tTraining Loss: 0.0001276654334392196\tValidation Loss: 0.00013057089571903388\n",
      "Epoch 7796  \tTraining Loss: 0.00012766334325660774\tValidation Loss: 0.00013056914629861927\n",
      "Epoch 7797  \tTraining Loss: 0.00012766125478333058\tValidation Loss: 0.00013056736229489666\n",
      "Epoch 7798  \tTraining Loss: 0.00012765916655592678\tValidation Loss: 0.0001305655361667745\n",
      "Epoch 7799  \tTraining Loss: 0.00012765707893357066\tValidation Loss: 0.00013056384520253406\n",
      "Epoch 7800  \tTraining Loss: 0.00012765499174990162\tValidation Loss: 0.00013056210708620048\n",
      "Epoch 7801  \tTraining Loss: 0.00012765290371253784\tValidation Loss: 0.00013056037099911088\n",
      "Epoch 7802  \tTraining Loss: 0.00012765081700553583\tValidation Loss: 0.00013055857427457526\n",
      "Epoch 7803  \tTraining Loss: 0.0001276487306853198\tValidation Loss: 0.0001305568406480232\n",
      "Epoch 7804  \tTraining Loss: 0.00012764664376767237\tValidation Loss: 0.00013055510207002976\n",
      "Epoch 7805  \tTraining Loss: 0.00012764455892317394\tValidation Loss: 0.0001305533256824927\n",
      "Epoch 7806  \tTraining Loss: 0.00012764247343258165\tValidation Loss: 0.00013055158387707924\n",
      "Epoch 7807  \tTraining Loss: 0.00012764038837172995\tValidation Loss: 0.00013054981325053374\n",
      "Epoch 7808  \tTraining Loss: 0.00012763830469638223\tValidation Loss: 0.00013054807336007357\n",
      "Epoch 7809  \tTraining Loss: 0.00012763622077333062\tValidation Loss: 0.00013054625669794656\n",
      "Epoch 7810  \tTraining Loss: 0.00012763413850411568\tValidation Loss: 0.00013054456865950757\n",
      "Epoch 7811  \tTraining Loss: 0.00012763205498757144\tValidation Loss: 0.00013054283805608697\n",
      "Epoch 7812  \tTraining Loss: 0.0001276299720444772\tValidation Loss: 0.00013054106956093893\n",
      "Epoch 7813  \tTraining Loss: 0.00012762789078535176\tValidation Loss: 0.0001305393302279711\n",
      "Epoch 7814  \tTraining Loss: 0.00012762580844983827\tValidation Loss: 0.00013053759087916134\n",
      "Epoch 7815  \tTraining Loss: 0.00012762372776466724\tValidation Loss: 0.0001305358170760733\n",
      "Epoch 7816  \tTraining Loss: 0.00012762164724910494\tValidation Loss: 0.00013053407888943247\n",
      "Epoch 7817  \tTraining Loss: 0.00012761956634748682\tValidation Loss: 0.00013053231215107296\n",
      "Epoch 7818  \tTraining Loss: 0.0001276174876401019\tValidation Loss: 0.0001305305765286844\n",
      "Epoch 7819  \tTraining Loss: 0.0001276154080050418\tValidation Loss: 0.0001305287644152244\n",
      "Epoch 7820  \tTraining Loss: 0.0001276133301661495\tValidation Loss: 0.00013052708057695244\n",
      "Epoch 7821  \tTraining Loss: 0.0001276112516090614\tValidation Loss: 0.00013052535447953667\n",
      "Epoch 7822  \tTraining Loss: 0.00012760917291481413\tValidation Loss: 0.00013052360071326492\n",
      "Epoch 7823  \tTraining Loss: 0.00012760709626848864\tValidation Loss: 0.0001305218399281981\n",
      "Epoch 7824  \tTraining Loss: 0.00012760501899510354\tValidation Loss: 0.0001305201091531561\n",
      "Epoch 7825  \tTraining Loss: 0.0001276029420897557\tValidation Loss: 0.00013051834638209132\n",
      "Epoch 7826  \tTraining Loss: 0.00012760086665777624\tValidation Loss: 0.00013051661437373297\n",
      "Epoch 7827  \tTraining Loss: 0.0001275987902346019\tValidation Loss: 0.0001305148824368955\n",
      "Epoch 7828  \tTraining Loss: 0.0001275967155616443\tValidation Loss: 0.00013051311567971168\n",
      "Epoch 7829  \tTraining Loss: 0.0001275946408840212\tValidation Loss: 0.00013051130801710158\n",
      "Epoch 7830  \tTraining Loss: 0.0001275925672068775\tValidation Loss: 0.00013050963406852843\n",
      "Epoch 7831  \tTraining Loss: 0.00012759049372615983\tValidation Loss: 0.0001305079137181186\n",
      "Epoch 7832  \tTraining Loss: 0.00012758841954355234\tValidation Loss: 0.00013050618431631583\n",
      "Epoch 7833  \tTraining Loss: 0.0001275863469848266\tValidation Loss: 0.00013050441827623348\n",
      "Epoch 7834  \tTraining Loss: 0.00012758427468908705\tValidation Loss: 0.0001305026877452423\n",
      "Epoch 7835  \tTraining Loss: 0.0001275822019154327\tValidation Loss: 0.00013050095807286955\n",
      "Epoch 7836  \tTraining Loss: 0.00012758013122048112\tValidation Loss: 0.00013049919394387993\n",
      "Epoch 7837  \tTraining Loss: 0.00012757805978649142\tValidation Loss: 0.00013049746587362134\n",
      "Epoch 7838  \tTraining Loss: 0.00012757598885207954\tValidation Loss: 0.00013049570888254352\n",
      "Epoch 7839  \tTraining Loss: 0.00012757391923337892\tValidation Loss: 0.0001304939833558071\n",
      "Epoch 7840  \tTraining Loss: 0.00012757184919910902\tValidation Loss: 0.00013049218161270028\n",
      "Epoch 7841  \tTraining Loss: 0.00012756978122506764\tValidation Loss: 0.0001304905071826555\n",
      "Epoch 7842  \tTraining Loss: 0.0001275677117543892\tValidation Loss: 0.0001304887910536224\n",
      "Epoch 7843  \tTraining Loss: 0.00012756564289514261\tValidation Loss: 0.00013048703663053559\n",
      "Epoch 7844  \tTraining Loss: 0.00012756357568118084\tValidation Loss: 0.00013048531194193575\n",
      "Epoch 7845  \tTraining Loss: 0.00012756150740022017\tValidation Loss: 0.00013048358709141366\n",
      "Epoch 7846  \tTraining Loss: 0.0001275594407788608\tValidation Loss: 0.00013048182740751045\n",
      "Epoch 7847  \tTraining Loss: 0.00012755737429862273\tValidation Loss: 0.00013048010392337269\n",
      "Epoch 7848  \tTraining Loss: 0.00012755530744128853\tValidation Loss: 0.00013047835139389443\n",
      "Epoch 7849  \tTraining Loss: 0.00012755324276742834\tValidation Loss: 0.00013047663051770549\n",
      "Epoch 7850  \tTraining Loss: 0.00012755117692572998\tValidation Loss: 0.00013047483359632242\n",
      "Epoch 7851  \tTraining Loss: 0.00012754911334449302\tValidation Loss: 0.00013047316358075778\n",
      "Epoch 7852  \tTraining Loss: 0.00012754704880968556\tValidation Loss: 0.00013047145213403646\n",
      "Epoch 7853  \tTraining Loss: 0.00012754498412900257\tValidation Loss: 0.00013046973213794822\n",
      "Epoch 7854  \tTraining Loss: 0.00012754292145565396\tValidation Loss: 0.00013046797547545721\n",
      "Epoch 7855  \tTraining Loss: 0.00012754085822701012\tValidation Loss: 0.00013046625483473256\n",
      "Epoch 7856  \tTraining Loss: 0.00012753879530146415\tValidation Loss: 0.00013046450510353824\n",
      "Epoch 7857  \tTraining Loss: 0.0001275367338931842\tValidation Loss: 0.00013046278736903737\n",
      "Epoch 7858  \tTraining Loss: 0.00012753467146216902\tValidation Loss: 0.00013046106981069424\n",
      "Epoch 7859  \tTraining Loss: 0.00012753261074569458\tValidation Loss: 0.0001304593171543883\n",
      "Epoch 7860  \tTraining Loss: 0.00012753055005822693\tValidation Loss: 0.00013045760073138911\n",
      "Epoch 7861  \tTraining Loss: 0.00012752848958961564\tValidation Loss: 0.00013045577909692017\n",
      "Epoch 7862  \tTraining Loss: 0.0001275264315703531\tValidation Loss: 0.00013045415063552473\n",
      "Epoch 7863  \tTraining Loss: 0.00012752437068413522\tValidation Loss: 0.00013045244483342387\n",
      "Epoch 7864  \tTraining Loss: 0.00012752231203094883\tValidation Loss: 0.00013045069465413662\n",
      "Epoch 7865  \tTraining Loss: 0.00012752025373393585\tValidation Loss: 0.00013044897902740897\n",
      "Epoch 7866  \tTraining Loss: 0.00012751819491365785\tValidation Loss: 0.00013044726368755728\n",
      "Epoch 7867  \tTraining Loss: 0.00012751613811750982\tValidation Loss: 0.00013044551346543686\n",
      "Epoch 7868  \tTraining Loss: 0.00012751408068130347\tValidation Loss: 0.00013044379994638588\n",
      "Epoch 7869  \tTraining Loss: 0.00012751202362638308\tValidation Loss: 0.0001304420570842521\n",
      "Epoch 7870  \tTraining Loss: 0.0001275099680053465\tValidation Loss: 0.00013044034628924476\n",
      "Epoch 7871  \tTraining Loss: 0.00012750791153082482\tValidation Loss: 0.0001304385597638923\n",
      "Epoch 7872  \tTraining Loss: 0.0001275058577838808\tValidation Loss: 0.0001304368992195262\n",
      "Epoch 7873  \tTraining Loss: 0.00012750380230095977\tValidation Loss: 0.0001304351978257138\n",
      "Epoch 7874  \tTraining Loss: 0.00012750174795687577\tValidation Loss: 0.00013043345374044762\n",
      "Epoch 7875  \tTraining Loss: 0.00012749969674155583\tValidation Loss: 0.0001304317697286051\n",
      "Epoch 7876  \tTraining Loss: 0.000127497644588233\tValidation Loss: 0.00013043007374470157\n",
      "Epoch 7877  \tTraining Loss: 0.00012749559392173285\tValidation Loss: 0.00013042833867319678\n",
      "Epoch 7878  \tTraining Loss: 0.00012749354369220883\tValidation Loss: 0.0001304266386806646\n",
      "Epoch 7879  \tTraining Loss: 0.00012749149290376192\tValidation Loss: 0.00013042493852344513\n",
      "Epoch 7880  \tTraining Loss: 0.00012748944395041901\tValidation Loss: 0.0001304232029054333\n",
      "Epoch 7881  \tTraining Loss: 0.00012748739469130386\tValidation Loss: 0.00013042150404569166\n",
      "Epoch 7882  \tTraining Loss: 0.00012748534559091864\tValidation Loss: 0.00013041970000576337\n",
      "Epoch 7883  \tTraining Loss: 0.0001274832993546598\tValidation Loss: 0.00013041808908374663\n",
      "Epoch 7884  \tTraining Loss: 0.00012748125006804427\tValidation Loss: 0.0001304164009043428\n",
      "Epoch 7885  \tTraining Loss: 0.00012747920293359975\tValidation Loss: 0.00013041466818598788\n",
      "Epoch 7886  \tTraining Loss: 0.0001274771562683156\tValidation Loss: 0.00013041297062176596\n",
      "Epoch 7887  \tTraining Loss: 0.0001274751090354211\tValidation Loss: 0.00013041127327922104\n",
      "Epoch 7888  \tTraining Loss: 0.00012747306361934687\tValidation Loss: 0.00013040954083467281\n",
      "Epoch 7889  \tTraining Loss: 0.0001274710179275202\tValidation Loss: 0.00013040784566010366\n",
      "Epoch 7890  \tTraining Loss: 0.00012746897215728452\tValidation Loss: 0.00013040612078168764\n",
      "Epoch 7891  \tTraining Loss: 0.00012746692835752268\tValidation Loss: 0.00013040442849711936\n",
      "Epoch 7892  \tTraining Loss: 0.00012746488331594603\tValidation Loss: 0.00013040273605905843\n",
      "Epoch 7893  \tTraining Loss: 0.00012746283978678404\tValidation Loss: 0.00013040093284835108\n",
      "Epoch 7894  \tTraining Loss: 0.00012746079796610858\tValidation Loss: 0.00013039932702940788\n",
      "Epoch 7895  \tTraining Loss: 0.0001274587535747955\tValidation Loss: 0.00013039764450755028\n",
      "Epoch 7896  \tTraining Loss: 0.0001274567116674204\tValidation Loss: 0.0001303959175040096\n",
      "Epoch 7897  \tTraining Loss: 0.0001274546695175972\tValidation Loss: 0.0001303942259361313\n",
      "Epoch 7898  \tTraining Loss: 0.00012745262726435985\tValidation Loss: 0.00013039250413498374\n",
      "Epoch 7899  \tTraining Loss: 0.0001274505870168803\tValidation Loss: 0.00013039081516718986\n",
      "Epoch 7900  \tTraining Loss: 0.0001274485455074816\tValidation Loss: 0.00013038912620105198\n",
      "Epoch 7901  \tTraining Loss: 0.00012744650531212374\tValidation Loss: 0.00013038740168287065\n",
      "Epoch 7902  \tTraining Loss: 0.00012744446588941184\tValidation Loss: 0.0001303857143798827\n",
      "Epoch 7903  \tTraining Loss: 0.00012744242572740186\tValidation Loss: 0.00013038402738324014\n",
      "Epoch 7904  \tTraining Loss: 0.00012744038732826236\tValidation Loss: 0.00013038222987003976\n",
      "Epoch 7905  \tTraining Loss: 0.0001274383500834061\tValidation Loss: 0.0001303806296788166\n",
      "Epoch 7906  \tTraining Loss: 0.0001274363106399276\tValidation Loss: 0.00013037892233593986\n",
      "Epoch 7907  \tTraining Loss: 0.00012743427390591032\tValidation Loss: 0.00013037723873480318\n",
      "Epoch 7908  \tTraining Loss: 0.0001274322359166617\tValidation Loss: 0.00013037555311233554\n",
      "Epoch 7909  \tTraining Loss: 0.00012743019921909876\tValidation Loss: 0.00013037383144968596\n",
      "Epoch 7910  \tTraining Loss: 0.00012742816333566852\tValidation Loss: 0.00013037214726080218\n",
      "Epoch 7911  \tTraining Loss: 0.00012742612669042113\tValidation Loss: 0.0001303704635428567\n",
      "Epoch 7912  \tTraining Loss: 0.00012742409168399425\tValidation Loss: 0.00013036874431872472\n",
      "Epoch 7913  \tTraining Loss: 0.00012742205675148394\tValidation Loss: 0.00013036706259886546\n",
      "Epoch 7914  \tTraining Loss: 0.00012742002142629682\tValidation Loss: 0.00013036538115360482\n",
      "Epoch 7915  \tTraining Loss: 0.00012741798809912878\tValidation Loss: 0.00013036366397768396\n",
      "Epoch 7916  \tTraining Loss: 0.00012741595467751247\tValidation Loss: 0.0001303619094364237\n",
      "Epoch 7917  \tTraining Loss: 0.00012741392162623198\tValidation Loss: 0.000130360284360607\n",
      "Epoch 7918  \tTraining Loss: 0.00012741188895458\tValidation Loss: 0.0001303586152302725\n",
      "Epoch 7919  \tTraining Loss: 0.0001274098557682621\tValidation Loss: 0.0001303569367228568\n",
      "Epoch 7920  \tTraining Loss: 0.00012740782422387598\tValidation Loss: 0.00013035522048654845\n",
      "Epoch 7921  \tTraining Loss: 0.0001274057928336873\tValidation Loss: 0.0001303535415181978\n",
      "Epoch 7922  \tTraining Loss: 0.00012740376101017095\tValidation Loss: 0.0001303518629055505\n",
      "Epoch 7923  \tTraining Loss: 0.0001274017311464699\tValidation Loss: 0.00013035014868796949\n",
      "Epoch 7924  \tTraining Loss: 0.0001273997007099732\tValidation Loss: 0.00013034847225773557\n",
      "Epoch 7925  \tTraining Loss: 0.00012739767047139505\tValidation Loss: 0.00013034676537371424\n",
      "Epoch 7926  \tTraining Loss: 0.00012739564192266377\tValidation Loss: 0.0001303450915566896\n",
      "Epoch 7927  \tTraining Loss: 0.00012739361272776073\tValidation Loss: 0.00013034334267848487\n",
      "Epoch 7928  \tTraining Loss: 0.00012739158526314952\tValidation Loss: 0.00013034171719865026\n",
      "Epoch 7929  \tTraining Loss: 0.00012738955694655249\tValidation Loss: 0.00013034005243367657\n",
      "Epoch 7930  \tTraining Loss: 0.00012738752859111065\tValidation Loss: 0.0001303383788060163\n",
      "Epoch 7931  \tTraining Loss: 0.0001273855021862968\tValidation Loss: 0.0001303366675176646\n",
      "Epoch 7932  \tTraining Loss: 0.00012738347527172048\tValidation Loss: 0.00013033499380132662\n",
      "Epoch 7933  \tTraining Loss: 0.00012738144848604749\tValidation Loss: 0.00013033328961433544\n",
      "Epoch 7934  \tTraining Loss: 0.0001273794234644955\tValidation Loss: 0.00013033161885359738\n",
      "Epoch 7935  \tTraining Loss: 0.00012737739729081943\tValidation Loss: 0.0001303299477645566\n",
      "Epoch 7936  \tTraining Loss: 0.00012737537254564515\tValidation Loss: 0.00013032824052664767\n",
      "Epoch 7937  \tTraining Loss: 0.00012737334833514698\tValidation Loss: 0.00013032657104280833\n",
      "Epoch 7938  \tTraining Loss: 0.00012737132371087146\tValidation Loss: 0.0001303248272008864\n",
      "Epoch 7939  \tTraining Loss: 0.00012736930161905838\tValidation Loss: 0.00013032320646828835\n",
      "Epoch 7940  \tTraining Loss: 0.00012736727776672277\tValidation Loss: 0.00013032154684521034\n",
      "Epoch 7941  \tTraining Loss: 0.00012736525440623392\tValidation Loss: 0.00013031984746379204\n",
      "Epoch 7942  \tTraining Loss: 0.00012736323289194932\tValidation Loss: 0.0001303181797108716\n",
      "Epoch 7943  \tTraining Loss: 0.000127361210198227\tValidation Loss: 0.00013031651121658204\n",
      "Epoch 7944  \tTraining Loss: 0.0001273591888915359\tValidation Loss: 0.00013031480658945321\n",
      "Epoch 7945  \tTraining Loss: 0.00012735716819807613\tValidation Loss: 0.0001303131400888457\n",
      "Epoch 7946  \tTraining Loss: 0.0001273551468320462\tValidation Loss: 0.0001303114737529777\n",
      "Epoch 7947  \tTraining Loss: 0.00012735312719813085\tValidation Loss: 0.0001303097713306655\n",
      "Epoch 7948  \tTraining Loss: 0.00012735110744319393\tValidation Loss: 0.00013030810696638455\n",
      "Epoch 7949  \tTraining Loss: 0.0001273490874237949\tValidation Loss: 0.00013030641160487582\n",
      "Epoch 7950  \tTraining Loss: 0.00012734707005925453\tValidation Loss: 0.00013030467549155904\n",
      "Epoch 7951  \tTraining Loss: 0.0001273450515206506\tValidation Loss: 0.0001303030975303192\n",
      "Epoch 7952  \tTraining Loss: 0.00012734303307380092\tValidation Loss: 0.00013030140640472163\n",
      "Epoch 7953  \tTraining Loss: 0.00012734101585560462\tValidation Loss: 0.00013029974439716073\n",
      "Epoch 7954  \tTraining Loss: 0.00012733899795630563\tValidation Loss: 0.0001302980806375499\n",
      "Epoch 7955  \tTraining Loss: 0.00012733698176070184\tValidation Loss: 0.00013029637943490286\n",
      "Epoch 7956  \tTraining Loss: 0.00012733496594499047\tValidation Loss: 0.00013029471114161316\n",
      "Epoch 7957  \tTraining Loss: 0.00012733294979225797\tValidation Loss: 0.00013029301551656982\n",
      "Epoch 7958  \tTraining Loss: 0.00012733093587298354\tValidation Loss: 0.0001302913548573002\n",
      "Epoch 7959  \tTraining Loss: 0.0001273289205616407\tValidation Loss: 0.000130289694163222\n",
      "Epoch 7960  \tTraining Loss: 0.00012732690645209308\tValidation Loss: 0.00013028799715972733\n",
      "Epoch 7961  \tTraining Loss: 0.00012732489344998097\tValidation Loss: 0.00013028626449116315\n",
      "Epoch 7962  \tTraining Loss: 0.0001273228805238957\tValidation Loss: 0.00013028469024558317\n",
      "Epoch 7963  \tTraining Loss: 0.00012732086762050227\tValidation Loss: 0.00013028300306061142\n",
      "Epoch 7964  \tTraining Loss: 0.00012731885527809606\tValidation Loss: 0.0001302813453533511\n",
      "Epoch 7965  \tTraining Loss: 0.0001273168425873009\tValidation Loss: 0.00013027968591922034\n",
      "Epoch 7966  \tTraining Loss: 0.00012731483190800376\tValidation Loss: 0.00013027798990871884\n",
      "Epoch 7967  \tTraining Loss: 0.00012731282054140316\tValidation Loss: 0.00013027633253612962\n",
      "Epoch 7968  \tTraining Loss: 0.00012731080946291984\tValidation Loss: 0.0001302746441360556\n",
      "Epoch 7969  \tTraining Loss: 0.00012730879998590648\tValidation Loss: 0.00013027298968238437\n",
      "Epoch 7970  \tTraining Loss: 0.00012730678942845288\tValidation Loss: 0.0001302713345999269\n",
      "Epoch 7971  \tTraining Loss: 0.0001273047803748038\tValidation Loss: 0.00013026964281422174\n",
      "Epoch 7972  \tTraining Loss: 0.00012730277169814127\tValidation Loss: 0.0001302679893582848\n",
      "Epoch 7973  \tTraining Loss: 0.00012730076279445578\tValidation Loss: 0.00013026626198032738\n",
      "Epoch 7974  \tTraining Loss: 0.00012729875624012868\tValidation Loss: 0.0001302646560823071\n",
      "Epoch 7975  \tTraining Loss: 0.00012729674791031728\tValidation Loss: 0.00013026301221125545\n",
      "Epoch 7976  \tTraining Loss: 0.00012729474020819987\tValidation Loss: 0.00013026132804420246\n",
      "Epoch 7977  \tTraining Loss: 0.0001272927342147167\tValidation Loss: 0.00013025967610276484\n",
      "Epoch 7978  \tTraining Loss: 0.00012729072710070688\tValidation Loss: 0.00013025802320539305\n",
      "Epoch 7979  \tTraining Loss: 0.0001272887214375031\tValidation Loss: 0.00013025633369551182\n",
      "Epoch 7980  \tTraining Loss: 0.00012728671625303323\tValidation Loss: 0.00013025468294395874\n",
      "Epoch 7981  \tTraining Loss: 0.00012728471045284485\tValidation Loss: 0.00013025303215089007\n",
      "Epoch 7982  \tTraining Loss: 0.0001272827064465422\tValidation Loss: 0.00013025134479631987\n",
      "Epoch 7983  \tTraining Loss: 0.00012728070218871867\tValidation Loss: 0.0001302496961338818\n",
      "Epoch 7984  \tTraining Loss: 0.00012727869778090688\tValidation Loss: 0.00013024801587782975\n",
      "Epoch 7985  \tTraining Loss: 0.00012727669578375736\tValidation Loss: 0.0001302462961359952\n",
      "Epoch 7986  \tTraining Loss: 0.00012727469289693384\tValidation Loss: 0.0001302447332188858\n",
      "Epoch 7987  \tTraining Loss: 0.00012727269004759082\tValidation Loss: 0.00013024305696784545\n",
      "Epoch 7988  \tTraining Loss: 0.00012727068830949815\tValidation Loss: 0.00013024141054718692\n",
      "Epoch 7989  \tTraining Loss: 0.00012726868593907005\tValidation Loss: 0.00013023976219840485\n",
      "Epoch 7990  \tTraining Loss: 0.000127266685315264\tValidation Loss: 0.00013023807691685312\n",
      "Epoch 7991  \tTraining Loss: 0.00012726468453240586\tValidation Loss: 0.00013023643068095354\n",
      "Epoch 7992  \tTraining Loss: 0.00012726268350155516\tValidation Loss: 0.0001302347529997226\n",
      "Epoch 7993  \tTraining Loss: 0.0001272606846018408\tValidation Loss: 0.00013023310966201376\n",
      "Epoch 7994  \tTraining Loss: 0.000127258684349092\tValidation Loss: 0.00013023146554828977\n",
      "Epoch 7995  \tTraining Loss: 0.00012725668533730385\tValidation Loss: 0.00013022978441744389\n",
      "Epoch 7996  \tTraining Loss: 0.00012725468722468987\tValidation Loss: 0.00013022814205662514\n",
      "Epoch 7997  \tTraining Loss: 0.00012725268876824785\tValidation Loss: 0.00013022642611461765\n",
      "Epoch 7998  \tTraining Loss: 0.0001272506920893565\tValidation Loss: 0.00013022483056978408\n",
      "Epoch 7999  \tTraining Loss: 0.0001272486943107449\tValidation Loss: 0.0001302231977058891\n",
      "Epoch 8000  \tTraining Loss: 0.00012724669632670031\tValidation Loss: 0.00013022160790581293\n",
      "Epoch 8001  \tTraining Loss: 0.0001272446987281638\tValidation Loss: 0.0001302199873441668\n",
      "Epoch 8002  \tTraining Loss: 0.00012724269980004063\tValidation Loss: 0.0001302183590274545\n",
      "Epoch 8003  \tTraining Loss: 0.00012724070211380506\tValidation Loss: 0.0001302166911029053\n",
      "Epoch 8004  \tTraining Loss: 0.0001272387053739032\tValidation Loss: 0.00013021506096312548\n",
      "Epoch 8005  \tTraining Loss: 0.0001272367077822814\tValidation Loss: 0.00013021342981333974\n",
      "Epoch 8006  \tTraining Loss: 0.00012723471179608447\tValidation Loss: 0.0001302117612455573\n",
      "Epoch 8007  \tTraining Loss: 0.00012723271595948985\tValidation Loss: 0.00013021013139725063\n",
      "Epoch 8008  \tTraining Loss: 0.00012723071966862413\tValidation Loss: 0.00013020850092924417\n",
      "Epoch 8009  \tTraining Loss: 0.0001272287253545558\tValidation Loss: 0.0001302068332014126\n",
      "Epoch 8010  \tTraining Loss: 0.00012722673089962314\tValidation Loss: 0.0001302051313720627\n",
      "Epoch 8011  \tTraining Loss: 0.00012722473683928436\tValidation Loss: 0.00013020355432226248\n",
      "Epoch 8012  \tTraining Loss: 0.00012722274321257842\tValidation Loss: 0.0001302019353849926\n",
      "Epoch 8013  \tTraining Loss: 0.00012722074902230012\tValidation Loss: 0.00013020030655614597\n",
      "Epoch 8014  \tTraining Loss: 0.00012721875645777092\tValidation Loss: 0.00013019863866305697\n",
      "Epoch 8015  \tTraining Loss: 0.0001272167640782135\tValidation Loss: 0.0001301970096196737\n",
      "Epoch 8016  \tTraining Loss: 0.00012721477122512104\tValidation Loss: 0.00013019538028297075\n",
      "Epoch 8017  \tTraining Loss: 0.000127212780323041\tValidation Loss: 0.00013019371397709212\n",
      "Epoch 8018  \tTraining Loss: 0.0001272107888620894\tValidation Loss: 0.0001301920870371856\n",
      "Epoch 8019  \tTraining Loss: 0.00012720879755659845\tValidation Loss: 0.00013019042795170654\n",
      "Epoch 8020  \tTraining Loss: 0.00012720680797252196\tValidation Loss: 0.00013018880338715508\n",
      "Epoch 8021  \tTraining Loss: 0.0001272048172353995\tValidation Loss: 0.00013018717766948452\n",
      "Epoch 8022  \tTraining Loss: 0.0001272028279601814\tValidation Loss: 0.00013018551438608115\n",
      "Epoch 8023  \tTraining Loss: 0.0001272008394922911\tValidation Loss: 0.0001301838174416332\n",
      "Epoch 8024  \tTraining Loss: 0.00012719885083241095\tValidation Loss: 0.00013018227645604087\n",
      "Epoch 8025  \tTraining Loss: 0.0001271968627534579\tValidation Loss: 0.0001301806220407213\n",
      "Epoch 8026  \tTraining Loss: 0.00012719487469802846\tValidation Loss: 0.0001301789982115976\n",
      "Epoch 8027  \tTraining Loss: 0.00012719288678617917\tValidation Loss: 0.00013017734037592106\n",
      "Epoch 8028  \tTraining Loss: 0.00012719090063552832\tValidation Loss: 0.00013017571702481035\n",
      "Epoch 8029  \tTraining Loss: 0.00012718891331077946\tValidation Loss: 0.00013017409274406285\n",
      "Epoch 8030  \tTraining Loss: 0.0001271869274182692\tValidation Loss: 0.00013017243112723785\n",
      "Epoch 8031  \tTraining Loss: 0.0001271849420452784\tValidation Loss: 0.00013017080902384023\n",
      "Epoch 8032  \tTraining Loss: 0.00012718295602212116\tValidation Loss: 0.00013016918643640336\n",
      "Epoch 8033  \tTraining Loss: 0.00012718097178527135\tValidation Loss: 0.0001301675264701248\n",
      "Epoch 8034  \tTraining Loss: 0.00012717898731396185\tValidation Loss: 0.00013016590597270643\n",
      "Epoch 8035  \tTraining Loss: 0.0001271770026655715\tValidation Loss: 0.0001301642529248937\n",
      "Epoch 8036  \tTraining Loss: 0.00012717502027719443\tValidation Loss: 0.000130162562090566\n",
      "Epoch 8037  \tTraining Loss: 0.00012717303725238283\tValidation Loss: 0.00013016102615257886\n",
      "Epoch 8038  \tTraining Loss: 0.0001271710541615592\tValidation Loss: 0.0001301593765290422\n",
      "Epoch 8039  \tTraining Loss: 0.00012716907217576965\tValidation Loss: 0.0001301577576657298\n",
      "Epoch 8040  \tTraining Loss: 0.0001271670895475535\tValidation Loss: 0.0001301561365551278\n",
      "Epoch 8041  \tTraining Loss: 0.00012716510868065494\tValidation Loss: 0.00013015447778523913\n",
      "Epoch 8042  \tTraining Loss: 0.00012716312762602538\tValidation Loss: 0.00013015285890676222\n",
      "Epoch 8043  \tTraining Loss: 0.00012716114634128884\tValidation Loss: 0.00013015120767726087\n",
      "Epoch 8044  \tTraining Loss: 0.00012715916716052761\tValidation Loss: 0.00013014959157084537\n",
      "Epoch 8045  \tTraining Loss: 0.00012715718662660385\tValidation Loss: 0.00013014797433196965\n",
      "Epoch 8046  \tTraining Loss: 0.000127155207359627\tValidation Loss: 0.0001301463193403197\n",
      "Epoch 8047  \tTraining Loss: 0.00012715322894064958\tValidation Loss: 0.0001301447039639182\n",
      "Epoch 8048  \tTraining Loss: 0.00012715124970122561\tValidation Loss: 0.00013014308787041772\n",
      "Epoch 8049  \tTraining Loss: 0.00012714927213722612\tValidation Loss: 0.00013014136198520948\n",
      "Epoch 8050  \tTraining Loss: 0.0001271472957869122\tValidation Loss: 0.0001301398313445185\n",
      "Epoch 8051  \tTraining Loss: 0.00012714531716344446\tValidation Loss: 0.00013013819220783208\n",
      "Epoch 8052  \tTraining Loss: 0.00012714334135424785\tValidation Loss: 0.00013013657742699184\n",
      "Epoch 8053  \tTraining Loss: 0.00012714136419971392\tValidation Loss: 0.00013013495898007939\n",
      "Epoch 8054  \tTraining Loss: 0.00012713938828780633\tValidation Loss: 0.00013013330251403357\n",
      "Epoch 8055  \tTraining Loss: 0.00012713741326919267\tValidation Loss: 0.00013013168615620365\n",
      "Epoch 8056  \tTraining Loss: 0.00012713543740597798\tValidation Loss: 0.00013013006929631305\n",
      "Epoch 8057  \tTraining Loss: 0.00012713346313787308\tValidation Loss: 0.00013012841494688244\n",
      "Epoch 8058  \tTraining Loss: 0.00012713148901178308\tValidation Loss: 0.00013012680073887445\n",
      "Epoch 8059  \tTraining Loss: 0.0001271295144158355\tValidation Loss: 0.00013012518583470248\n",
      "Epoch 8060  \tTraining Loss: 0.00012712754178017535\tValidation Loss: 0.00013012353321967555\n",
      "Epoch 8061  \tTraining Loss: 0.00012712556855246182\tValidation Loss: 0.00013012192067573722\n",
      "Epoch 8062  \tTraining Loss: 0.00012712359549083019\tValidation Loss: 0.00013012027505882867\n",
      "Epoch 8063  \tTraining Loss: 0.00012712162448270587\tValidation Loss: 0.00013011859277330812\n",
      "Epoch 8064  \tTraining Loss: 0.00012711965271946084\tValidation Loss: 0.00013011706410628055\n",
      "Epoch 8065  \tTraining Loss: 0.0001271176812024132\tValidation Loss: 0.00013011542184457428\n",
      "Epoch 8066  \tTraining Loss: 0.00012711571010881658\tValidation Loss: 0.0001301138058555692\n",
      "Epoch 8067  \tTraining Loss: 0.00012711373746385587\tValidation Loss: 0.0001301121993296986\n",
      "Epoch 8068  \tTraining Loss: 0.00012711176677536432\tValidation Loss: 0.00013011055024359536\n",
      "Epoch 8069  \tTraining Loss: 0.00012710979558274655\tValidation Loss: 0.00013010894024195513\n",
      "Epoch 8070  \tTraining Loss: 0.0001271078244383927\tValidation Loss: 0.00013010729667778617\n",
      "Epoch 8071  \tTraining Loss: 0.0001271058551580483\tValidation Loss: 0.00013010568862090454\n",
      "Epoch 8072  \tTraining Loss: 0.00012710388464972982\tValidation Loss: 0.00013010407912057262\n",
      "Epoch 8073  \tTraining Loss: 0.00012710191547518236\tValidation Loss: 0.00013010243152276806\n",
      "Epoch 8074  \tTraining Loss: 0.0001270999470043012\tValidation Loss: 0.00013010082420694541\n",
      "Epoch 8075  \tTraining Loss: 0.00012709797778991341\tValidation Loss: 0.00013009921597508973\n",
      "Epoch 8076  \tTraining Loss: 0.00012709601021835504\tValidation Loss: 0.00013009756975338444\n",
      "Epoch 8077  \tTraining Loss: 0.00012709404314113407\tValidation Loss: 0.00013009589224343205\n",
      "Epoch 8078  \tTraining Loss: 0.0001270920758337337\tValidation Loss: 0.00013009436848385167\n",
      "Epoch 8079  \tTraining Loss: 0.0001270901093839474\tValidation Loss: 0.00013009273134833885\n",
      "Epoch 8080  \tTraining Loss: 0.00012708814264733364\tValidation Loss: 0.00013009112616704725\n",
      "Epoch 8081  \tTraining Loss: 0.00012708617631496542\tValidation Loss: 0.00013008948606648907\n",
      "Epoch 8082  \tTraining Loss: 0.0001270842115150855\tValidation Loss: 0.0001300878816201423\n",
      "Epoch 8083  \tTraining Loss: 0.00012708224565156827\tValidation Loss: 0.00013008627590885863\n",
      "Epoch 8084  \tTraining Loss: 0.00012708028127842122\tValidation Loss: 0.00013008463222126764\n",
      "Epoch 8085  \tTraining Loss: 0.00012707831729063455\tValidation Loss: 0.00013008302925365096\n",
      "Epoch 8086  \tTraining Loss: 0.00012707635271531908\tValidation Loss: 0.00013008142546983218\n",
      "Epoch 8087  \tTraining Loss: 0.00012707438993699027\tValidation Loss: 0.00013007978366202355\n",
      "Epoch 8088  \tTraining Loss: 0.00012707242688538584\tValidation Loss: 0.00013007818252333978\n",
      "Epoch 8089  \tTraining Loss: 0.00012707046364683326\tValidation Loss: 0.00013007654786147593\n",
      "Epoch 8090  \tTraining Loss: 0.00012706850252439252\tValidation Loss: 0.00013007487762134334\n",
      "Epoch 8091  \tTraining Loss: 0.00012706654108977935\tValidation Loss: 0.0001300733599796058\n",
      "Epoch 8092  \tTraining Loss: 0.00012706457935072104\tValidation Loss: 0.00013007172865862576\n",
      "Epoch 8093  \tTraining Loss: 0.00012706261884677336\tValidation Loss: 0.00013007012942376544\n",
      "Epoch 8094  \tTraining Loss: 0.00012706065763204555\tValidation Loss: 0.00013006852763210202\n",
      "Epoch 8095  \tTraining Loss: 0.00012705869806020902\tValidation Loss: 0.00013006688753860158\n",
      "Epoch 8096  \tTraining Loss: 0.0001270567385211644\tValidation Loss: 0.0001300652885327899\n",
      "Epoch 8097  \tTraining Loss: 0.00012705477856897593\tValidation Loss: 0.0001300636888625434\n",
      "Epoch 8098  \tTraining Loss: 0.0001270528205739225\tValidation Loss: 0.0001300620512148534\n",
      "Epoch 8099  \tTraining Loss: 0.00012705086197885984\tValidation Loss: 0.00013006045460601728\n",
      "Epoch 8100  \tTraining Loss: 0.00012704890351635052\tValidation Loss: 0.00013005882438755955\n",
      "Epoch 8101  \tTraining Loss: 0.00012704694682378449\tValidation Loss: 0.00013005723027789052\n",
      "Epoch 8102  \tTraining Loss: 0.00012704498894077005\tValidation Loss: 0.00013005563448534885\n",
      "Epoch 8103  \tTraining Loss: 0.00012704303243803972\tValidation Loss: 0.00013005400011005422\n",
      "Epoch 8104  \tTraining Loss: 0.0001270410765773335\tValidation Loss: 0.00013005233538910431\n",
      "Epoch 8105  \tTraining Loss: 0.0001270391210262202\tValidation Loss: 0.00013005082331323529\n",
      "Epoch 8106  \tTraining Loss: 0.000127037165646175\tValidation Loss: 0.00013004919772209103\n",
      "Epoch 8107  \tTraining Loss: 0.00012703521052156395\tValidation Loss: 0.00013004760462487222\n",
      "Epoch 8108  \tTraining Loss: 0.0001270332552514747\tValidation Loss: 0.0001300459761477767\n",
      "Epoch 8109  \tTraining Loss: 0.00012703130205640102\tValidation Loss: 0.00013004438379475813\n",
      "Epoch 8110  \tTraining Loss: 0.0001270293475178213\tValidation Loss: 0.00013004279000958376\n",
      "Epoch 8111  \tTraining Loss: 0.00012702739420133232\tValidation Loss: 0.00013004115788713336\n",
      "Epoch 8112  \tTraining Loss: 0.00012702544180311982\tValidation Loss: 0.00013003956699324885\n",
      "Epoch 8113  \tTraining Loss: 0.0001270234885420874\tValidation Loss: 0.00013003797512636036\n",
      "Epoch 8114  \tTraining Loss: 0.0001270215368133544\tValidation Loss: 0.0001300363448781374\n",
      "Epoch 8115  \tTraining Loss: 0.00012701958533694121\tValidation Loss: 0.00013003475581031476\n",
      "Epoch 8116  \tTraining Loss: 0.0001270176333304383\tValidation Loss: 0.0001300331656377667\n",
      "Epoch 8117  \tTraining Loss: 0.000127015683176864\tValidation Loss: 0.00013003153695244544\n",
      "Epoch 8118  \tTraining Loss: 0.00012701373263023066\tValidation Loss: 0.00013002994945487451\n",
      "Epoch 8119  \tTraining Loss: 0.0001270117825745092\tValidation Loss: 0.00013002825716141697\n",
      "Epoch 8120  \tTraining Loss: 0.00012700983455561716\tValidation Loss: 0.00013002675425884022\n",
      "Epoch 8121  \tTraining Loss: 0.00012700788397234422\tValidation Loss: 0.00013002517878850668\n",
      "Epoch 8122  \tTraining Loss: 0.00012700593578187538\tValidation Loss: 0.00013002354437338662\n",
      "Epoch 8123  \tTraining Loss: 0.0001270039881893847\tValidation Loss: 0.00013002195332625962\n",
      "Epoch 8124  \tTraining Loss: 0.00012700203991717086\tValidation Loss: 0.0001300203622080719\n",
      "Epoch 8125  \tTraining Loss: 0.00012700009334094172\tValidation Loss: 0.0001300187333353333\n",
      "Epoch 8126  \tTraining Loss: 0.00012699814668982675\tValidation Loss: 0.00013001714645967564\n",
      "Epoch 8127  \tTraining Loss: 0.00012699619967202985\tValidation Loss: 0.00013001555884566154\n",
      "Epoch 8128  \tTraining Loss: 0.00012699425467135533\tValidation Loss: 0.00013001393287679475\n",
      "Epoch 8129  \tTraining Loss: 0.0001269923089521247\tValidation Loss: 0.00013001234853620835\n",
      "Epoch 8130  \tTraining Loss: 0.00012699036347845778\tValidation Loss: 0.00013001073005050977\n",
      "Epoch 8131  \tTraining Loss: 0.00012698841965737016\tValidation Loss: 0.0001300091482159801\n",
      "Epoch 8132  \tTraining Loss: 0.00012698647469729741\tValidation Loss: 0.00013000756450420425\n",
      "Epoch 8133  \tTraining Loss: 0.00012698453135462918\tValidation Loss: 0.00013000587135276675\n",
      "Epoch 8134  \tTraining Loss: 0.0001269825893162599\tValidation Loss: 0.00013000437219238018\n",
      "Epoch 8135  \tTraining Loss: 0.00012698064491679424\tValidation Loss: 0.0001300027970103303\n",
      "Epoch 8136  \tTraining Loss: 0.00012697870308667046\tValidation Loss: 0.00013000117460150034\n",
      "Epoch 8137  \tTraining Loss: 0.0001269767608281004\tValidation Loss: 0.00012999959228480275\n",
      "Epoch 8138  \tTraining Loss: 0.0001269748185364609\tValidation Loss: 0.00012999797550508735\n",
      "Epoch 8139  \tTraining Loss: 0.0001269728781850014\tValidation Loss: 0.0001299963957829654\n",
      "Epoch 8140  \tTraining Loss: 0.0001269709365482375\tValidation Loss: 0.00012999481445759585\n",
      "Epoch 8141  \tTraining Loss: 0.00012696899621090336\tValidation Loss: 0.000129993194307826\n",
      "Epoch 8142  \tTraining Loss: 0.00012696705663826154\tValidation Loss: 0.0001299916158845934\n",
      "Epoch 8143  \tTraining Loss: 0.00012696511627131784\tValidation Loss: 0.00012999003622642997\n",
      "Epoch 8144  \tTraining Loss: 0.0001269631775201376\tValidation Loss: 0.00012998841771856464\n",
      "Epoch 8145  \tTraining Loss: 0.00012696123885432448\tValidation Loss: 0.00012998684093646753\n",
      "Epoch 8146  \tTraining Loss: 0.00012695929973361738\tValidation Loss: 0.00012998526283163514\n",
      "Epoch 8147  \tTraining Loss: 0.00012695736255512974\tValidation Loss: 0.0001299836457803292\n",
      "Epoch 8148  \tTraining Loss: 0.00012695542480450672\tValidation Loss: 0.00012998207048833278\n",
      "Epoch 8149  \tTraining Loss: 0.00012695348768141488\tValidation Loss: 0.0001299803904209728\n",
      "Epoch 8150  \tTraining Loss: 0.00012695155244754193\tValidation Loss: 0.0001299788993275164\n",
      "Epoch 8151  \tTraining Loss: 0.00012694961465696718\tValidation Loss: 0.00012997733162021708\n",
      "Epoch 8152  \tTraining Loss: 0.0001269476790490563\tValidation Loss: 0.00012997571625444714\n",
      "Epoch 8153  \tTraining Loss: 0.0001269457438295406\tValidation Loss: 0.0001299741411778442\n",
      "Epoch 8154  \tTraining Loss: 0.00012694380801430243\tValidation Loss: 0.0001299725646279824\n",
      "Epoch 8155  \tTraining Loss: 0.00012694187399579933\tValidation Loss: 0.00012997094934130953\n",
      "Epoch 8156  \tTraining Loss: 0.00012693993969463126\tValidation Loss: 0.0001299693763292601\n",
      "Epoch 8157  \tTraining Loss: 0.00012693800519369137\tValidation Loss: 0.00012996776883384263\n",
      "Epoch 8158  \tTraining Loss: 0.00012693607278306703\tValidation Loss: 0.00012996619858833037\n",
      "Epoch 8159  \tTraining Loss: 0.00012693413900529145\tValidation Loss: 0.0001299646263735033\n",
      "Epoch 8160  \tTraining Loss: 0.00012693220645707036\tValidation Loss: 0.00012996301485245396\n",
      "Epoch 8161  \tTraining Loss: 0.00012693027481071445\tValidation Loss: 0.00012996144525271143\n",
      "Epoch 8162  \tTraining Loss: 0.00012692834229491268\tValidation Loss: 0.00012995987417428105\n",
      "Epoch 8163  \tTraining Loss: 0.00012692641132756288\tValidation Loss: 0.0001299582638985944\n",
      "Epoch 8164  \tTraining Loss: 0.0001269244807721452\tValidation Loss: 0.00012995662570038812\n",
      "Epoch 8165  \tTraining Loss: 0.00012692255033426158\tValidation Loss: 0.00012995513787016948\n",
      "Epoch 8166  \tTraining Loss: 0.00012692062046467198\tValidation Loss: 0.00012995353628500293\n",
      "Epoch 8167  \tTraining Loss: 0.00012691869046623836\tValidation Loss: 0.00012995196851557948\n",
      "Epoch 8168  \tTraining Loss: 0.0001269167606892491\tValidation Loss: 0.00012995036433218816\n",
      "Epoch 8169  \tTraining Loss: 0.0001269148326107887\tValidation Loss: 0.00012994879738707673\n",
      "Epoch 8170  \tTraining Loss: 0.00012691290336006302\tValidation Loss: 0.00012994722862203476\n",
      "Epoch 8171  \tTraining Loss: 0.00012691097552495383\tValidation Loss: 0.00012994562069112352\n",
      "Epoch 8172  \tTraining Loss: 0.0001269090482149139\tValidation Loss: 0.00012994405516178863\n",
      "Epoch 8173  \tTraining Loss: 0.0001269071202209439\tValidation Loss: 0.00012994248827982084\n",
      "Epoch 8174  \tTraining Loss: 0.00012690519396026562\tValidation Loss: 0.00012994088218735392\n",
      "Epoch 8175  \tTraining Loss: 0.0001269032675437068\tValidation Loss: 0.00012993931844798416\n",
      "Epoch 8176  \tTraining Loss: 0.00012690134079599265\tValidation Loss: 0.00012993771963024393\n",
      "Epoch 8177  \tTraining Loss: 0.00012689941626332105\tValidation Loss: 0.00012993615821375388\n",
      "Epoch 8178  \tTraining Loss: 0.00012689749029516008\tValidation Loss: 0.00012993459456447372\n",
      "Epoch 8179  \tTraining Loss: 0.00012689556549711373\tValidation Loss: 0.00012993299125095093\n",
      "Epoch 8180  \tTraining Loss: 0.00012689364199536092\tValidation Loss: 0.0001299313605217418\n",
      "Epoch 8181  \tTraining Loss: 0.000126891718031523\tValidation Loss: 0.00012992987947980957\n",
      "Epoch 8182  \tTraining Loss: 0.0001268897943406492\tValidation Loss: 0.00012992828458211318\n",
      "Epoch 8183  \tTraining Loss: 0.00012688787129394057\tValidation Loss: 0.00012992672381142677\n",
      "Epoch 8184  \tTraining Loss: 0.00012688594780772852\tValidation Loss: 0.00012992516003059788\n",
      "Epoch 8185  \tTraining Loss: 0.00012688402625447162\tValidation Loss: 0.00012992355676416572\n",
      "Epoch 8186  \tTraining Loss: 0.00012688210413979738\tValidation Loss: 0.00012992199637618758\n",
      "Epoch 8187  \tTraining Loss: 0.0001268801820908445\tValidation Loss: 0.00012992040105274028\n",
      "Epoch 8188  \tTraining Loss: 0.00012687826186445918\tValidation Loss: 0.00012991884363394466\n",
      "Epoch 8189  \tTraining Loss: 0.00012687634039606414\tValidation Loss: 0.0001299172841283782\n",
      "Epoch 8190  \tTraining Loss: 0.00012687442028998244\tValidation Loss: 0.00012991568495951945\n",
      "Epoch 8191  \tTraining Loss: 0.00012687250081503548\tValidation Loss: 0.00012991412834783832\n",
      "Epoch 8192  \tTraining Loss: 0.00012687058059693575\tValidation Loss: 0.00012991257009794648\n",
      "Epoch 8193  \tTraining Loss: 0.00012686866206233386\tValidation Loss: 0.00012991097225680163\n",
      "Epoch 8194  \tTraining Loss: 0.00012686674346950024\tValidation Loss: 0.0001299094170575589\n",
      "Epoch 8195  \tTraining Loss: 0.00012686482447815092\tValidation Loss: 0.00012990786019647586\n",
      "Epoch 8196  \tTraining Loss: 0.00012686290774815577\tValidation Loss: 0.00012990619448362378\n",
      "Epoch 8197  \tTraining Loss: 0.00012686099094384964\tValidation Loss: 0.00012990472204999154\n",
      "Epoch 8198  \tTraining Loss: 0.00012685907287673053\tValidation Loss: 0.00012990313997246616\n",
      "Epoch 8199  \tTraining Loss: 0.00012685715690560848\tValidation Loss: 0.00012990158744530256\n",
      "Epoch 8200  \tTraining Loss: 0.0001268552399221768\tValidation Loss: 0.00012990003112366838\n",
      "Epoch 8201  \tTraining Loss: 0.00012685332449825413\tValidation Loss: 0.00012989843482970502\n",
      "Epoch 8202  \tTraining Loss: 0.00012685140930745185\tValidation Loss: 0.0001298968815955628\n",
      "Epoch 8203  \tTraining Loss: 0.00012684949357051313\tValidation Loss: 0.00012989532698631957\n",
      "Epoch 8204  \tTraining Loss: 0.00012684757971042498\tValidation Loss: 0.00012989373291558907\n",
      "Epoch 8205  \tTraining Loss: 0.0001268456654023806\tValidation Loss: 0.0001298921819263107\n",
      "Epoch 8206  \tTraining Loss: 0.00012684375104497943\tValidation Loss: 0.00012989059540780449\n",
      "Epoch 8207  \tTraining Loss: 0.00012684183861582063\tValidation Loss: 0.0001298890469170622\n",
      "Epoch 8208  \tTraining Loss: 0.00012683992488594792\tValidation Loss: 0.00012988749604194604\n",
      "Epoch 8209  \tTraining Loss: 0.00012683801247142992\tValidation Loss: 0.00012988590511728187\n",
      "Epoch 8210  \tTraining Loss: 0.0001268361007810069\tValidation Loss: 0.00012988435703778297\n",
      "Epoch 8211  \tTraining Loss: 0.00012683418829502466\tValidation Loss: 0.00012988280714329843\n",
      "Epoch 8212  \tTraining Loss: 0.00012683227756213255\tValidation Loss: 0.00012988114845859084\n",
      "Epoch 8213  \tTraining Loss: 0.00012683036776968288\tValidation Loss: 0.00012987968282112352\n",
      "Epoch 8214  \tTraining Loss: 0.00012682845583195454\tValidation Loss: 0.00012987810739479783\n",
      "Epoch 8215  \tTraining Loss: 0.00012682654673643176\tValidation Loss: 0.00012987656183060736\n",
      "Epoch 8216  \tTraining Loss: 0.00012682463625068798\tValidation Loss: 0.0001298750123770494\n",
      "Epoch 8217  \tTraining Loss: 0.00012682272695840546\tValidation Loss: 0.0001298734227388506\n",
      "Epoch 8218  \tTraining Loss: 0.0001268208186311765\tValidation Loss: 0.000129871876469013\n",
      "Epoch 8219  \tTraining Loss: 0.00012681890938652898\tValidation Loss: 0.00012987032872946723\n",
      "Epoch 8220  \tTraining Loss: 0.00012681700165805058\tValidation Loss: 0.0001298687413169667\n",
      "Epoch 8221  \tTraining Loss: 0.00012681509420247664\tValidation Loss: 0.0001298671972955926\n",
      "Epoch 8222  \tTraining Loss: 0.0001268131861746864\tValidation Loss: 0.00012986565160095047\n",
      "Epoch 8223  \tTraining Loss: 0.000126811279999314\tValidation Loss: 0.00012986406600466862\n",
      "Epoch 8224  \tTraining Loss: 0.00012680937342094147\tValidation Loss: 0.00012986252372431919\n",
      "Epoch 8225  \tTraining Loss: 0.00012680746674212145\tValidation Loss: 0.00012986094546437772\n",
      "Epoch 8226  \tTraining Loss: 0.00012680556203919987\tValidation Loss: 0.00012985940551054403\n",
      "Epoch 8227  \tTraining Loss: 0.00012680365600628807\tValidation Loss: 0.00012985786299561663\n",
      "Epoch 8228  \tTraining Loss: 0.00012680175126675094\tValidation Loss: 0.0001298562801322693\n",
      "Epoch 8229  \tTraining Loss: 0.0001267998475718308\tValidation Loss: 0.00012985467177974988\n",
      "Epoch 8230  \tTraining Loss: 0.00012679794350919983\tValidation Loss: 0.00012985321122269307\n",
      "Epoch 8231  \tTraining Loss: 0.0001267960398647678\tValidation Loss: 0.00012985163658893554\n",
      "Epoch 8232  \tTraining Loss: 0.00012679413660038124\tValidation Loss: 0.00012985009716472466\n",
      "Epoch 8233  \tTraining Loss: 0.00012679223303409826\tValidation Loss: 0.00012984852018379646\n",
      "Epoch 8234  \tTraining Loss: 0.00012679033167328977\tValidation Loss: 0.0001298469816453137\n",
      "Epoch 8235  \tTraining Loss: 0.00012678842886737805\tValidation Loss: 0.00012984544086587548\n",
      "Epoch 8236  \tTraining Loss: 0.0001267865272344565\tValidation Loss: 0.00012984386002835914\n",
      "Epoch 8237  \tTraining Loss: 0.0001267846266058585\tValidation Loss: 0.00012984232287601308\n",
      "Epoch 8238  \tTraining Loss: 0.00012678272503374204\tValidation Loss: 0.00012984078396950145\n",
      "Epoch 8239  \tTraining Loss: 0.00012678082496368417\tValidation Loss: 0.00012983920496571376\n",
      "Epoch 8240  \tTraining Loss: 0.00012677892519383756\tValidation Loss: 0.00012983766960195981\n",
      "Epoch 8241  \tTraining Loss: 0.00012677702483234905\tValidation Loss: 0.00012983613235055953\n",
      "Epoch 8242  \tTraining Loss: 0.00012677512631271428\tValidation Loss: 0.00012983455486853976\n",
      "Epoch 8243  \tTraining Loss: 0.0001267732274096817\tValidation Loss: 0.00012983302103575684\n",
      "Epoch 8244  \tTraining Loss: 0.00012677132838164137\tValidation Loss: 0.00012983145084553689\n",
      "Epoch 8245  \tTraining Loss: 0.00012676943134980854\tValidation Loss: 0.0001298299192948702\n",
      "Epoch 8246  \tTraining Loss: 0.00012676753344265345\tValidation Loss: 0.0001298283166861196\n",
      "Epoch 8247  \tTraining Loss: 0.00012676563688483232\tValidation Loss: 0.0001298268226501112\n",
      "Epoch 8248  \tTraining Loss: 0.00012676373998334246\tValidation Loss: 0.0001298252984398798\n",
      "Epoch 8249  \tTraining Loss: 0.0001267618428090795\tValidation Loss: 0.00012982376402743105\n",
      "Epoch 8250  \tTraining Loss: 0.00012675994738947394\tValidation Loss: 0.0001298221878327834\n",
      "Epoch 8251  \tTraining Loss: 0.00012675805180860134\tValidation Loss: 0.00012982065542465314\n",
      "Epoch 8252  \tTraining Loss: 0.00012675615587716784\tValidation Loss: 0.0001298190868797828\n",
      "Epoch 8253  \tTraining Loss: 0.00012675426217159197\tValidation Loss: 0.00012981755751755568\n",
      "Epoch 8254  \tTraining Loss: 0.00012675236700348234\tValidation Loss: 0.0001298160257283836\n",
      "Epoch 8255  \tTraining Loss: 0.00012675047300658016\tValidation Loss: 0.00012981445345102924\n",
      "Epoch 8256  \tTraining Loss: 0.00012674858001753903\tValidation Loss: 0.00012981292507567853\n",
      "Epoch 8257  \tTraining Loss: 0.00012674668607739955\tValidation Loss: 0.00012981139469620933\n",
      "Epoch 8258  \tTraining Loss: 0.00012674479364146727\tValidation Loss: 0.00012980982386118147\n",
      "Epoch 8259  \tTraining Loss: 0.00012674290150023755\tValidation Loss: 0.00012980829698026788\n",
      "Epoch 8260  \tTraining Loss: 0.00012674100876449107\tValidation Loss: 0.00012980676804565333\n",
      "Epoch 8261  \tTraining Loss: 0.00012673911787617058\tValidation Loss: 0.00012980519858585583\n",
      "Epoch 8262  \tTraining Loss: 0.00012673722659179096\tValidation Loss: 0.00012980367313552917\n",
      "Epoch 8263  \tTraining Loss: 0.00012673533564273986\tValidation Loss: 0.00012980204305809917\n",
      "Epoch 8264  \tTraining Loss: 0.000126733446897184\tValidation Loss: 0.00012980060037774596\n",
      "Epoch 8265  \tTraining Loss: 0.00012673155637877385\tValidation Loss: 0.00012979897987421766\n",
      "Epoch 8266  \tTraining Loss: 0.00012672966845368857\tValidation Loss: 0.0001297973937778633\n",
      "Epoch 8267  \tTraining Loss: 0.00012672778117277535\tValidation Loss: 0.00012979585867195875\n",
      "Epoch 8268  \tTraining Loss: 0.0001267258932329058\tValidation Loss: 0.00012979432476250825\n",
      "Epoch 8269  \tTraining Loss: 0.00012672400696385323\tValidation Loss: 0.00012979275230388078\n",
      "Epoch 8270  \tTraining Loss: 0.0001267221206541382\tValidation Loss: 0.0001297912255275982\n",
      "Epoch 8271  \tTraining Loss: 0.00012672023393447098\tValidation Loss: 0.00012978969756842354\n",
      "Epoch 8272  \tTraining Loss: 0.00012671834918165077\tValidation Loss: 0.00012978812960358826\n",
      "Epoch 8273  \tTraining Loss: 0.00012671646379589543\tValidation Loss: 0.000129786606424428\n",
      "Epoch 8274  \tTraining Loss: 0.00012671457851258668\tValidation Loss: 0.00012978504659950737\n",
      "Epoch 8275  \tTraining Loss: 0.00012671269503134895\tValidation Loss: 0.0001297835263074256\n",
      "Epoch 8276  \tTraining Loss: 0.00012671081030793976\tValidation Loss: 0.00012978200326110466\n",
      "Epoch 8277  \tTraining Loss: 0.00012670892695583415\tValidation Loss: 0.00012978037174227303\n",
      "Epoch 8278  \tTraining Loss: 0.0001267070454171342\tValidation Loss: 0.00012977893243087987\n",
      "Epoch 8279  \tTraining Loss: 0.00012670516120991953\tValidation Loss: 0.00012977741767064728\n",
      "Epoch 8280  \tTraining Loss: 0.00012670327947212918\tValidation Loss: 0.00012977585408592356\n",
      "Epoch 8281  \tTraining Loss: 0.0001267013975045015\tValidation Loss: 0.00012977433370561296\n",
      "Epoch 8282  \tTraining Loss: 0.0001266995152481752\tValidation Loss: 0.00012977277629165008\n",
      "Epoch 8283  \tTraining Loss: 0.0001266976351885456\tValidation Loss: 0.0001297712587906114\n",
      "Epoch 8284  \tTraining Loss: 0.0001266957536851207\tValidation Loss: 0.0001297697387785782\n",
      "Epoch 8285  \tTraining Loss: 0.00012669387333435572\tValidation Loss: 0.00012976817801800617\n",
      "Epoch 8286  \tTraining Loss: 0.0001266919940254319\tValidation Loss: 0.00012976666200552285\n",
      "Epoch 8287  \tTraining Loss: 0.00012669011375146774\tValidation Loss: 0.00012976514388340416\n",
      "Epoch 8288  \tTraining Loss: 0.00012668823493143402\tValidation Loss: 0.00012976358499120708\n",
      "Epoch 8289  \tTraining Loss: 0.00012668635650246065\tValidation Loss: 0.00012976207085023235\n",
      "Epoch 8290  \tTraining Loss: 0.00012668447743328064\tValidation Loss: 0.0001297605545072206\n",
      "Epoch 8291  \tTraining Loss: 0.00012668260012931826\tValidation Loss: 0.000129758997291877\n",
      "Epoch 8292  \tTraining Loss: 0.00012668072277239396\tValidation Loss: 0.00012975741744527934\n",
      "Epoch 8293  \tTraining Loss: 0.00012667884582205632\tValidation Loss: 0.00012975594791728328\n",
      "Epoch 8294  \tTraining Loss: 0.00012667696957522847\tValidation Loss: 0.00012975444487331293\n",
      "Epoch 8295  \tTraining Loss: 0.00012667509245075756\tValidation Loss: 0.00012975293078246978\n",
      "Epoch 8296  \tTraining Loss: 0.00012667321662004135\tValidation Loss: 0.0001297513741047457\n",
      "Epoch 8297  \tTraining Loss: 0.00012667134158771817\tValidation Loss: 0.00012974986219970242\n",
      "Epoch 8298  \tTraining Loss: 0.00012666946571042152\tValidation Loss: 0.00012974834832773472\n",
      "Epoch 8299  \tTraining Loss: 0.00012666759139675848\tValidation Loss: 0.0001297467938118555\n",
      "Epoch 8300  \tTraining Loss: 0.0001266657172474149\tValidation Loss: 0.00012974528453343784\n",
      "Epoch 8301  \tTraining Loss: 0.00012666384256748074\tValidation Loss: 0.00012974377316277264\n",
      "Epoch 8302  \tTraining Loss: 0.00012666196976013192\tValidation Loss: 0.00012974222088782481\n",
      "Epoch 8303  \tTraining Loss: 0.00012666009649570442\tValidation Loss: 0.00012974071372134707\n",
      "Epoch 8304  \tTraining Loss: 0.0001266582231422047\tValidation Loss: 0.00012973916911369893\n",
      "Epoch 8305  \tTraining Loss: 0.00012665635176714216\tValidation Loss: 0.00012973766448236827\n",
      "Epoch 8306  \tTraining Loss: 0.00012665447904845663\tValidation Loss: 0.0001297361567725763\n",
      "Epoch 8307  \tTraining Loss: 0.00012665260799826298\tValidation Loss: 0.00012973454063394983\n",
      "Epoch 8308  \tTraining Loss: 0.00012665073814678533\tValidation Loss: 0.00012973311633852924\n",
      "Epoch 8309  \tTraining Loss: 0.00012664886592385525\tValidation Loss: 0.00012973161659580388\n",
      "Epoch 8310  \tTraining Loss: 0.00012664699608038878\tValidation Loss: 0.0001297300676131177\n",
      "Epoch 8311  \tTraining Loss: 0.00012664512617677933\tValidation Loss: 0.00012972856244527684\n",
      "Epoch 8312  \tTraining Loss: 0.00012664325586535499\tValidation Loss: 0.00012972705489481707\n",
      "Epoch 8313  \tTraining Loss: 0.00012664138753212966\tValidation Loss: 0.00012972550650016112\n",
      "Epoch 8314  \tTraining Loss: 0.0001266395185241722\tValidation Loss: 0.000129724003740398\n",
      "Epoch 8315  \tTraining Loss: 0.00012663764963760229\tValidation Loss: 0.00012972246357428267\n",
      "Epoch 8316  \tTraining Loss: 0.00012663578251973276\tValidation Loss: 0.00012972096379244676\n",
      "Epoch 8317  \tTraining Loss: 0.00012663391415985737\tValidation Loss: 0.00012971946098458515\n",
      "Epoch 8318  \tTraining Loss: 0.0001266320471618854\tValidation Loss: 0.00012971791668676142\n",
      "Epoch 8319  \tTraining Loss: 0.00012663018077533247\tValidation Loss: 0.00012971641763109482\n",
      "Epoch 8320  \tTraining Loss: 0.0001266283136272031\tValidation Loss: 0.00012971491607831023\n",
      "Epoch 8321  \tTraining Loss: 0.00012662644816527804\tValidation Loss: 0.00012971330639320567\n",
      "Epoch 8322  \tTraining Loss: 0.00012662458381631827\tValidation Loss: 0.00012971188853720295\n",
      "Epoch 8323  \tTraining Loss: 0.0001266227171800306\tValidation Loss: 0.00012971035989471858\n",
      "Epoch 8324  \tTraining Loss: 0.00012662085338710572\tValidation Loss: 0.00012970886356589707\n",
      "Epoch 8325  \tTraining Loss: 0.00012661898818485472\tValidation Loss: 0.0001297073626663618\n",
      "Epoch 8326  \tTraining Loss: 0.0001266171241463777\tValidation Loss: 0.00012970582008904448\n",
      "Epoch 8327  \tTraining Loss: 0.0001266152611128069\tValidation Loss: 0.00012970432324373537\n",
      "Epoch 8328  \tTraining Loss: 0.00012661339711930405\tValidation Loss: 0.00012970282421414\n",
      "Epoch 8329  \tTraining Loss: 0.00012661153458732376\tValidation Loss: 0.0001297012840017556\n",
      "Epoch 8330  \tTraining Loss: 0.00012660967241733144\tValidation Loss: 0.0001296997895357513\n",
      "Epoch 8331  \tTraining Loss: 0.00012660780960793217\tValidation Loss: 0.00012969829267257542\n",
      "Epoch 8332  \tTraining Loss: 0.00012660594857145807\tValidation Loss: 0.00012969675439001503\n",
      "Epoch 8333  \tTraining Loss: 0.00012660408727054682\tValidation Loss: 0.00012969526177528955\n",
      "Epoch 8334  \tTraining Loss: 0.0001266022256765453\tValidation Loss: 0.00012969373108977152\n",
      "Epoch 8335  \tTraining Loss: 0.00012660036625835417\tValidation Loss: 0.00012969224092036603\n",
      "Epoch 8336  \tTraining Loss: 0.00012659850558858162\tValidation Loss: 0.00012969068082218112\n",
      "Epoch 8337  \tTraining Loss: 0.00012659664674996807\tValidation Loss: 0.00012968922509345928\n",
      "Epoch 8338  \tTraining Loss: 0.0001265947874587828\tValidation Loss: 0.00012968774204307715\n",
      "Epoch 8339  \tTraining Loss: 0.00012659292776547956\tValidation Loss: 0.00012968624837096342\n",
      "Epoch 8340  \tTraining Loss: 0.0001265910696755738\tValidation Loss: 0.00012968471171000403\n",
      "Epoch 8341  \tTraining Loss: 0.00012658921170817674\tValidation Loss: 0.0001296832208301566\n",
      "Epoch 8342  \tTraining Loss: 0.00012658735322078145\tValidation Loss: 0.00012968172770685876\n",
      "Epoch 8343  \tTraining Loss: 0.00012658549661782965\tValidation Loss: 0.00012968019329988775\n",
      "Epoch 8344  \tTraining Loss: 0.00012658363952348846\tValidation Loss: 0.00012967870505108407\n",
      "Epoch 8345  \tTraining Loss: 0.00012658178235788254\tValidation Loss: 0.00012967717871027954\n",
      "Epoch 8346  \tTraining Loss: 0.00012657992714608827\tValidation Loss: 0.0001296756932406032\n",
      "Epoch 8347  \tTraining Loss: 0.0001265780705907906\tValidation Loss: 0.00012967420445590648\n",
      "Epoch 8348  \tTraining Loss: 0.0001265762153071577\tValidation Loss: 0.0001296726736862577\n",
      "Epoch 8349  \tTraining Loss: 0.0001265743608108698\tValidation Loss: 0.0001296711887339728\n",
      "Epoch 8350  \tTraining Loss: 0.00012657250545589302\tValidation Loss: 0.00012966970104344753\n",
      "Epoch 8351  \tTraining Loss: 0.00012657065190097654\tValidation Loss: 0.0001296681053181703\n",
      "Epoch 8352  \tTraining Loss: 0.00012656879922555\tValidation Loss: 0.0001296667010953736\n",
      "Epoch 8353  \tTraining Loss: 0.0001265669443235495\tValidation Loss: 0.00012966522152397917\n",
      "Epoch 8354  \tTraining Loss: 0.00012656509213243778\tValidation Loss: 0.00012966369217090224\n",
      "Epoch 8355  \tTraining Loss: 0.0001265632392110753\tValidation Loss: 0.00012966220758478409\n",
      "Epoch 8356  \tTraining Loss: 0.00012656138646689535\tValidation Loss: 0.00012966068449301007\n",
      "Epoch 8357  \tTraining Loss: 0.00012655953543941955\tValidation Loss: 0.00012965920272592943\n",
      "Epoch 8358  \tTraining Loss: 0.0001265576831845751\tValidation Loss: 0.0001296577178646496\n",
      "Epoch 8359  \tTraining Loss: 0.00012655583231512593\tValidation Loss: 0.00012965619111572374\n",
      "Epoch 8360  \tTraining Loss: 0.00012655398200267485\tValidation Loss: 0.00012965471061326386\n",
      "Epoch 8361  \tTraining Loss: 0.0001265521309438046\tValidation Loss: 0.00012965322743626505\n",
      "Epoch 8362  \tTraining Loss: 0.0001265502815741378\tValidation Loss: 0.00012965170235520145\n",
      "Epoch 8363  \tTraining Loss: 0.00012654843210691986\tValidation Loss: 0.00012965022352403506\n",
      "Epoch 8364  \tTraining Loss: 0.00012654658222043867\tValidation Loss: 0.0001296487419277464\n",
      "Epoch 8365  \tTraining Loss: 0.0001265447343378593\tValidation Loss: 0.00012964721745411275\n",
      "Epoch 8366  \tTraining Loss: 0.00012654288595868502\tValidation Loss: 0.00012964567305974158\n",
      "Epoch 8367  \tTraining Loss: 0.00012654103834671904\tValidation Loss: 0.00012964423517270845\n",
      "Epoch 8368  \tTraining Loss: 0.00012653919101045382\tValidation Loss: 0.00012964276584644772\n",
      "Epoch 8369  \tTraining Loss: 0.00012653734301708433\tValidation Loss: 0.00012964128490637708\n",
      "Epoch 8370  \tTraining Loss: 0.0001265354965572323\tValidation Loss: 0.00012963976030880514\n",
      "Epoch 8371  \tTraining Loss: 0.00012653365040178052\tValidation Loss: 0.00012963828212825116\n",
      "Epoch 8372  \tTraining Loss: 0.00012653180362580987\tValidation Loss: 0.0001296368012959658\n",
      "Epoch 8373  \tTraining Loss: 0.0001265299586584421\tValidation Loss: 0.00012963527871612005\n",
      "Epoch 8374  \tTraining Loss: 0.0001265281133514653\tValidation Loss: 0.00012963380300425503\n",
      "Epoch 8375  \tTraining Loss: 0.00012652626781577337\tValidation Loss: 0.0001296322884631721\n",
      "Epoch 8376  \tTraining Loss: 0.00012652442438242434\tValidation Loss: 0.00012963081548908928\n",
      "Epoch 8377  \tTraining Loss: 0.00012652257952266673\tValidation Loss: 0.0001296293388312491\n",
      "Epoch 8378  \tTraining Loss: 0.00012652073586671048\tValidation Loss: 0.00012962781975546308\n",
      "Epoch 8379  \tTraining Loss: 0.0001265188931314966\tValidation Loss: 0.0001296263472811432\n",
      "Epoch 8380  \tTraining Loss: 0.00012651704946200096\tValidation Loss: 0.00012962487185613113\n",
      "Epoch 8381  \tTraining Loss: 0.00012651520766191344\tValidation Loss: 0.00012962328854143574\n",
      "Epoch 8382  \tTraining Loss: 0.0001265133666006861\tValidation Loss: 0.0001296218966181432\n",
      "Epoch 8383  \tTraining Loss: 0.00012651152337257802\tValidation Loss: 0.00012962042917386395\n",
      "Epoch 8384  \tTraining Loss: 0.00012650968280150635\tValidation Loss: 0.00012961891158255975\n",
      "Epoch 8385  \tTraining Loss: 0.00012650784160819802\tValidation Loss: 0.0001296174394948344\n",
      "Epoch 8386  \tTraining Loss: 0.00012650600047723888\tValidation Loss: 0.0001296159282024437\n",
      "Epoch 8387  \tTraining Loss: 0.0001265041611699457\tValidation Loss: 0.00012961445869970944\n",
      "Epoch 8388  \tTraining Loss: 0.00012650232056879074\tValidation Loss: 0.00012961298588344046\n",
      "Epoch 8389  \tTraining Loss: 0.00012650048130460347\tValidation Loss: 0.00012961147081350292\n",
      "Epoch 8390  \tTraining Loss: 0.00012649864269104118\tValidation Loss: 0.00012961000272449796\n",
      "Epoch 8391  \tTraining Loss: 0.0001264968032750785\tValidation Loss: 0.00012960853162990947\n",
      "Epoch 8392  \tTraining Loss: 0.00012649496550640058\tValidation Loss: 0.00012960701821671842\n",
      "Epoch 8393  \tTraining Loss: 0.00012649312772091423\tValidation Loss: 0.000129605551774026\n",
      "Epoch 8394  \tTraining Loss: 0.0001264912894666404\tValidation Loss: 0.00012960408223022766\n",
      "Epoch 8395  \tTraining Loss: 0.00012648945318095527\tValidation Loss: 0.00012960257026586156\n",
      "Epoch 8396  \tTraining Loss: 0.0001264876165788787\tValidation Loss: 0.0001296010398393387\n",
      "Epoch 8397  \tTraining Loss: 0.00012648578044517525\tValidation Loss: 0.00012959961443726\n",
      "Epoch 8398  \tTraining Loss: 0.00012648394476418191\tValidation Loss: 0.00012959815840890327\n",
      "Epoch 8399  \tTraining Loss: 0.00012648210838524425\tValidation Loss: 0.00012959669062002896\n",
      "Epoch 8400  \tTraining Loss: 0.0001264802735132748\tValidation Loss: 0.00012959517875886896\n",
      "Epoch 8401  \tTraining Loss: 0.00012647843899610983\tValidation Loss: 0.00012959371391393576\n",
      "Epoch 8402  \tTraining Loss: 0.00012647660382411198\tValidation Loss: 0.0001295922462244209\n",
      "Epoch 8403  \tTraining Loss: 0.00012647477044094404\tValidation Loss: 0.0001295907363606557\n",
      "Epoch 8404  \tTraining Loss: 0.0001264729367548934\tValidation Loss: 0.00012958927396069592\n",
      "Epoch 8405  \tTraining Loss: 0.00012647110279695545\tValidation Loss: 0.00012958777217043246\n",
      "Epoch 8406  \tTraining Loss: 0.0001264692709762518\tValidation Loss: 0.00012958631250294494\n",
      "Epoch 8407  \tTraining Loss: 0.00012646743770289515\tValidation Loss: 0.0001295848489479711\n",
      "Epoch 8408  \tTraining Loss: 0.00012646560562232635\tValidation Loss: 0.00012958334254470943\n",
      "Epoch 8409  \tTraining Loss: 0.00012646377448194396\tValidation Loss: 0.00012958188328418446\n",
      "Epoch 8410  \tTraining Loss: 0.00012646194240767134\tValidation Loss: 0.00012958035553203325\n",
      "Epoch 8411  \tTraining Loss: 0.00012646011287730132\tValidation Loss: 0.00012957892915637232\n",
      "Epoch 8412  \tTraining Loss: 0.00012645828195618982\tValidation Loss: 0.00012957747774111872\n",
      "Epoch 8413  \tTraining Loss: 0.00012645645099034026\tValidation Loss: 0.00012957601526581797\n",
      "Epoch 8414  \tTraining Loss: 0.00012645462200097006\tValidation Loss: 0.00012957450881905056\n",
      "Epoch 8415  \tTraining Loss: 0.0001264527923769092\tValidation Loss: 0.00012957304982649738\n",
      "Epoch 8416  \tTraining Loss: 0.0001264509628061046\tValidation Loss: 0.00012957155141528448\n",
      "Epoch 8417  \tTraining Loss: 0.0001264491350533466\tValidation Loss: 0.00012957009560566056\n",
      "Epoch 8418  \tTraining Loss: 0.00012644730600503887\tValidation Loss: 0.00012956863606208854\n",
      "Epoch 8419  \tTraining Loss: 0.00012644547830620973\tValidation Loss: 0.0001295671336873442\n",
      "Epoch 8420  \tTraining Loss: 0.0001264436512320081\tValidation Loss: 0.00012956567881307435\n",
      "Epoch 8421  \tTraining Loss: 0.0001264418233597021\tValidation Loss: 0.00012956422066649095\n",
      "Epoch 8422  \tTraining Loss: 0.00012643999715433903\tValidation Loss: 0.00012956271972721094\n",
      "Epoch 8423  \tTraining Loss: 0.00012643817089114969\tValidation Loss: 0.0001295612663406448\n",
      "Epoch 8424  \tTraining Loss: 0.00012643634417107319\tValidation Loss: 0.0001295598096296257\n",
      "Epoch 8425  \tTraining Loss: 0.0001264345194746819\tValidation Loss: 0.00012955824525680097\n",
      "Epoch 8426  \tTraining Loss: 0.0001264326952041368\tValidation Loss: 0.00012955687184563333\n",
      "Epoch 8427  \tTraining Loss: 0.0001264308692971067\tValidation Loss: 0.00012955538630991353\n",
      "Epoch 8428  \tTraining Loss: 0.00012642904555972982\tValidation Loss: 0.00012955393541507588\n",
      "Epoch 8429  \tTraining Loss: 0.00012642722072343283\tValidation Loss: 0.00012955247912087729\n",
      "Epoch 8430  \tTraining Loss: 0.0001264253974029013\tValidation Loss: 0.00012955097965735137\n",
      "Epoch 8431  \tTraining Loss: 0.0001264235743718664\tValidation Loss: 0.00012954952817272698\n",
      "Epoch 8432  \tTraining Loss: 0.0001264217507084495\tValidation Loss: 0.00012954807364952075\n",
      "Epoch 8433  \tTraining Loss: 0.00012641992887573022\tValidation Loss: 0.00012954657644012455\n",
      "Epoch 8434  \tTraining Loss: 0.00012641810665541412\tValidation Loss: 0.00012954512721654717\n",
      "Epoch 8435  \tTraining Loss: 0.00012641628424276178\tValidation Loss: 0.00012954363795514967\n",
      "Epoch 8436  \tTraining Loss: 0.00012641446387950386\tValidation Loss: 0.00012954219133257347\n",
      "Epoch 8437  \tTraining Loss: 0.00012641264209927513\tValidation Loss: 0.0001295407405689176\n",
      "Epoch 8438  \tTraining Loss: 0.00012641082156294873\tValidation Loss: 0.00012953924649503992\n",
      "Epoch 8439  \tTraining Loss: 0.000126409001863036\tValidation Loss: 0.0001295378001580746\n",
      "Epoch 8440  \tTraining Loss: 0.00012640718140340603\tValidation Loss: 0.00012953628568222657\n",
      "Epoch 8441  \tTraining Loss: 0.00012640536328482345\tValidation Loss: 0.0001295348712049415\n",
      "Epoch 8442  \tTraining Loss: 0.00012640354378679777\tValidation Loss: 0.0001295334325605475\n",
      "Epoch 8443  \tTraining Loss: 0.00012640172429549584\tValidation Loss: 0.00012953198271360722\n",
      "Epoch 8444  \tTraining Loss: 0.0001263999068459078\tValidation Loss: 0.0001295304884813592\n",
      "Epoch 8445  \tTraining Loss: 0.00012639808862941688\tValidation Loss: 0.00012952904232424033\n",
      "Epoch 8446  \tTraining Loss: 0.00012639627059276288\tValidation Loss: 0.0001295275561784557\n",
      "Epoch 8447  \tTraining Loss: 0.00012639445424003702\tValidation Loss: 0.00012952611321026173\n",
      "Epoch 8448  \tTraining Loss: 0.00012639263665067689\tValidation Loss: 0.00012952466629540291\n",
      "Epoch 8449  \tTraining Loss: 0.0001263908204846193\tValidation Loss: 0.00012952317611531462\n",
      "Epoch 8450  \tTraining Loss: 0.00012638900479391586\tValidation Loss: 0.00012952173405064444\n",
      "Epoch 8451  \tTraining Loss: 0.00012638718837132615\tValidation Loss: 0.00012952028850958346\n",
      "Epoch 8452  \tTraining Loss: 0.0001263853736963488\tValidation Loss: 0.00012951879974414644\n",
      "Epoch 8453  \tTraining Loss: 0.00012638355880048146\tValidation Loss: 0.0001295173591475574\n",
      "Epoch 8454  \tTraining Loss: 0.000126381743543432\tValidation Loss: 0.00012951587800822515\n",
      "Epoch 8455  \tTraining Loss: 0.00012637993064613294\tValidation Loss: 0.00012951437541444634\n",
      "Epoch 8456  \tTraining Loss: 0.0001263781170248958\tValidation Loss: 0.00012951301114471378\n",
      "Epoch 8457  \tTraining Loss: 0.00012637630317146806\tValidation Loss: 0.0001295115313067831\n",
      "Epoch 8458  \tTraining Loss: 0.00012637449062184272\tValidation Loss: 0.0001295100918862363\n",
      "Epoch 8459  \tTraining Loss: 0.00012637267723507393\tValidation Loss: 0.00012950864759696067\n",
      "Epoch 8460  \tTraining Loss: 0.00012637086545880783\tValidation Loss: 0.00012950715997754752\n",
      "Epoch 8461  \tTraining Loss: 0.0001263690537344772\tValidation Loss: 0.00012950572106835441\n",
      "Epoch 8462  \tTraining Loss: 0.00012636724148881582\tValidation Loss: 0.0001295042789793399\n",
      "Epoch 8463  \tTraining Loss: 0.0001263654311872918\tValidation Loss: 0.0001295027938117454\n",
      "Epoch 8464  \tTraining Loss: 0.00012636362026635894\tValidation Loss: 0.0001295013572752471\n",
      "Epoch 8465  \tTraining Loss: 0.00012636180937769114\tValidation Loss: 0.0001294998801425739\n",
      "Epoch 8466  \tTraining Loss: 0.00012636000030806103\tValidation Loss: 0.00012949844623722423\n",
      "Epoch 8467  \tTraining Loss: 0.00012635818992870197\tValidation Loss: 0.00012949700798299196\n",
      "Epoch 8468  \tTraining Loss: 0.0001263563809137313\tValidation Loss: 0.00012949552598674727\n",
      "Epoch 8469  \tTraining Loss: 0.00012635457249220012\tValidation Loss: 0.00012949409234753347\n",
      "Epoch 8470  \tTraining Loss: 0.00012635276353809623\tValidation Loss: 0.00012949259093323693\n",
      "Epoch 8471  \tTraining Loss: 0.00012635095682557708\tValidation Loss: 0.000129491188150785\n",
      "Epoch 8472  \tTraining Loss: 0.00012634914858941203\tValidation Loss: 0.00012948976208383212\n",
      "Epoch 8473  \tTraining Loss: 0.00012634734057611653\tValidation Loss: 0.00012948828742100099\n",
      "Epoch 8474  \tTraining Loss: 0.0001263455346509201\tValidation Loss: 0.0001294868548057296\n",
      "Epoch 8475  \tTraining Loss: 0.00012634372729595708\tValidation Loss: 0.00012948541779069173\n",
      "Epoch 8476  \tTraining Loss: 0.0001263419211726308\tValidation Loss: 0.00012948393729626128\n",
      "Epoch 8477  \tTraining Loss: 0.00012634011590486954\tValidation Loss: 0.00012948250572205507\n",
      "Epoch 8478  \tTraining Loss: 0.00012633830970777297\tValidation Loss: 0.00012948107071863904\n",
      "Epoch 8479  \tTraining Loss: 0.00012633650507385481\tValidation Loss: 0.00012947959228338003\n",
      "Epoch 8480  \tTraining Loss: 0.00012633470058667312\tValidation Loss: 0.00012947816269519568\n",
      "Epoch 8481  \tTraining Loss: 0.00012633289552424148\tValidation Loss: 0.00012947672948698168\n",
      "Epoch 8482  \tTraining Loss: 0.00012633109236814588\tValidation Loss: 0.00012947525265980054\n",
      "Epoch 8483  \tTraining Loss: 0.0001263292886687539\tValidation Loss: 0.0001294738246489253\n",
      "Epoch 8484  \tTraining Loss: 0.00012632748492338996\tValidation Loss: 0.00012947235553432176\n",
      "Epoch 8485  \tTraining Loss: 0.00012632568332914153\tValidation Loss: 0.00012947086612278197\n",
      "Epoch 8486  \tTraining Loss: 0.00012632388094093125\tValidation Loss: 0.00012946951389349953\n",
      "Epoch 8487  \tTraining Loss: 0.000126322078595868\tValidation Loss: 0.00012946804592581023\n",
      "Epoch 8488  \tTraining Loss: 0.0001263202772217094\tValidation Loss: 0.0001294666190631764\n",
      "Epoch 8489  \tTraining Loss: 0.0001263184751693292\tValidation Loss: 0.00012946518714755298\n",
      "Epoch 8490  \tTraining Loss: 0.00012631667489957662\tValidation Loss: 0.00012946371147638875\n",
      "Epoch 8491  \tTraining Loss: 0.0001263148743356838\tValidation Loss: 0.00012946228514199468\n",
      "Epoch 8492  \tTraining Loss: 0.0001263130734748524\tValidation Loss: 0.00012946081794172872\n",
      "Epoch 8493  \tTraining Loss: 0.00012631127475621357\tValidation Loss: 0.0001294593947047825\n",
      "Epoch 8494  \tTraining Loss: 0.0001263094745589067\tValidation Loss: 0.00012945796710009275\n",
      "Epoch 8495  \tTraining Loss: 0.0001263076755724322\tValidation Loss: 0.00012945649549063554\n",
      "Epoch 8496  \tTraining Loss: 0.0001263058774858852\tValidation Loss: 0.00012945507287721452\n",
      "Epoch 8497  \tTraining Loss: 0.0001263040784419685\tValidation Loss: 0.00012945364643628823\n",
      "Epoch 8498  \tTraining Loss: 0.00012630228094693538\tValidation Loss: 0.00012945217608666994\n",
      "Epoch 8499  \tTraining Loss: 0.00012630048362735903\tValidation Loss: 0.00012945075482881957\n",
      "Epoch 8500  \tTraining Loss: 0.00012629868609027617\tValidation Loss: 0.00012944926621694526\n",
      "Epoch 8501  \tTraining Loss: 0.00012629689077291387\tValidation Loss: 0.00012944787490921288\n",
      "Epoch 8502  \tTraining Loss: 0.0001262950936306715\tValidation Loss: 0.0001294464612299404\n",
      "Epoch 8503  \tTraining Loss: 0.00012629329711719538\tValidation Loss: 0.00012944499845602344\n",
      "Epoch 8504  \tTraining Loss: 0.00012629150228125917\tValidation Loss: 0.0001294435783437351\n",
      "Epoch 8505  \tTraining Loss: 0.000126289706213184\tValidation Loss: 0.00012944215362985584\n",
      "Epoch 8506  \tTraining Loss: 0.00012628791158708248\tValidation Loss: 0.00012944068500655355\n",
      "Epoch 8507  \tTraining Loss: 0.00012628611739392732\tValidation Loss: 0.00012943926593190035\n",
      "Epoch 8508  \tTraining Loss: 0.00012628432247475198\tValidation Loss: 0.0001294378432245865\n",
      "Epoch 8509  \tTraining Loss: 0.00012628252933520883\tValidation Loss: 0.00012943637665228713\n",
      "Epoch 8510  \tTraining Loss: 0.0001262807359076963\tValidation Loss: 0.00012943495955323583\n",
      "Epoch 8511  \tTraining Loss: 0.00012627894217684084\tValidation Loss: 0.0001294335009112636\n",
      "Epoch 8512  \tTraining Loss: 0.0001262771505854236\tValidation Loss: 0.0001294320863284613\n",
      "Epoch 8513  \tTraining Loss: 0.00012627535751146913\tValidation Loss: 0.00012943066703769945\n",
      "Epoch 8514  \tTraining Loss: 0.0001262735656570205\tValidation Loss: 0.00012942920332056776\n",
      "Epoch 8515  \tTraining Loss: 0.00012627177505874305\tValidation Loss: 0.00012942772566902025\n",
      "Epoch 8516  \tTraining Loss: 0.00012626998384001337\tValidation Loss: 0.00012942638479282128\n",
      "Epoch 8517  \tTraining Loss: 0.00012626819300120382\tValidation Loss: 0.00012942492827535196\n",
      "Epoch 8518  \tTraining Loss: 0.0001262664026279202\tValidation Loss: 0.00012942351366572363\n",
      "Epoch 8519  \tTraining Loss: 0.00012626461182276564\tValidation Loss: 0.00012942209387797242\n",
      "Epoch 8520  \tTraining Loss: 0.00012626282304804395\tValidation Loss: 0.00012942062994550574\n",
      "Epoch 8521  \tTraining Loss: 0.00012626103347880752\tValidation Loss: 0.00012941921600705824\n",
      "Epoch 8522  \tTraining Loss: 0.00012625924410597373\tValidation Loss: 0.0001294177606396671\n",
      "Epoch 8523  \tTraining Loss: 0.0001262574563766451\tValidation Loss: 0.00012941634983418238\n",
      "Epoch 8524  \tTraining Loss: 0.00012625566741038106\tValidation Loss: 0.00012941493445162172\n",
      "Epoch 8525  \tTraining Loss: 0.00012625387990928923\tValidation Loss: 0.0001294134746297853\n",
      "Epoch 8526  \tTraining Loss: 0.00012625209279752296\tValidation Loss: 0.00012941206443330833\n",
      "Epoch 8527  \tTraining Loss: 0.0001262503049757858\tValidation Loss: 0.0001294106502036954\n",
      "Epoch 8528  \tTraining Loss: 0.00012624851896333173\tValidation Loss: 0.00012940919163019574\n",
      "Epoch 8529  \tTraining Loss: 0.0001262467326037672\tValidation Loss: 0.00012940778277694106\n",
      "Epoch 8530  \tTraining Loss: 0.00012624494643531008\tValidation Loss: 0.00012940628505578943\n",
      "Epoch 8531  \tTraining Loss: 0.0001262431623925819\tValidation Loss: 0.00012940495800772026\n",
      "Epoch 8532  \tTraining Loss: 0.00012624137546999453\tValidation Loss: 0.00012940355212315494\n",
      "Epoch 8533  \tTraining Loss: 0.00012623959062121897\tValidation Loss: 0.00012940209321738558\n",
      "Epoch 8534  \tTraining Loss: 0.00012623780636988936\tValidation Loss: 0.00012940068245997615\n",
      "Epoch 8535  \tTraining Loss: 0.00012623602132421255\tValidation Loss: 0.00012939926742132487\n",
      "Epoch 8536  \tTraining Loss: 0.00012623423793869126\tValidation Loss: 0.0001293978081967759\n",
      "Epoch 8537  \tTraining Loss: 0.0001262324544905429\tValidation Loss: 0.0001293963992483012\n",
      "Epoch 8538  \tTraining Loss: 0.00012623067056555996\tValidation Loss: 0.0001293949865089404\n",
      "Epoch 8539  \tTraining Loss: 0.00012622888860845948\tValidation Loss: 0.00012939352952138106\n",
      "Epoch 8540  \tTraining Loss: 0.0001262271059679024\tValidation Loss: 0.00012939212272828498\n",
      "Epoch 8541  \tTraining Loss: 0.00012622532337958927\tValidation Loss: 0.00012939067384567303\n",
      "Epoch 8542  \tTraining Loss: 0.00012622354260411644\tValidation Loss: 0.00012938926970306708\n",
      "Epoch 8543  \tTraining Loss: 0.00012622176051129192\tValidation Loss: 0.0001293878606829267\n",
      "Epoch 8544  \tTraining Loss: 0.0001262199800094627\tValidation Loss: 0.0001293863443062694\n",
      "Epoch 8545  \tTraining Loss: 0.00012621820085550325\tValidation Loss: 0.00012938501755603178\n",
      "Epoch 8546  \tTraining Loss: 0.00012621641917059302\tValidation Loss: 0.00012938361560263178\n",
      "Epoch 8547  \tTraining Loss: 0.00012621463994100037\tValidation Loss: 0.00012938216151972591\n",
      "Epoch 8548  \tTraining Loss: 0.0001262128604862352\tValidation Loss: 0.0001293807564513818\n",
      "Epoch 8549  \tTraining Loss: 0.00012621108064832446\tValidation Loss: 0.00012937934732144994\n",
      "Epoch 8550  \tTraining Loss: 0.00012620930284692187\tValidation Loss: 0.0001293778939856069\n",
      "Epoch 8551  \tTraining Loss: 0.0001262075242175927\tValidation Loss: 0.00012937649134565145\n",
      "Epoch 8552  \tTraining Loss: 0.00012620574577684602\tValidation Loss: 0.00012937504662510912\n",
      "Epoch 8553  \tTraining Loss: 0.00012620396901640536\tValidation Loss: 0.00012937364703018706\n",
      "Epoch 8554  \tTraining Loss: 0.00012620219100274876\tValidation Loss: 0.0001293722425894542\n",
      "Epoch 8555  \tTraining Loss: 0.0001262004143773628\tValidation Loss: 0.00012937079323914604\n",
      "Epoch 8556  \tTraining Loss: 0.00012619863827560823\tValidation Loss: 0.0001293693941615114\n",
      "Epoch 8557  \tTraining Loss: 0.0001261968613986562\tValidation Loss: 0.000129367990835706\n",
      "Epoch 8558  \tTraining Loss: 0.00012619508627658812\tValidation Loss: 0.00012936648045284432\n",
      "Epoch 8559  \tTraining Loss: 0.00012619331212614075\tValidation Loss: 0.00012936515966525288\n",
      "Epoch 8560  \tTraining Loss: 0.00012619153568781805\tValidation Loss: 0.00012936372538087517\n",
      "Epoch 8561  \tTraining Loss: 0.00012618976208573306\tValidation Loss: 0.00012936232863874626\n",
      "Epoch 8562  \tTraining Loss: 0.0001261879870291159\tValidation Loss: 0.00012936092564882183\n",
      "Epoch 8563  \tTraining Loss: 0.00012618621312806953\tValidation Loss: 0.0001293594776147833\n",
      "Epoch 8564  \tTraining Loss: 0.00012618444021403381\tValidation Loss: 0.00012935808036996534\n",
      "Epoch 8565  \tTraining Loss: 0.0001261826662919391\tValidation Loss: 0.0001293566792039447\n",
      "Epoch 8566  \tTraining Loss: 0.00012618089381759748\tValidation Loss: 0.00012935523346231182\n",
      "Epoch 8567  \tTraining Loss: 0.0001261791216978374\tValidation Loss: 0.0001293538385080575\n",
      "Epoch 8568  \tTraining Loss: 0.000126177348885428\tValidation Loss: 0.00012935243940788118\n",
      "Epoch 8569  \tTraining Loss: 0.00012617557782717358\tValidation Loss: 0.0001293509954873471\n",
      "Epoch 8570  \tTraining Loss: 0.00012617380650732134\tValidation Loss: 0.00012934960226971192\n",
      "Epoch 8571  \tTraining Loss: 0.0001261720348081278\tValidation Loss: 0.0001293481662855409\n",
      "Epoch 8572  \tTraining Loss: 0.00012617026534999232\tValidation Loss: 0.0001293467755454332\n",
      "Epoch 8573  \tTraining Loss: 0.00012616849484723582\tValidation Loss: 0.00012934531752143246\n",
      "Epoch 8574  \tTraining Loss: 0.0001261667255700218\tValidation Loss: 0.0001293439531678641\n",
      "Epoch 8575  \tTraining Loss: 0.00012616495615808875\tValidation Loss: 0.00012934256850296032\n",
      "Epoch 8576  \tTraining Loss: 0.00012616318626435135\tValidation Loss: 0.00012934117195826406\n",
      "Epoch 8577  \tTraining Loss: 0.00012616141792507707\tValidation Loss: 0.00012933972917283161\n",
      "Epoch 8578  \tTraining Loss: 0.00012615964977287823\tValidation Loss: 0.00012933833726316745\n",
      "Epoch 8579  \tTraining Loss: 0.0001261578810118881\tValidation Loss: 0.00012933694138650436\n",
      "Epoch 8580  \tTraining Loss: 0.00012615611408241356\tValidation Loss: 0.00012933550084169874\n",
      "Epoch 8581  \tTraining Loss: 0.00012615434673379352\tValidation Loss: 0.00012933411150225336\n",
      "Epoch 8582  \tTraining Loss: 0.00012615257916003087\tValidation Loss: 0.00012933267938116554\n",
      "Epoch 8583  \tTraining Loss: 0.0001261508136724203\tValidation Loss: 0.00012933129286601\n",
      "Epoch 8584  \tTraining Loss: 0.00012614904672094608\tValidation Loss: 0.00012932990120516832\n",
      "Epoch 8585  \tTraining Loss: 0.00012614728096169983\tValidation Loss: 0.00012932846413444697\n",
      "Epoch 8586  \tTraining Loss: 0.00012614551611665566\tValidation Loss: 0.00012932707791024967\n",
      "Epoch 8587  \tTraining Loss: 0.00012614375062096607\tValidation Loss: 0.0001293256253270087\n",
      "Epoch 8588  \tTraining Loss: 0.000126141987066295\tValidation Loss: 0.0001293242660577649\n",
      "Epoch 8589  \tTraining Loss: 0.00012614022239754643\tValidation Loss: 0.0001293228870178606\n",
      "Epoch 8590  \tTraining Loss: 0.00012613845764930992\tValidation Loss: 0.0001293214960867453\n",
      "Epoch 8591  \tTraining Loss: 0.00012613669484835268\tValidation Loss: 0.0001293200587499357\n",
      "Epoch 8592  \tTraining Loss: 0.00012613493144421039\tValidation Loss: 0.0001293186726203756\n",
      "Epoch 8593  \tTraining Loss: 0.00012613316799491958\tValidation Loss: 0.00012931724367607977\n",
      "Epoch 8594  \tTraining Loss: 0.00012613140645572173\tValidation Loss: 0.00012931586081873586\n",
      "Epoch 8595  \tTraining Loss: 0.00012612964353822837\tValidation Loss: 0.0001293144729654914\n",
      "Epoch 8596  \tTraining Loss: 0.00012612788190058142\tValidation Loss: 0.0001293130397171958\n",
      "Epoch 8597  \tTraining Loss: 0.00012612612100062616\tValidation Loss: 0.00012931165767657987\n",
      "Epoch 8598  \tTraining Loss: 0.00012612435920766074\tValidation Loss: 0.00012931027114987124\n",
      "Epoch 8599  \tTraining Loss: 0.000126122598997644\tValidation Loss: 0.00012930883927886826\n",
      "Epoch 8600  \tTraining Loss: 0.00012612083887106164\tValidation Loss: 0.00012930745867197955\n",
      "Epoch 8601  \tTraining Loss: 0.00012611907837036324\tValidation Loss: 0.00012930601192177353\n",
      "Epoch 8602  \tTraining Loss: 0.00012611732049000785\tValidation Loss: 0.00012930465788533378\n",
      "Epoch 8603  \tTraining Loss: 0.0001261155605426267\tValidation Loss: 0.0001293032845277946\n",
      "Epoch 8604  \tTraining Loss: 0.00012611380119833298\tValidation Loss: 0.00012930186031639548\n",
      "Epoch 8605  \tTraining Loss: 0.0001261120435731409\tValidation Loss: 0.00012930048085176094\n",
      "Epoch 8606  \tTraining Loss: 0.00012611028467912397\tValidation Loss: 0.00012929909607646055\n",
      "Epoch 8607  \tTraining Loss: 0.0001261085271636632\tValidation Loss: 0.00012929766593018435\n",
      "Epoch 8608  \tTraining Loss: 0.00012610677018510722\tValidation Loss: 0.00012929628748962801\n",
      "Epoch 8609  \tTraining Loss: 0.0001261050124115847\tValidation Loss: 0.00012929490471761586\n",
      "Epoch 8610  \tTraining Loss: 0.00012610325632123897\tValidation Loss: 0.00012929347661838562\n",
      "Epoch 8611  \tTraining Loss: 0.0001261015001127441\tValidation Loss: 0.0001292921001466254\n",
      "Epoch 8612  \tTraining Loss: 0.00012609974343643382\tValidation Loss: 0.00012929071914747289\n",
      "Epoch 8613  \tTraining Loss: 0.0001260979887602084\tValidation Loss: 0.0001292892926304356\n",
      "Epoch 8614  \tTraining Loss: 0.00012609623332850454\tValidation Loss: 0.00012928791770788433\n",
      "Epoch 8615  \tTraining Loss: 0.00012609447808435408\tValidation Loss: 0.0001292864379015342\n",
      "Epoch 8616  \tTraining Loss: 0.00012609272566045088\tValidation Loss: 0.0001292851416432993\n",
      "Epoch 8617  \tTraining Loss: 0.00012609097004371854\tValidation Loss: 0.0001292837694842187\n",
      "Epoch 8618  \tTraining Loss: 0.00012608921663851463\tValidation Loss: 0.00012928234399291094\n",
      "Epoch 8619  \tTraining Loss: 0.00012608746354531036\tValidation Loss: 0.000129280968892182\n",
      "Epoch 8620  \tTraining Loss: 0.00012608570978070674\tValidation Loss: 0.00012927958915208697\n",
      "Epoch 8621  \tTraining Loss: 0.00012608395781153278\tValidation Loss: 0.00012927816411166277\n",
      "Epoch 8622  \tTraining Loss: 0.00012608220549668766\tValidation Loss: 0.00012927679119915478\n",
      "Epoch 8623  \tTraining Loss: 0.00012608045288034224\tValidation Loss: 0.00012927537478159957\n",
      "Epoch 8624  \tTraining Loss: 0.00012607870241357393\tValidation Loss: 0.00012927400484226715\n",
      "Epoch 8625  \tTraining Loss: 0.00012607695044041304\tValidation Loss: 0.00012927262950552618\n",
      "Epoch 8626  \tTraining Loss: 0.0001260751996422979\tValidation Loss: 0.00012927120791654303\n",
      "Epoch 8627  \tTraining Loss: 0.00012607344979260468\tValidation Loss: 0.00012926983796964994\n",
      "Epoch 8628  \tTraining Loss: 0.0001260716989360973\tValidation Loss: 0.0001292684632487672\n",
      "Epoch 8629  \tTraining Loss: 0.0001260699495687838\tValidation Loss: 0.000129267042696936\n",
      "Epoch 8630  \tTraining Loss: 0.00012606820091991407\tValidation Loss: 0.0001292656129471082\n",
      "Epoch 8631  \tTraining Loss: 0.000126066451820309\tValidation Loss: 0.0001292643156542874\n",
      "Epoch 8632  \tTraining Loss: 0.00012606470339188283\tValidation Loss: 0.00012926290209380013\n",
      "Epoch 8633  \tTraining Loss: 0.0001260629548994292\tValidation Loss: 0.00012926153317762085\n",
      "Epoch 8634  \tTraining Loss: 0.00012606120641787572\tValidation Loss: 0.0001292601190832591\n",
      "Epoch 8635  \tTraining Loss: 0.00012605945980316067\tValidation Loss: 0.00012925875157615379\n",
      "Epoch 8636  \tTraining Loss: 0.00012605771182294212\tValidation Loss: 0.0001292573788586375\n",
      "Epoch 8637  \tTraining Loss: 0.00012605596515257003\tValidation Loss: 0.00012925596032668132\n",
      "Epoch 8638  \tTraining Loss: 0.00012605421915802442\tValidation Loss: 0.00012925459404525679\n",
      "Epoch 8639  \tTraining Loss: 0.00012605247229051727\tValidation Loss: 0.0001292532231198135\n",
      "Epoch 8640  \tTraining Loss: 0.00012605072704849254\tValidation Loss: 0.00012925180634952956\n",
      "Epoch 8641  \tTraining Loss: 0.00012604898180398208\tValidation Loss: 0.00012925044178293507\n",
      "Epoch 8642  \tTraining Loss: 0.00012604723602552872\tValidation Loss: 0.00012924907243021117\n",
      "Epoch 8643  \tTraining Loss: 0.00012604549220003185\tValidation Loss: 0.0001292476570918188\n",
      "Epoch 8644  \tTraining Loss: 0.00012604374806503553\tValidation Loss: 0.00012924623316036977\n",
      "Epoch 8645  \tTraining Loss: 0.00012604200435286823\tValidation Loss: 0.0001292449017626481\n",
      "Epoch 8646  \tTraining Loss: 0.0001260402610979055\tValidation Loss: 0.00012924354678271006\n",
      "Epoch 8647  \tTraining Loss: 0.00012603851707315909\tValidation Loss: 0.00012924217878125836\n",
      "Epoch 8648  \tTraining Loss: 0.00012603677453252348\tValidation Loss: 0.0001292407632602692\n",
      "Epoch 8649  \tTraining Loss: 0.00012603503236531514\tValidation Loss: 0.00012923940000890552\n",
      "Epoch 8650  \tTraining Loss: 0.0001260332894761234\tValidation Loss: 0.0001292380322264639\n",
      "Epoch 8651  \tTraining Loss: 0.00012603154835956942\tValidation Loss: 0.00012923661870212672\n",
      "Epoch 8652  \tTraining Loss: 0.00012602980694414778\tValidation Loss: 0.0001292352578556454\n",
      "Epoch 8653  \tTraining Loss: 0.0001260280651790874\tValidation Loss: 0.00012923385281376248\n",
      "Epoch 8654  \tTraining Loss: 0.0001260263256010829\tValidation Loss: 0.00012923249476962326\n",
      "Epoch 8655  \tTraining Loss: 0.0001260245844899732\tValidation Loss: 0.00012923113104150178\n",
      "Epoch 8656  \tTraining Loss: 0.0001260228445468296\tValidation Loss: 0.0001292297208532552\n",
      "Epoch 8657  \tTraining Loss: 0.00012602110556648533\tValidation Loss: 0.00012922836300578039\n",
      "Epoch 8658  \tTraining Loss: 0.00012601936596875118\tValidation Loss: 0.00012922693960522444\n",
      "Epoch 8659  \tTraining Loss: 0.00012601762816792414\tValidation Loss: 0.00012922560629998337\n",
      "Epoch 8660  \tTraining Loss: 0.0001260158893176119\tValidation Loss: 0.0001292242553406043\n",
      "Epoch 8661  \tTraining Loss: 0.00012601415037071152\tValidation Loss: 0.00012922289213403542\n",
      "Epoch 8662  \tTraining Loss: 0.00012601241339525832\tValidation Loss: 0.0001292214815281919\n",
      "Epoch 8663  \tTraining Loss: 0.0001260106757679196\tValidation Loss: 0.00012922012364982805\n",
      "Epoch 8664  \tTraining Loss: 0.00012600893813856782\tValidation Loss: 0.00012921872156340265\n",
      "Epoch 8665  \tTraining Loss: 0.00012600720235414088\tValidation Loss: 0.00012921736697741785\n",
      "Epoch 8666  \tTraining Loss: 0.0001260054652053544\tValidation Loss: 0.0001292160068661319\n",
      "Epoch 8667  \tTraining Loss: 0.00012600372939411354\tValidation Loss: 0.00012921460031456393\n",
      "Epoch 8668  \tTraining Loss: 0.00012600199420494742\tValidation Loss: 0.00012921324647507831\n",
      "Epoch 8669  \tTraining Loss: 0.00012600025816132565\tValidation Loss: 0.0001292118876398284\n",
      "Epoch 8670  \tTraining Loss: 0.00012599852378198086\tValidation Loss: 0.00012921048241994082\n",
      "Epoch 8671  \tTraining Loss: 0.00012599678932295\tValidation Loss: 0.00012920912997152004\n",
      "Epoch 8672  \tTraining Loss: 0.00012599505471549416\tValidation Loss: 0.00012920771218671566\n",
      "Epoch 8673  \tTraining Loss: 0.00012599332251667195\tValidation Loss: 0.00012920638389798196\n",
      "Epoch 8674  \tTraining Loss: 0.00012599158819108928\tValidation Loss: 0.00012920503841306024\n",
      "Epoch 8675  \tTraining Loss: 0.00012598985467700044\tValidation Loss: 0.000129203640822219\n",
      "Epoch 8676  \tTraining Loss: 0.00012598812265325084\tValidation Loss: 0.00012920228943848831\n",
      "Epoch 8677  \tTraining Loss: 0.00012598638945656536\tValidation Loss: 0.0001292009322278336\n",
      "Epoch 8678  \tTraining Loss: 0.00012598465777790928\tValidation Loss: 0.00012919952860640936\n",
      "Epoch 8679  \tTraining Loss: 0.00012598292635702563\tValidation Loss: 0.00012919817820495653\n",
      "Epoch 8680  \tTraining Loss: 0.0001259811942615844\tValidation Loss: 0.00012919682296404537\n",
      "Epoch 8681  \tTraining Loss: 0.00012597946401244072\tValidation Loss: 0.00012919542135594216\n",
      "Epoch 8682  \tTraining Loss: 0.00012597773331849495\tValidation Loss: 0.00012919407288960905\n",
      "Epoch 8683  \tTraining Loss: 0.00012597600241977304\tValidation Loss: 0.00012919267949511746\n",
      "Epoch 8684  \tTraining Loss: 0.00012597427355262226\tValidation Loss: 0.00012919133359736953\n",
      "Epoch 8685  \tTraining Loss: 0.00012597254322225393\tValidation Loss: 0.0001291899817205648\n",
      "Epoch 8686  \tTraining Loss: 0.00012597081447381423\tValidation Loss: 0.00012918852299003566\n",
      "Epoch 8687  \tTraining Loss: 0.00012596908711005444\tValidation Loss: 0.0001291872524802982\n",
      "Epoch 8688  \tTraining Loss: 0.0001259673571406197\tValidation Loss: 0.00012918590694943502\n",
      "Epoch 8689  \tTraining Loss: 0.0001259656296058455\tValidation Loss: 0.0001291845074788324\n",
      "Epoch 8690  \tTraining Loss: 0.0001259639018967715\tValidation Loss: 0.0001291831600788081\n",
      "Epoch 8691  \tTraining Loss: 0.0001259621737412925\tValidation Loss: 0.0001291818076081794\n",
      "Epoch 8692  \tTraining Loss: 0.00012596044763643895\tValidation Loss: 0.000129180408841701\n",
      "Epoch 8693  \tTraining Loss: 0.00012595872067186113\tValidation Loss: 0.00012917906375396404\n",
      "Epoch 8694  \tTraining Loss: 0.00012595699391283752\tValidation Loss: 0.00012917767376785825\n",
      "Epoch 8695  \tTraining Loss: 0.0001259552687761733\tValidation Loss: 0.0001291763316970141\n",
      "Epoch 8696  \tTraining Loss: 0.00012595354237842813\tValidation Loss: 0.00012917498370010376\n",
      "Epoch 8697  \tTraining Loss: 0.00012595181744291822\tValidation Loss: 0.00012917358868015706\n",
      "Epoch 8698  \tTraining Loss: 0.00012595009288085158\tValidation Loss: 0.00012917224691258355\n",
      "Epoch 8699  \tTraining Loss: 0.00012594836758055076\tValidation Loss: 0.00012917089986276805\n",
      "Epoch 8700  \tTraining Loss: 0.0001259466443944254\tValidation Loss: 0.0001291694462940169\n",
      "Epoch 8701  \tTraining Loss: 0.00012594492148141002\tValidation Loss: 0.00012916818095422722\n",
      "Epoch 8702  \tTraining Loss: 0.00012594319667277236\tValidation Loss: 0.0001291668034222654\n",
      "Epoch 8703  \tTraining Loss: 0.00012594147370496365\tValidation Loss: 0.0001291654714390828\n",
      "Epoch 8704  \tTraining Loss: 0.00012593974943075483\tValidation Loss: 0.0001291641298382077\n",
      "Epoch 8705  \tTraining Loss: 0.00012593802652996076\tValidation Loss: 0.00012916274031216364\n",
      "Epoch 8706  \tTraining Loss: 0.00012593630421071404\tValidation Loss: 0.00012916140401542794\n",
      "Epoch 8707  \tTraining Loss: 0.0001259345810488674\tValidation Loss: 0.0001291600623969372\n",
      "Epoch 8708  \tTraining Loss: 0.00012593285964139947\tValidation Loss: 0.00012915867385866777\n",
      "Epoch 8709  \tTraining Loss: 0.00012593113800307194\tValidation Loss: 0.00012915733893036825\n",
      "Epoch 8710  \tTraining Loss: 0.0001259294159787817\tValidation Loss: 0.00012915595850469415\n",
      "Epoch 8711  \tTraining Loss: 0.00012592769614175558\tValidation Loss: 0.00012915462607935297\n",
      "Epoch 8712  \tTraining Loss: 0.00012592597475743678\tValidation Loss: 0.00012915328747185946\n",
      "Epoch 8713  \tTraining Loss: 0.00012592425460799167\tValidation Loss: 0.00012915190151382533\n",
      "Epoch 8714  \tTraining Loss: 0.00012592253531073716\tValidation Loss: 0.00012915056907005846\n",
      "Epoch 8715  \tTraining Loss: 0.0001259208155504145\tValidation Loss: 0.00012914917166193183\n",
      "Epoch 8716  \tTraining Loss: 0.00012591909744594484\tValidation Loss: 0.00012914786184832806\n",
      "Epoch 8717  \tTraining Loss: 0.00012591737822106267\tValidation Loss: 0.00012914653603342832\n",
      "Epoch 8718  \tTraining Loss: 0.0001259156590216349\tValidation Loss: 0.00012914515735167604\n",
      "Epoch 8719  \tTraining Loss: 0.0001259139420695306\tValidation Loss: 0.00012914382574171317\n",
      "Epoch 8720  \tTraining Loss: 0.00012591222355441337\tValidation Loss: 0.0001291424879997174\n",
      "Epoch 8721  \tTraining Loss: 0.0001259105062345502\tValidation Loss: 0.00012914110321902034\n",
      "Epoch 8722  \tTraining Loss: 0.0001259087898368415\tValidation Loss: 0.00012913977256597477\n",
      "Epoch 8723  \tTraining Loss: 0.0001259070724202058\tValidation Loss: 0.00012913843677737286\n",
      "Epoch 8724  \tTraining Loss: 0.0001259053565691183\tValidation Loss: 0.00012913705399417844\n",
      "Epoch 8725  \tTraining Loss: 0.00012590364085165908\tValidation Loss: 0.00012913572526404514\n",
      "Epoch 8726  \tTraining Loss: 0.00012590192450902233\tValidation Loss: 0.00012913439119906526\n",
      "Epoch 8727  \tTraining Loss: 0.0001259002101132516\tValidation Loss: 0.0001291330099439482\n",
      "Epoch 8728  \tTraining Loss: 0.0001258984950836516\tValidation Loss: 0.00012913168271110612\n",
      "Epoch 8729  \tTraining Loss: 0.00012589678005374644\tValidation Loss: 0.00012913030955824905\n",
      "Epoch 8730  \tTraining Loss: 0.00012589506725615223\tValidation Loss: 0.0001291289255016904\n",
      "Epoch 8731  \tTraining Loss: 0.00012589335331937598\tValidation Loss: 0.0001291276693773008\n",
      "Epoch 8732  \tTraining Loss: 0.00012589163966784993\tValidation Loss: 0.0001291262960051693\n",
      "Epoch 8733  \tTraining Loss: 0.00012588992681366286\tValidation Loss: 0.00012912496946518374\n",
      "Epoch 8734  \tTraining Loss: 0.00012588821331573606\tValidation Loss: 0.0001291236363017223\n",
      "Epoch 8735  \tTraining Loss: 0.00012588650172306928\tValidation Loss: 0.00012912225585891014\n",
      "Epoch 8736  \tTraining Loss: 0.0001258847895799702\tValidation Loss: 0.00012912092999829475\n",
      "Epoch 8737  \tTraining Loss: 0.00012588307735038407\tValidation Loss: 0.0001291195584599204\n",
      "Epoch 8738  \tTraining Loss: 0.00012588136700945028\tValidation Loss: 0.00012911823578671503\n",
      "Epoch 8739  \tTraining Loss: 0.00012587965526175717\tValidation Loss: 0.00012911690694335725\n",
      "Epoch 8740  \tTraining Loss: 0.000125877944889137\tValidation Loss: 0.00012911553048686056\n",
      "Epoch 8741  \tTraining Loss: 0.0001258762350772322\tValidation Loss: 0.00012911420824160047\n",
      "Epoch 8742  \tTraining Loss: 0.00012587452442041\tValidation Loss: 0.00012911288043554984\n",
      "Epoch 8743  \tTraining Loss: 0.00012587281551380662\tValidation Loss: 0.00012911150512961295\n",
      "Epoch 8744  \tTraining Loss: 0.00012587110636806383\tValidation Loss: 0.00012911018414023433\n",
      "Epoch 8745  \tTraining Loss: 0.00012586939723318645\tValidation Loss: 0.0001291087580007458\n",
      "Epoch 8746  \tTraining Loss: 0.00012586769068457455\tValidation Loss: 0.00012910751411858063\n",
      "Epoch 8747  \tTraining Loss: 0.00012586598103137917\tValidation Loss: 0.0001291061944345456\n",
      "Epoch 8748  \tTraining Loss: 0.00012586427345494982\tValidation Loss: 0.00012910481975846134\n",
      "Epoch 8749  \tTraining Loss: 0.0001258625664857385\tValidation Loss: 0.000129103498321827\n",
      "Epoch 8750  \tTraining Loss: 0.00012586085866050855\tValidation Loss: 0.00012910217134185192\n",
      "Epoch 8751  \tTraining Loss: 0.0001258591525592329\tValidation Loss: 0.00012910079715494635\n",
      "Epoch 8752  \tTraining Loss: 0.00012585744626747566\tValidation Loss: 0.00012909947788240658\n",
      "Epoch 8753  \tTraining Loss: 0.00012585573952963408\tValidation Loss: 0.00012909811252117978\n",
      "Epoch 8754  \tTraining Loss: 0.00012585403503094131\tValidation Loss: 0.0001290967962479054\n",
      "Epoch 8755  \tTraining Loss: 0.00012585232894506737\tValidation Loss: 0.0001290954736032895\n",
      "Epoch 8756  \tTraining Loss: 0.00012585062406754433\tValidation Loss: 0.00012909410304690565\n",
      "Epoch 8757  \tTraining Loss: 0.000125848920084029\tValidation Loss: 0.00012909278700140003\n",
      "Epoch 8758  \tTraining Loss: 0.0001258472150837759\tValidation Loss: 0.00012909146525019136\n",
      "Epoch 8759  \tTraining Loss: 0.00012584551167469822\tValidation Loss: 0.0001290900957419579\n",
      "Epoch 8760  \tTraining Loss: 0.00012584380858154446\tValidation Loss: 0.00012908872214089382\n",
      "Epoch 8761  \tTraining Loss: 0.00012584210548843285\tValidation Loss: 0.00012908747622906644\n",
      "Epoch 8762  \tTraining Loss: 0.00012584040305272604\tValidation Loss: 0.0001290861132004957\n",
      "Epoch 8763  \tTraining Loss: 0.00012583870022395962\tValidation Loss: 0.00012908479787586763\n",
      "Epoch 8764  \tTraining Loss: 0.00012583699774923397\tValidation Loss: 0.00012908343485884132\n",
      "Epoch 8765  \tTraining Loss: 0.00012583529674399895\tValidation Loss: 0.0001290821210737053\n",
      "Epoch 8766  \tTraining Loss: 0.0001258335945359698\tValidation Loss: 0.00012908080111760731\n",
      "Epoch 8767  \tTraining Loss: 0.00012583189391331592\tValidation Loss: 0.00012907943341233295\n",
      "Epoch 8768  \tTraining Loss: 0.00012583019342691886\tValidation Loss: 0.00012907812074991652\n",
      "Epoch 8769  \tTraining Loss: 0.0001258284923002481\tValidation Loss: 0.00012907680251112436\n",
      "Epoch 8770  \tTraining Loss: 0.00012582679314270884\tValidation Loss: 0.00012907543649652765\n",
      "Epoch 8771  \tTraining Loss: 0.00012582509330629277\tValidation Loss: 0.00012907412548091582\n",
      "Epoch 8772  \tTraining Loss: 0.0001258233935121763\tValidation Loss: 0.00012907276770570363\n",
      "Epoch 8773  \tTraining Loss: 0.000125821695460079\tValidation Loss: 0.00012907145919677443\n",
      "Epoch 8774  \tTraining Loss: 0.00012581999606408497\tValidation Loss: 0.0001290701440168778\n",
      "Epoch 8775  \tTraining Loss: 0.00012581829834408984\tValidation Loss: 0.00012906872218862005\n",
      "Epoch 8776  \tTraining Loss: 0.00012581660179727095\tValidation Loss: 0.00012906748790780827\n",
      "Epoch 8777  \tTraining Loss: 0.00012581490273769745\tValidation Loss: 0.00012906617868076046\n",
      "Epoch 8778  \tTraining Loss: 0.000125813206379914\tValidation Loss: 0.00012906481436039688\n",
      "Epoch 8779  \tTraining Loss: 0.00012581150933288176\tValidation Loss: 0.0001290635040855665\n",
      "Epoch 8780  \tTraining Loss: 0.00012580981235115837\tValidation Loss: 0.00012906214697112726\n",
      "Epoch 8781  \tTraining Loss: 0.00012580811710015614\tValidation Loss: 0.00012906083970531707\n",
      "Epoch 8782  \tTraining Loss: 0.00012580642050925937\tValidation Loss: 0.00012905952613271316\n",
      "Epoch 8783  \tTraining Loss: 0.0001258047253867609\tValidation Loss: 0.00012905816447835198\n",
      "Epoch 8784  \tTraining Loss: 0.00012580303063654749\tValidation Loss: 0.00012905685810943857\n",
      "Epoch 8785  \tTraining Loss: 0.00012580133512300482\tValidation Loss: 0.00012905554596766503\n",
      "Epoch 8786  \tTraining Loss: 0.00012579964147061977\tValidation Loss: 0.0001290541857540401\n",
      "Epoch 8787  \tTraining Loss: 0.0001257979473563673\tValidation Loss: 0.0001290528808405927\n",
      "Epoch 8788  \tTraining Loss: 0.00012579625306839373\tValidation Loss: 0.00012905152881957977\n",
      "Epoch 8789  \tTraining Loss: 0.00012579456073111574\tValidation Loss: 0.00012905022636753417\n",
      "Epoch 8790  \tTraining Loss: 0.00012579286714503596\tValidation Loss: 0.00012904885890976486\n",
      "Epoch 8791  \tTraining Loss: 0.00012579117560439365\tValidation Loss: 0.00012904757539948015\n",
      "Epoch 8792  \tTraining Loss: 0.0001257894831474266\tValidation Loss: 0.00012904627803467637\n",
      "Epoch 8793  \tTraining Loss: 0.00012578779040488136\tValidation Loss: 0.00012904496767923541\n",
      "Epoch 8794  \tTraining Loss: 0.00012578610041820217\tValidation Loss: 0.00012904359016776865\n",
      "Epoch 8795  \tTraining Loss: 0.00012578441032242034\tValidation Loss: 0.0001290422901280705\n",
      "Epoch 8796  \tTraining Loss: 0.00012578272017605788\tValidation Loss: 0.00012904094485577153\n",
      "Epoch 8797  \tTraining Loss: 0.00012578103190009733\tValidation Loss: 0.00012903964918593234\n",
      "Epoch 8798  \tTraining Loss: 0.00012577934219763721\tValidation Loss: 0.000129038346258497\n",
      "Epoch 8799  \tTraining Loss: 0.0001257776540394377\tValidation Loss: 0.00012903699421567467\n",
      "Epoch 8800  \tTraining Loss: 0.00012577596614806794\tValidation Loss: 0.00012903569706579042\n",
      "Epoch 8801  \tTraining Loss: 0.0001257742775276759\tValidation Loss: 0.0001290343935511366\n",
      "Epoch 8802  \tTraining Loss: 0.0001257725909524888\tValidation Loss: 0.00012903304136842642\n",
      "Epoch 8803  \tTraining Loss: 0.00012577090358817052\tValidation Loss: 0.0001290317444729617\n",
      "Epoch 8804  \tTraining Loss: 0.0001257692164487133\tValidation Loss: 0.00012903039998634895\n",
      "Epoch 8805  \tTraining Loss: 0.0001257675307911323\tValidation Loss: 0.0001290291051495171\n",
      "Epoch 8806  \tTraining Loss: 0.00012576584389639462\tValidation Loss: 0.0001290278032728088\n",
      "Epoch 8807  \tTraining Loss: 0.000125764158733402\tValidation Loss: 0.00012902645250158202\n",
      "Epoch 8808  \tTraining Loss: 0.00012576247345690085\tValidation Loss: 0.000129025157095184\n",
      "Epoch 8809  \tTraining Loss: 0.00012576078774010997\tValidation Loss: 0.00012902381401425398\n",
      "Epoch 8810  \tTraining Loss: 0.0001257591041575536\tValidation Loss: 0.00012902252068514096\n",
      "Epoch 8811  \tTraining Loss: 0.0001257574190091687\tValidation Loss: 0.0001290212202936168\n",
      "Epoch 8812  \tTraining Loss: 0.00012575573527732044\tValidation Loss: 0.0001290198709510942\n",
      "Epoch 8813  \tTraining Loss: 0.00012575405206437007\tValidation Loss: 0.00012901857708343473\n",
      "Epoch 8814  \tTraining Loss: 0.00012575236799039878\tValidation Loss: 0.00012901727699654713\n",
      "Epoch 8815  \tTraining Loss: 0.00012575068584519451\tValidation Loss: 0.00012901592822903618\n",
      "Epoch 8816  \tTraining Loss: 0.0001257490031429623\tValidation Loss: 0.00012901463515321696\n",
      "Epoch 8817  \tTraining Loss: 0.00012574732043520826\tValidation Loss: 0.0001290132943249428\n",
      "Epoch 8818  \tTraining Loss: 0.0001257456394299451\tValidation Loss: 0.0001290120034752695\n",
      "Epoch 8819  \tTraining Loss: 0.00012574395707180009\tValidation Loss: 0.00012901070554249665\n",
      "Epoch 8820  \tTraining Loss: 0.00012574227634588102\tValidation Loss: 0.00012900935856667583\n",
      "Epoch 8821  \tTraining Loss: 0.00012574059570557748\tValidation Loss: 0.00012900806726561374\n",
      "Epoch 8822  \tTraining Loss: 0.00012573891442770993\tValidation Loss: 0.0001290067280623832\n",
      "Epoch 8823  \tTraining Loss: 0.00012573723547219034\tValidation Loss: 0.00012900543889506514\n",
      "Epoch 8824  \tTraining Loss: 0.00012573555485174216\tValidation Loss: 0.0001290041425905121\n",
      "Epoch 8825  \tTraining Loss: 0.00012573387556406816\tValidation Loss: 0.00012900279716392092\n",
      "Epoch 8826  \tTraining Loss: 0.0001257321969631335\tValidation Loss: 0.00012900150750695327\n",
      "Epoch 8827  \tTraining Loss: 0.00012573051741229276\tValidation Loss: 0.00012900021155515235\n",
      "Epoch 8828  \tTraining Loss: 0.0001257288397159272\tValidation Loss: 0.00012899886674881932\n",
      "Epoch 8829  \tTraining Loss: 0.0001257271616114466\tValidation Loss: 0.0001289975779265197\n",
      "Epoch 8830  \tTraining Loss: 0.00012572548335408184\tValidation Loss: 0.00012899624110907293\n",
      "Epoch 8831  \tTraining Loss: 0.00012572380693850931\tValidation Loss: 0.0001289949545441368\n",
      "Epoch 8832  \tTraining Loss: 0.0001257221290957995\tValidation Loss: 0.00012899366081201087\n",
      "Epoch 8833  \tTraining Loss: 0.0001257204528250538\tValidation Loss: 0.00012899231785768912\n",
      "Epoch 8834  \tTraining Loss: 0.00012571877676090866\tValidation Loss: 0.00012899103086729043\n",
      "Epoch 8835  \tTraining Loss: 0.00012571709998462989\tValidation Loss: 0.00012898973753421654\n",
      "Epoch 8836  \tTraining Loss: 0.0001257154253045738\tValidation Loss: 0.00012898839523389362\n",
      "Epoch 8837  \tTraining Loss: 0.0001257137497320756\tValidation Loss: 0.0001289871091031892\n",
      "Epoch 8838  \tTraining Loss: 0.00012571207449022277\tValidation Loss: 0.00012898577481798\n",
      "Epoch 8839  \tTraining Loss: 0.0001257104006038516\tValidation Loss: 0.00012898449095832717\n",
      "Epoch 8840  \tTraining Loss: 0.00012570872553123972\tValidation Loss: 0.0001289831998739408\n",
      "Epoch 8841  \tTraining Loss: 0.00012570705227653924\tValidation Loss: 0.00012898185944923814\n",
      "Epoch 8842  \tTraining Loss: 0.0001257053787361249\tValidation Loss: 0.0001289805751706724\n",
      "Epoch 8843  \tTraining Loss: 0.00012570370493036914\tValidation Loss: 0.0001289792425819311\n",
      "Epoch 8844  \tTraining Loss: 0.00012570203306425535\tValidation Loss: 0.0001289779604723117\n",
      "Epoch 8845  \tTraining Loss: 0.00012570035971793845\tValidation Loss: 0.00012897667107992017\n",
      "Epoch 8846  \tTraining Loss: 0.00012569868790774353\tValidation Loss: 0.00012897533226463394\n",
      "Epoch 8847  \tTraining Loss: 0.0001256970163778419\tValidation Loss: 0.0001289740496880678\n",
      "Epoch 8848  \tTraining Loss: 0.00012569534409458924\tValidation Loss: 0.00012897276067652737\n",
      "Epoch 8849  \tTraining Loss: 0.00012569367388041836\tValidation Loss: 0.00012897142251017154\n",
      "Epoch 8850  \tTraining Loss: 0.00012569200282917185\tValidation Loss: 0.0001289701407967724\n",
      "Epoch 8851  \tTraining Loss: 0.00012569033205484244\tValidation Loss: 0.00012896881067581074\n",
      "Epoch 8852  \tTraining Loss: 0.00012568866268266172\tValidation Loss: 0.00012896753124803825\n",
      "Epoch 8853  \tTraining Loss: 0.0001256869920968315\tValidation Loss: 0.00012896624450390654\n",
      "Epoch 8854  \tTraining Loss: 0.00012568532331437384\tValidation Loss: 0.00012896490823309804\n",
      "Epoch 8855  \tTraining Loss: 0.00012568365427613582\tValidation Loss: 0.00012896362839323995\n",
      "Epoch 8856  \tTraining Loss: 0.00012568198494396384\tValidation Loss: 0.00012896229998998271\n",
      "Epoch 8857  \tTraining Loss: 0.00012568031757340505\tValidation Loss: 0.00012896102233385073\n",
      "Epoch 8858  \tTraining Loss: 0.00012567864870768487\tValidation Loss: 0.00012895973730275284\n",
      "Epoch 8859  \tTraining Loss: 0.00012567698137548114\tValidation Loss: 0.00012895840266157648\n",
      "Epoch 8860  \tTraining Loss: 0.00012567531432958274\tValidation Loss: 0.00012895712454372727\n",
      "Epoch 8861  \tTraining Loss: 0.000125673646523301\tValidation Loss: 0.0001289558399024864\n",
      "Epoch 8862  \tTraining Loss: 0.00012567198079102738\tValidation Loss: 0.00012895450592000473\n",
      "Epoch 8863  \tTraining Loss: 0.00012567031421256426\tValidation Loss: 0.00012895322867538918\n",
      "Epoch 8864  \tTraining Loss: 0.00012566864792127802\tValidation Loss: 0.0001289519027685038\n",
      "Epoch 8865  \tTraining Loss: 0.00012566698301562737\tValidation Loss: 0.00012895062782247886\n",
      "Epoch 8866  \tTraining Loss: 0.0001256653169007317\tValidation Loss: 0.00012894934546667887\n",
      "Epoch 8867  \tTraining Loss: 0.0001256636526054722\tValidation Loss: 0.00012894801339571847\n",
      "Epoch 8868  \tTraining Loss: 0.0001256619880226828\tValidation Loss: 0.0001289467380399964\n",
      "Epoch 8869  \tTraining Loss: 0.00012566032317879467\tValidation Loss: 0.0001289454138650955\n",
      "Epoch 8870  \tTraining Loss: 0.00012565866025757982\tValidation Loss: 0.0001289441407048197\n",
      "Epoch 8871  \tTraining Loss: 0.00012565699585688123\tValidation Loss: 0.00012894286007551607\n",
      "Epoch 8872  \tTraining Loss: 0.00012565533301701472\tValidation Loss: 0.00012894152964699123\n",
      "Epoch 8873  \tTraining Loss: 0.00012565367040959536\tValidation Loss: 0.0001289402560260425\n",
      "Epoch 8874  \tTraining Loss: 0.00012565200706486038\tValidation Loss: 0.00012893897579161945\n",
      "Epoch 8875  \tTraining Loss: 0.00012565034582853269\tValidation Loss: 0.00012893764602778842\n",
      "Epoch 8876  \tTraining Loss: 0.00012564868367793614\tValidation Loss: 0.00012893637328654663\n",
      "Epoch 8877  \tTraining Loss: 0.00012564702188357657\tValidation Loss: 0.00012893505162630485\n",
      "Epoch 8878  \tTraining Loss: 0.00012564536139990322\tValidation Loss: 0.00012893378119452177\n",
      "Epoch 8879  \tTraining Loss: 0.0001256436997407481\tValidation Loss: 0.00012893250325817326\n",
      "Epoch 8880  \tTraining Loss: 0.00012564203994630915\tValidation Loss: 0.00012893117541673102\n",
      "Epoch 8881  \tTraining Loss: 0.00012564037977493616\tValidation Loss: 0.0001289299045748564\n",
      "Epoch 8882  \tTraining Loss: 0.00012563871943280705\tValidation Loss: 0.00012892858465625407\n",
      "Epoch 8883  \tTraining Loss: 0.00012563706091721965\tValidation Loss: 0.00012892731601990163\n",
      "Epoch 8884  \tTraining Loss: 0.0001256354009665457\tValidation Loss: 0.00012892603981928935\n",
      "Epoch 8885  \tTraining Loss: 0.0001256337426322521\tValidation Loss: 0.00012892471362906655\n",
      "Epoch 8886  \tTraining Loss: 0.00012563208442005553\tValidation Loss: 0.00012892344453086795\n",
      "Epoch 8887  \tTraining Loss: 0.00012563042554056253\tValidation Loss: 0.00012892212625773973\n",
      "Epoch 8888  \tTraining Loss: 0.00012562876897986505\tValidation Loss: 0.0001289208593597279\n",
      "Epoch 8889  \tTraining Loss: 0.00012562711073619092\tValidation Loss: 0.0001289195848614946\n",
      "Epoch 8890  \tTraining Loss: 0.00012562545386859354\tValidation Loss: 0.00012891826030397792\n",
      "Epoch 8891  \tTraining Loss: 0.00012562379760492484\tValidation Loss: 0.0001289169929411492\n",
      "Epoch 8892  \tTraining Loss: 0.00012562214041268008\tValidation Loss: 0.00012891571884106296\n",
      "Epoch 8893  \tTraining Loss: 0.000125620485155395\tValidation Loss: 0.00012891439495395582\n",
      "Epoch 8894  \tTraining Loss: 0.00012561882933245848\tValidation Loss: 0.00012891312847802626\n",
      "Epoch 8895  \tTraining Loss: 0.0001256171735195465\tValidation Loss: 0.00012891181273226994\n",
      "Epoch 8896  \tTraining Loss: 0.00012561551935452736\tValidation Loss: 0.0001289105485800226\n",
      "Epoch 8897  \tTraining Loss: 0.00012561386384013986\tValidation Loss: 0.0001289092767933678\n",
      "Epoch 8898  \tTraining Loss: 0.00012561221003402974\tValidation Loss: 0.00012890795484171843\n",
      "Epoch 8899  \tTraining Loss: 0.0001256105561655192\tValidation Loss: 0.00012890669027790374\n",
      "Epoch 8900  \tTraining Loss: 0.00012560890181393772\tValidation Loss: 0.00012890537628531314\n",
      "Epoch 8901  \tTraining Loss: 0.00012560724973482675\tValidation Loss: 0.0001289041074443589\n",
      "Epoch 8902  \tTraining Loss: 0.00012560559703248922\tValidation Loss: 0.00012890285598039943\n",
      "Epoch 8903  \tTraining Loss: 0.00012560394588473397\tValidation Loss: 0.0001289015463100286\n",
      "Epoch 8904  \tTraining Loss: 0.0001256022951479035\tValidation Loss: 0.00012890029135573277\n",
      "Epoch 8905  \tTraining Loss: 0.00012560064358373632\tValidation Loss: 0.0001288990282644232\n",
      "Epoch 8906  \tTraining Loss: 0.00012559899405647636\tValidation Loss: 0.00012889771447698072\n",
      "Epoch 8907  \tTraining Loss: 0.0001255973437666521\tValidation Loss: 0.00012889645793796417\n",
      "Epoch 8908  \tTraining Loss: 0.00012559569367982394\tValidation Loss: 0.0001288951515730855\n",
      "Epoch 8909  \tTraining Loss: 0.0001255940450566103\tValidation Loss: 0.0001288938968558735\n",
      "Epoch 8910  \tTraining Loss: 0.0001255923951789756\tValidation Loss: 0.00012889263425477066\n",
      "Epoch 8911  \tTraining Loss: 0.00012559074709184073\tValidation Loss: 0.0001288913211739324\n",
      "Epoch 8912  \tTraining Loss: 0.00012558909877876935\tValidation Loss: 0.00012889006567021603\n",
      "Epoch 8913  \tTraining Loss: 0.0001255874501382621\tValidation Loss: 0.00012888876040243437\n",
      "Epoch 8914  \tTraining Loss: 0.00012558580348284043\tValidation Loss: 0.00012888750698602546\n",
      "Epoch 8915  \tTraining Loss: 0.0001255841553102804\tValidation Loss: 0.00012888624572835012\n",
      "Epoch 8916  \tTraining Loss: 0.0001255825086733592\tValidation Loss: 0.00012888493397862\n",
      "Epoch 8917  \tTraining Loss: 0.00012558086232055142\tValidation Loss: 0.00012888367995443818\n",
      "Epoch 8918  \tTraining Loss: 0.0001255792151976723\tValidation Loss: 0.00012888241897660537\n",
      "Epoch 8919  \tTraining Loss: 0.00012557757016117113\tValidation Loss: 0.0001288811077943653\n",
      "Epoch 8920  \tTraining Loss: 0.00012557592425477818\tValidation Loss: 0.00012887985456688434\n",
      "Epoch 8921  \tTraining Loss: 0.00012557427865299268\tValidation Loss: 0.0001288785515238895\n",
      "Epoch 8922  \tTraining Loss: 0.00012557263441033276\tValidation Loss: 0.0001288773005882147\n",
      "Epoch 8923  \tTraining Loss: 0.00012557098896120544\tValidation Loss: 0.00012887604180679603\n",
      "Epoch 8924  \tTraining Loss: 0.0001255693453557687\tValidation Loss: 0.00012887473245147774\n",
      "Epoch 8925  \tTraining Loss: 0.000125567701414899\tValidation Loss: 0.00012887348103809038\n",
      "Epoch 8926  \tTraining Loss: 0.0001255660572536154\tValidation Loss: 0.00012887217965826622\n",
      "Epoch 8927  \tTraining Loss: 0.0001255644149654495\tValidation Loss: 0.00012887093044838517\n",
      "Epoch 8928  \tTraining Loss: 0.00012556277121198982\tValidation Loss: 0.00012886967333948442\n",
      "Epoch 8929  \tTraining Loss: 0.00012556112905368213\tValidation Loss: 0.00012886836557724732\n",
      "Epoch 8930  \tTraining Loss: 0.0001255594870585716\tValidation Loss: 0.0001288671158565999\n",
      "Epoch 8931  \tTraining Loss: 0.00012555784434969703\tValidation Loss: 0.000128865859113533\n",
      "Epoch 8932  \tTraining Loss: 0.00012555620379152578\tValidation Loss: 0.00012886455199438793\n",
      "Epoch 8933  \tTraining Loss: 0.0001255545622332478\tValidation Loss: 0.00012886330313718056\n",
      "Epoch 8934  \tTraining Loss: 0.00012555292110854783\tValidation Loss: 0.00012886200421921137\n",
      "Epoch 8935  \tTraining Loss: 0.00012555128120866947\tValidation Loss: 0.00012886075769412148\n",
      "Epoch 8936  \tTraining Loss: 0.00012554964016553812\tValidation Loss: 0.00012885950323977374\n",
      "Epoch 8937  \tTraining Loss: 0.00012554800103786973\tValidation Loss: 0.00012885819802991026\n",
      "Epoch 8938  \tTraining Loss: 0.0001255463614300852\tValidation Loss: 0.00012885695106136717\n",
      "Epoch 8939  \tTraining Loss: 0.00012554472174563348\tValidation Loss: 0.0001288556538745666\n",
      "Epoch 8940  \tTraining Loss: 0.00012554308378525688\tValidation Loss: 0.00012885440913782103\n",
      "Epoch 8941  \tTraining Loss: 0.00012554144443027956\tValidation Loss: 0.00012885315641362663\n",
      "Epoch 8942  \tTraining Loss: 0.00012553980674984245\tValidation Loss: 0.00012885185285018777\n",
      "Epoch 8943  \tTraining Loss: 0.00012553816907294707\tValidation Loss: 0.0001288506076239957\n",
      "Epoch 8944  \tTraining Loss: 0.0001255365308376557\tValidation Loss: 0.00012884931208163403\n",
      "Epoch 8945  \tTraining Loss: 0.00012553489480406177\tValidation Loss: 0.00012884806908509584\n",
      "Epoch 8946  \tTraining Loss: 0.000125533257135268\tValidation Loss: 0.0001288468180661174\n",
      "Epoch 8947  \tTraining Loss: 0.00012553162090719213\tValidation Loss: 0.00012884551613846984\n",
      "Epoch 8948  \tTraining Loss: 0.00012552998515103625\tValidation Loss: 0.00012884427265354596\n",
      "Epoch 8949  \tTraining Loss: 0.0001255283485211024\tValidation Loss: 0.00012884302202803826\n",
      "Epoch 8950  \tTraining Loss: 0.00012552671389657192\tValidation Loss: 0.00012884172077176593\n",
      "Epoch 8951  \tTraining Loss: 0.00012552507856230404\tValidation Loss: 0.0001288404781780967\n",
      "Epoch 8952  \tTraining Loss: 0.0001255234433716325\tValidation Loss: 0.0001288391851738377\n",
      "Epoch 8953  \tTraining Loss: 0.00012552180968744675\tValidation Loss: 0.00012883794493748395\n",
      "Epoch 8954  \tTraining Loss: 0.00012552017471382121\tValidation Loss: 0.0001288366966453189\n",
      "Epoch 8955  \tTraining Loss: 0.00012551854152398348\tValidation Loss: 0.0001288353973384973\n",
      "Epoch 8956  \tTraining Loss: 0.00012551690811729213\tValidation Loss: 0.0001288341566708907\n",
      "Epoch 8957  \tTraining Loss: 0.00012551527437105773\tValidation Loss: 0.0001288328654321207\n",
      "Epoch 8958  \tTraining Loss: 0.0001255136426037167\tValidation Loss: 0.00012883162701639102\n",
      "Epoch 8959  \tTraining Loss: 0.00012551200930913329\tValidation Loss: 0.00012883038048420715\n",
      "Epoch 8960  \tTraining Loss: 0.00012551037757070892\tValidation Loss: 0.00012882908285132062\n",
      "Epoch 8961  \tTraining Loss: 0.00012550874607259805\tValidation Loss: 0.00012882784395193287\n",
      "Epoch 8962  \tTraining Loss: 0.000125507113812994\tValidation Loss: 0.00012882659781953546\n",
      "Epoch 8963  \tTraining Loss: 0.00012550548367881736\tValidation Loss: 0.00012882530086388724\n",
      "Epoch 8964  \tTraining Loss: 0.00012550385259427983\tValidation Loss: 0.00012882406286088586\n",
      "Epoch 8965  \tTraining Loss: 0.0001255022218932779\tValidation Loss: 0.00012882277418429974\n",
      "Epoch 8966  \tTraining Loss: 0.0001255005924542783\tValidation Loss: 0.00012882153854752964\n",
      "Epoch 8967  \tTraining Loss: 0.00012549896184468285\tValidation Loss: 0.00012882029475706083\n",
      "Epoch 8968  \tTraining Loss: 0.00012549733314584305\tValidation Loss: 0.00012881899975719516\n",
      "Epoch 8969  \tTraining Loss: 0.00012549570397582584\tValidation Loss: 0.0001288177636855043\n",
      "Epoch 8970  \tTraining Loss: 0.00012549407472003677\tValidation Loss: 0.00012881647677860647\n",
      "Epoch 8971  \tTraining Loss: 0.00012549244718480394\tValidation Loss: 0.0001288152429661864\n",
      "Epoch 8972  \tTraining Loss: 0.00012549081824813466\tValidation Loss: 0.0001288140009387345\n",
      "Epoch 8973  \tTraining Loss: 0.00012548919100162357\tValidation Loss: 0.0001288127076152388\n",
      "Epoch 8974  \tTraining Loss: 0.00012548756372712897\tValidation Loss: 0.00012881147331391359\n",
      "Epoch 8975  \tTraining Loss: 0.00012548593592568195\tValidation Loss: 0.00012881018807650062\n",
      "Epoch 8976  \tTraining Loss: 0.0001254843102820904\tValidation Loss: 0.00012880895602772517\n",
      "Epoch 8977  \tTraining Loss: 0.00012548268301676115\tValidation Loss: 0.0001288077157265106\n",
      "Epoch 8978  \tTraining Loss: 0.00012548105722787954\tValidation Loss: 0.00012880642405754258\n",
      "Epoch 8979  \tTraining Loss: 0.00012547943183931665\tValidation Loss: 0.0001288051915149038\n",
      "Epoch 8980  \tTraining Loss: 0.00012547780560416105\tValidation Loss: 0.00012880395161042518\n",
      "Epoch 8981  \tTraining Loss: 0.00012547618142335707\tValidation Loss: 0.00012880266061800403\n",
      "Epoch 8982  \tTraining Loss: 0.00012547455643474595\tValidation Loss: 0.00012880142897226196\n",
      "Epoch 8983  \tTraining Loss: 0.00012547293168777742\tValidation Loss: 0.00012880014629282178\n",
      "Epoch 8984  \tTraining Loss: 0.00012547130833719117\tValidation Loss: 0.00012879891702268216\n",
      "Epoch 8985  \tTraining Loss: 0.000125469683743945\tValidation Loss: 0.00012879767946391273\n",
      "Epoch 8986  \tTraining Loss: 0.0001254680610032132\tValidation Loss: 0.00012879639042870307\n",
      "Epoch 8987  \tTraining Loss: 0.00012546643790820123\tValidation Loss: 0.0001287951607150117\n",
      "Epoch 8988  \tTraining Loss: 0.00012546481461110423\tValidation Loss: 0.00012879387980474169\n",
      "Epoch 8989  \tTraining Loss: 0.00012546319314353833\tValidation Loss: 0.00012879265235820987\n",
      "Epoch 8990  \tTraining Loss: 0.00012546157021523765\tValidation Loss: 0.00012879141656089708\n",
      "Epoch 8991  \tTraining Loss: 0.00012545994893138846\tValidation Loss: 0.00012879012919969466\n",
      "Epoch 8992  \tTraining Loss: 0.0001254583277113814\tValidation Loss: 0.00012878890125388854\n",
      "Epoch 8993  \tTraining Loss: 0.00012545670587313414\tValidation Loss: 0.00012878762200967423\n",
      "Epoch 8994  \tTraining Loss: 0.00012545508627682258\tValidation Loss: 0.00012878639632331662\n",
      "Epoch 8995  \tTraining Loss: 0.00012545346501197658\tValidation Loss: 0.00012878516224811796\n",
      "Epoch 8996  \tTraining Loss: 0.00012545184519019187\tValidation Loss: 0.0001287838765366727\n",
      "Epoch 8997  \tTraining Loss: 0.00012545022583591463\tValidation Loss: 0.00012878265034479755\n",
      "Epoch 8998  \tTraining Loss: 0.00012544860559658992\tValidation Loss: 0.00012878141666011232\n",
      "Epoch 8999  \tTraining Loss: 0.00012544698738624775\tValidation Loss: 0.0001287801316204265\n",
      "Epoch 9000  \tTraining Loss: 0.00012544536841919385\tValidation Loss: 0.000128778906321322\n",
      "Epoch 9001  \tTraining Loss: 0.00012544374964315345\tValidation Loss: 0.00012877762962525515\n",
      "Epoch 9002  \tTraining Loss: 0.00012544213230710307\tValidation Loss: 0.00012877640670794606\n",
      "Epoch 9003  \tTraining Loss: 0.00012544051370190913\tValidation Loss: 0.00012877517536486187\n",
      "Epoch 9004  \tTraining Loss: 0.00012543889693589097\tValidation Loss: 0.00012877389227608575\n",
      "Epoch 9005  \tTraining Loss: 0.00012543727984268535\tValidation Loss: 0.0001287726689022538\n",
      "Epoch 9006  \tTraining Loss: 0.00012543566252075039\tValidation Loss: 0.0001287713939676619\n",
      "Epoch 9007  \tTraining Loss: 0.00012543404704805147\tValidation Loss: 0.0001287701728663601\n",
      "Epoch 9008  \tTraining Loss: 0.0001254324301001274\tValidation Loss: 0.00012876894327667386\n",
      "Epoch 9009  \tTraining Loss: 0.00012543081479517696\tValidation Loss: 0.00012876766185340727\n",
      "Epoch 9010  \tTraining Loss: 0.00012542919955744257\tValidation Loss: 0.0001287664402391716\n",
      "Epoch 9011  \tTraining Loss: 0.000125427583698516\tValidation Loss: 0.00012876516696173844\n",
      "Epoch 9012  \tTraining Loss: 0.00012542597007762085\tValidation Loss: 0.00012876394761196806\n",
      "Epoch 9013  \tTraining Loss: 0.00012542435478554408\tValidation Loss: 0.00012876271973539708\n",
      "Epoch 9014  \tTraining Loss: 0.00012542274094676443\tValidation Loss: 0.00012876143995252463\n",
      "Epoch 9015  \tTraining Loss: 0.000125421127555438\tValidation Loss: 0.00012876022008312418\n",
      "Epoch 9016  \tTraining Loss: 0.00012541951328438284\tValidation Loss: 0.00012875899258853006\n",
      "Epoch 9017  \tTraining Loss: 0.0001254179010602127\tValidation Loss: 0.00012875771347044948\n",
      "Epoch 9018  \tTraining Loss: 0.00012541628804388168\tValidation Loss: 0.00012875649448763367\n",
      "Epoch 9019  \tTraining Loss: 0.00012541467525437386\tValidation Loss: 0.00012875522374250327\n",
      "Epoch 9020  \tTraining Loss: 0.0001254130638623298\tValidation Loss: 0.00012875400714650119\n",
      "Epoch 9021  \tTraining Loss: 0.0001254114512178644\tValidation Loss: 0.00012875278198635158\n",
      "Epoch 9022  \tTraining Loss: 0.00012540984044196281\tValidation Loss: 0.0001287515048100118\n",
      "Epoch 9023  \tTraining Loss: 0.00012540822928050328\tValidation Loss: 0.0001287502877429817\n",
      "Epoch 9024  \tTraining Loss: 0.00012540661794902894\tValidation Loss: 0.00012874901874909063\n",
      "Epoch 9025  \tTraining Loss: 0.00012540500840760733\tValidation Loss: 0.00012874778397507487\n",
      "Epoch 9026  \tTraining Loss: 0.0001254033975550655\tValidation Loss: 0.00012874655634629271\n",
      "Epoch 9027  \tTraining Loss: 0.00012540178839075267\tValidation Loss: 0.00012874527818626795\n",
      "Epoch 9028  \tTraining Loss: 0.00012540017922040708\tValidation Loss: 0.0001287440609198771\n",
      "Epoch 9029  \tTraining Loss: 0.00012539856949501983\tValidation Loss: 0.0001287427919516999\n",
      "Epoch 9030  \tTraining Loss: 0.00012539696194401086\tValidation Loss: 0.00012874157752470477\n",
      "Epoch 9031  \tTraining Loss: 0.00012539535275085824\tValidation Loss: 0.00012874035455472414\n",
      "Epoch 9032  \tTraining Loss: 0.00012539374504094217\tValidation Loss: 0.00012873907950716753\n",
      "Epoch 9033  \tTraining Loss: 0.00012539213771688414\tValidation Loss: 0.00012873786485901692\n",
      "Epoch 9034  \tTraining Loss: 0.000125390529540974\tValidation Loss: 0.00012873664248533775\n",
      "Epoch 9035  \tTraining Loss: 0.00012538892344237493\tValidation Loss: 0.00012873536826300534\n",
      "Epoch 9036  \tTraining Loss: 0.00012538731648954486\tValidation Loss: 0.00012873415462766278\n",
      "Epoch 9037  \tTraining Loss: 0.0001253857098218992\tValidation Loss: 0.00012873288886998898\n",
      "Epoch 9038  \tTraining Loss: 0.00012538410449199785\tValidation Loss: 0.00012873167768480688\n",
      "Epoch 9039  \tTraining Loss: 0.00012538249793673953\tValidation Loss: 0.00012873045780147888\n",
      "Epoch 9040  \tTraining Loss: 0.00012538089327982679\tValidation Loss: 0.00012872918565643725\n",
      "Epoch 9041  \tTraining Loss: 0.00012537928817651445\tValidation Loss: 0.00012872797406016052\n",
      "Epoch 9042  \tTraining Loss: 0.0001253776829599378\tValidation Loss: 0.00012872671016716099\n",
      "Epoch 9043  \tTraining Loss: 0.00012537607946936638\tValidation Loss: 0.00012872550089604187\n",
      "Epoch 9044  \tTraining Loss: 0.00012537447455799274\tValidation Loss: 0.0001287242828588008\n",
      "Epoch 9045  \tTraining Loss: 0.00012537287135825514\tValidation Loss: 0.00012872301246781435\n",
      "Epoch 9046  \tTraining Loss: 0.0001253712680871762\tValidation Loss: 0.00012872180271815225\n",
      "Epoch 9047  \tTraining Loss: 0.00012536966432910072\tValidation Loss: 0.00012872054056567117\n",
      "Epoch 9048  \tTraining Loss: 0.0001253680626676566\tValidation Loss: 0.00012871933312872086\n",
      "Epoch 9049  \tTraining Loss: 0.00012536645939870378\tValidation Loss: 0.0001287181168847715\n",
      "Epoch 9050  \tTraining Loss: 0.00012536485765961985\tValidation Loss: 0.00012871684821210357\n",
      "Epoch 9051  \tTraining Loss: 0.000125363256213214\tValidation Loss: 0.00012871564028483454\n",
      "Epoch 9052  \tTraining Loss: 0.00012536165395729245\tValidation Loss: 0.00012871442446193063\n",
      "Epoch 9053  \tTraining Loss: 0.00012536005383012857\tValidation Loss: 0.00012871315649520135\n",
      "Epoch 9054  \tTraining Loss: 0.00012535845274507776\tValidation Loss: 0.00012871194949658872\n",
      "Epoch 9055  \tTraining Loss: 0.00012535685204716058\tValidation Loss: 0.00012871068999218877\n",
      "Epoch 9056  \tTraining Loss: 0.00012535525258020226\tValidation Loss: 0.00012870948542441384\n",
      "Epoch 9057  \tTraining Loss: 0.00012535365193732282\tValidation Loss: 0.0001287082720090572\n",
      "Epoch 9058  \tTraining Loss: 0.00012535205325022602\tValidation Loss: 0.00012870700605198388\n",
      "Epoch 9059  \tTraining Loss: 0.0001253504540008775\tValidation Loss: 0.00012870580103551045\n",
      "Epoch 9060  \tTraining Loss: 0.0001253488547522085\tValidation Loss: 0.00012870454334560263\n",
      "Epoch 9061  \tTraining Loss: 0.0001253472571110969\tValidation Loss: 0.00012870334064652217\n",
      "Epoch 9062  \tTraining Loss: 0.00012534565810450529\tValidation Loss: 0.00012870212903481329\n",
      "Epoch 9063  \tTraining Loss: 0.00012534406087264782\tValidation Loss: 0.0001287008647912318\n",
      "Epoch 9064  \tTraining Loss: 0.000125342463442281\tValidation Loss: 0.00012869966158224704\n",
      "Epoch 9065  \tTraining Loss: 0.0001253408656502006\tValidation Loss: 0.00012869840559439214\n",
      "Epoch 9066  \tTraining Loss: 0.00012533926982503856\tValidation Loss: 0.00012869720469171339\n",
      "Epoch 9067  \tTraining Loss: 0.0001253376724533483\tValidation Loss: 0.00012869599483579863\n",
      "Epoch 9068  \tTraining Loss: 0.00012533607668031323\tValidation Loss: 0.00012869473227321637\n",
      "Epoch 9069  \tTraining Loss: 0.0001253344810614752\tValidation Loss: 0.0001286935308496402\n",
      "Epoch 9070  \tTraining Loss: 0.000125332884728876\tValidation Loss: 0.0001286922765476173\n",
      "Epoch 9071  \tTraining Loss: 0.00012533129071287455\tValidation Loss: 0.00012869107742923756\n",
      "Epoch 9072  \tTraining Loss: 0.0001253296949741815\tValidation Loss: 0.00012868986931923736\n",
      "Epoch 9073  \tTraining Loss: 0.00012532810066230089\tValidation Loss: 0.00012868860842899866\n",
      "Epoch 9074  \tTraining Loss: 0.00012532650684866348\tValidation Loss: 0.00012868740878316666\n",
      "Epoch 9075  \tTraining Loss: 0.00012532491211738873\tValidation Loss: 0.0001286862010644243\n",
      "Epoch 9076  \tTraining Loss: 0.00012532331941852136\tValidation Loss: 0.0001286849408525742\n",
      "Epoch 9077  \tTraining Loss: 0.00012532172595385106\tValidation Loss: 0.0001286837421088804\n",
      "Epoch 9078  \tTraining Loss: 0.00012532013268290636\tValidation Loss: 0.00012868249038432375\n",
      "Epoch 9079  \tTraining Loss: 0.000125318540829753\tValidation Loss: 0.0001286812940655737\n",
      "Epoch 9080  \tTraining Loss: 0.00012531694770211382\tValidation Loss: 0.00012868008871483316\n",
      "Epoch 9081  \tTraining Loss: 0.00012531535644369815\tValidation Loss: 0.00012867883047099922\n",
      "Epoch 9082  \tTraining Loss: 0.00012531376479563977\tValidation Loss: 0.00012867763366751988\n",
      "Epoch 9083  \tTraining Loss: 0.00012531217297439938\tValidation Loss: 0.00012867638371515992\n",
      "Epoch 9084  \tTraining Loss: 0.0001253105829279989\tValidation Loss: 0.00012867518922325279\n",
      "Epoch 9085  \tTraining Loss: 0.0001253089914273442\tValidation Loss: 0.00012867398563440227\n",
      "Epoch 9086  \tTraining Loss: 0.00012530740162472394\tValidation Loss: 0.00012867272906245584\n",
      "Epoch 9087  \tTraining Loss: 0.00012530581177654012\tValidation Loss: 0.000128671534025495\n",
      "Epoch 9088  \tTraining Loss: 0.00012530422141251917\tValidation Loss: 0.00012867028573427836\n",
      "Epoch 9089  \tTraining Loss: 0.0001253026331629421\tValidation Loss: 0.00012866909299856234\n",
      "Epoch 9090  \tTraining Loss: 0.00012530104328795137\tValidation Loss: 0.00012866789112554836\n",
      "Epoch 9091  \tTraining Loss: 0.00012529945494488945\tValidation Loss: 0.00012866663619491458\n",
      "Epoch 9092  \tTraining Loss: 0.00012529786688907711\tValidation Loss: 0.00012866544290441294\n",
      "Epoch 9093  \tTraining Loss: 0.00012529627801719545\tValidation Loss: 0.0001286642414011472\n",
      "Epoch 9094  \tTraining Loss: 0.00012529469128713227\tValidation Loss: 0.00012866298712934805\n",
      "Epoch 9095  \tTraining Loss: 0.0001252931035717016\tValidation Loss: 0.00012866179472240376\n",
      "Epoch 9096  \tTraining Loss: 0.00012529151626804576\tValidation Loss: 0.00012866054895915573\n",
      "Epoch 9097  \tTraining Loss: 0.00012528993015964375\tValidation Loss: 0.0001286593589744442\n",
      "Epoch 9098  \tTraining Loss: 0.00012528834288420582\tValidation Loss: 0.00012865815981264763\n",
      "Epoch 9099  \tTraining Loss: 0.00012528675759368736\tValidation Loss: 0.00012865690748107772\n",
      "Epoch 9100  \tTraining Loss: 0.000125285171681517\tValidation Loss: 0.00012865571698661746\n",
      "Epoch 9101  \tTraining Loss: 0.0001252835858267796\tValidation Loss: 0.00012865447296787444\n",
      "Epoch 9102  \tTraining Loss: 0.0001252820015116878\tValidation Loss: 0.00012865328478288602\n",
      "Epoch 9103  \tTraining Loss: 0.00012528041585611364\tValidation Loss: 0.00012865208735616236\n",
      "Epoch 9104  \tTraining Loss: 0.00012527883202075328\tValidation Loss: 0.00012865083667000336\n",
      "Epoch 9105  \tTraining Loss: 0.00012527724789491206\tValidation Loss: 0.00012864964791621606\n",
      "Epoch 9106  \tTraining Loss: 0.00012527566349684584\tValidation Loss: 0.00012864840553297583\n",
      "Epoch 9107  \tTraining Loss: 0.00012527408096496968\tValidation Loss: 0.00012864721907916582\n",
      "Epoch 9108  \tTraining Loss: 0.0001252724969279779\tValidation Loss: 0.00012864602334354983\n",
      "Epoch 9109  \tTraining Loss: 0.0001252709145517429\tValidation Loss: 0.0001286447742742684\n",
      "Epoch 9110  \tTraining Loss: 0.0001252693322045891\tValidation Loss: 0.0001286435872431019\n",
      "Epoch 9111  \tTraining Loss: 0.00012526774926645528\tValidation Loss: 0.00012864234648317943\n",
      "Epoch 9112  \tTraining Loss: 0.00012526616851082266\tValidation Loss: 0.0001286411617525131\n",
      "Epoch 9113  \tTraining Loss: 0.00012526458609060174\tValidation Loss: 0.00012863996770218794\n",
      "Epoch 9114  \tTraining Loss: 0.00012526300517616892\tValidation Loss: 0.00012863872024517306\n",
      "Epoch 9115  \tTraining Loss: 0.00012526142460120225\tValidation Loss: 0.00012863753493301108\n",
      "Epoch 9116  \tTraining Loss: 0.00012525984317876782\tValidation Loss: 0.00012863634123316616\n",
      "Epoch 9117  \tTraining Loss: 0.00012525826387942355\tValidation Loss: 0.00012863509441802566\n",
      "Epoch 9118  \tTraining Loss: 0.00012525668363201026\tValidation Loss: 0.00012863390997362826\n",
      "Epoch 9119  \tTraining Loss: 0.00012525510375842273\tValidation Loss: 0.00012863267169909597\n",
      "Epoch 9120  \tTraining Loss: 0.00012525352511114202\tValidation Loss: 0.00012863148967828206\n",
      "Epoch 9121  \tTraining Loss: 0.00012525194527618854\tValidation Loss: 0.0001286302982983611\n",
      "Epoch 9122  \tTraining Loss: 0.00012525036741834554\tValidation Loss: 0.00012862905339979867\n",
      "Epoch 9123  \tTraining Loss: 0.00012524878895436536\tValidation Loss: 0.00012862787084403615\n",
      "Epoch 9124  \tTraining Loss: 0.00012524721053166292\tValidation Loss: 0.00012862663428968315\n",
      "Epoch 9125  \tTraining Loss: 0.0001252456336578837\tValidation Loss: 0.00012862545404473407\n",
      "Epoch 9126  \tTraining Loss: 0.0001252440554338886\tValidation Loss: 0.00012862426437601273\n",
      "Epoch 9127  \tTraining Loss: 0.00012524247903317228\tValidation Loss: 0.00012862302109899247\n",
      "Epoch 9128  \tTraining Loss: 0.00012524090233574666\tValidation Loss: 0.00012862184026061802\n",
      "Epoch 9129  \tTraining Loss: 0.0001252393253716939\tValidation Loss: 0.00012862060531832883\n",
      "Epoch 9130  \tTraining Loss: 0.00012523775026137177\tValidation Loss: 0.00012861942678172466\n",
      "Epoch 9131  \tTraining Loss: 0.00012523617364708324\tValidation Loss: 0.0001286182387813287\n",
      "Epoch 9132  \tTraining Loss: 0.00012523459870747612\tValidation Loss: 0.00012861699709844284\n",
      "Epoch 9133  \tTraining Loss: 0.00012523302376900804\tValidation Loss: 0.00012861581796054375\n",
      "Epoch 9134  \tTraining Loss: 0.00012523144826688296\tValidation Loss: 0.0001286145846192801\n",
      "Epoch 9135  \tTraining Loss: 0.00012522987491308778\tValidation Loss: 0.00012861340778413192\n",
      "Epoch 9136  \tTraining Loss: 0.00012522829990672306\tValidation Loss: 0.00012861222144738607\n",
      "Epoch 9137  \tTraining Loss: 0.00012522672643091971\tValidation Loss: 0.0001286109813551592\n",
      "Epoch 9138  \tTraining Loss: 0.00012522515324494863\tValidation Loss: 0.00012860980391524453\n",
      "Epoch 9139  \tTraining Loss: 0.00012522357923111304\tValidation Loss: 0.00012860861791298806\n",
      "Epoch 9140  \tTraining Loss: 0.00012522200737245898\tValidation Loss: 0.00012860737844886953\n",
      "Epoch 9141  \tTraining Loss: 0.00012522043450140114\tValidation Loss: 0.00012860620186405213\n",
      "Epoch 9142  \tTraining Loss: 0.00012521886206788154\tValidation Loss: 0.0001286049709746162\n",
      "Epoch 9143  \tTraining Loss: 0.00012521729079014747\tValidation Loss: 0.00012860379681678248\n",
      "Epoch 9144  \tTraining Loss: 0.00012521571835495796\tValidation Loss: 0.0001286026131180351\n",
      "Epoch 9145  \tTraining Loss: 0.00012521414793979272\tValidation Loss: 0.00012860137555195056\n",
      "Epoch 9146  \tTraining Loss: 0.00012521257683257574\tValidation Loss: 0.0001286002008369833\n",
      "Epoch 9147  \tTraining Loss: 0.0001252110058519507\tValidation Loss: 0.00012859897164843552\n",
      "Epoch 9148  \tTraining Loss: 0.00012520943632812788\tValidation Loss: 0.00012859779924759694\n",
      "Epoch 9149  \tTraining Loss: 0.00012520786549511984\tValidation Loss: 0.00012859661744418243\n",
      "Epoch 9150  \tTraining Loss: 0.0001252062965391001\tValidation Loss: 0.0001285953819472412\n",
      "Epoch 9151  \tTraining Loss: 0.00012520472717887003\tValidation Loss: 0.00012859420942045116\n",
      "Epoch 9152  \tTraining Loss: 0.0001252031576589235\tValidation Loss: 0.00012859298228315699\n",
      "Epoch 9153  \tTraining Loss: 0.00012520158987900209\tValidation Loss: 0.00012859181205493683\n",
      "Epoch 9154  \tTraining Loss: 0.00012520002064695055\tValidation Loss: 0.00012859063214959714\n",
      "Epoch 9155  \tTraining Loss: 0.00012519845315407056\tValidation Loss: 0.00012858939821298125\n",
      "Epoch 9156  \tTraining Loss: 0.0001251968855332697\tValidation Loss: 0.00012858822735206107\n",
      "Epoch 9157  \tTraining Loss: 0.00012519531747728896\tValidation Loss: 0.0001285870017824313\n",
      "Epoch 9158  \tTraining Loss: 0.00012519375143437995\tValidation Loss: 0.00012858583322139264\n",
      "Epoch 9159  \tTraining Loss: 0.0001251921838015276\tValidation Loss: 0.00012858465494648312\n",
      "Epoch 9160  \tTraining Loss: 0.00012519061777448813\tValidation Loss: 0.00012858342256749988\n",
      "Epoch 9161  \tTraining Loss: 0.0001251890518866991\tValidation Loss: 0.0001285822533707278\n",
      "Epoch 9162  \tTraining Loss: 0.00012518748529719546\tValidation Loss: 0.00012858102936723071\n",
      "Epoch 9163  \tTraining Loss: 0.00012518592098506806\tValidation Loss: 0.00012857986247218818\n",
      "Epoch 9164  \tTraining Loss: 0.00012518435494957974\tValidation Loss: 0.00012857868582648356\n",
      "Epoch 9165  \tTraining Loss: 0.00012518279039069467\tValidation Loss: 0.0001285774550037511\n",
      "Epoch 9166  \tTraining Loss: 0.00012518122622986414\tValidation Loss: 0.00012857628746981307\n",
      "Epoch 9167  \tTraining Loss: 0.0001251796611806195\tValidation Loss: 0.00012857511113402431\n",
      "Epoch 9168  \tTraining Loss: 0.00012517809824242427\tValidation Loss: 0.00012857388091738485\n",
      "Epoch 9169  \tTraining Loss: 0.00012517653438028886\tValidation Loss: 0.00012857271421692814\n",
      "Epoch 9170  \tTraining Loss: 0.0001251749708671473\tValidation Loss: 0.00012857149261048294\n",
      "Epoch 9171  \tTraining Loss: 0.00012517340858949442\tValidation Loss: 0.0001285703283366439\n",
      "Epoch 9172  \tTraining Loss: 0.00012517184510831575\tValidation Loss: 0.0001285691542743273\n",
      "Epoch 9173  \tTraining Loss: 0.00012517028361764616\tValidation Loss: 0.0001285679259232016\n",
      "Epoch 9174  \tTraining Loss: 0.00012516872149420002\tValidation Loss: 0.00012856676105837234\n",
      "Epoch 9175  \tTraining Loss: 0.00012516715943796598\tValidation Loss: 0.00012856554111923463\n",
      "Epoch 9176  \tTraining Loss: 0.00012516559888912232\tValidation Loss: 0.000128564378567781\n",
      "Epoch 9177  \tTraining Loss: 0.00012516403699960646\tValidation Loss: 0.00012856320616396732\n",
      "Epoch 9178  \tTraining Loss: 0.0001251624770064515\tValidation Loss: 0.0001285618943956299\n",
      "Epoch 9179  \tTraining Loss: 0.0001251609169240542\tValidation Loss: 0.00012856081023864536\n",
      "Epoch 9180  \tTraining Loss: 0.0001251593560928156\tValidation Loss: 0.00012855960730240722\n",
      "Epoch 9181  \tTraining Loss: 0.0001251577972063663\tValidation Loss: 0.0001285584504060713\n",
      "Epoch 9182  \tTraining Loss: 0.00012515623689323064\tValidation Loss: 0.00012855728167038337\n",
      "Epoch 9183  \tTraining Loss: 0.0001251546784048558\tValidation Loss: 0.00012855605754772225\n",
      "Epoch 9184  \tTraining Loss: 0.00012515311963925014\tValidation Loss: 0.00012855489655011878\n",
      "Epoch 9185  \tTraining Loss: 0.00012515156060207924\tValidation Loss: 0.0001285536799167218\n",
      "Epoch 9186  \tTraining Loss: 0.00012515000339291112\tValidation Loss: 0.0001285525206360549\n",
      "Epoch 9187  \tTraining Loss: 0.0001251484446794494\tValidation Loss: 0.0001285513512851824\n",
      "Epoch 9188  \tTraining Loss: 0.00012514688768706646\tValidation Loss: 0.0001285501273236804\n",
      "Epoch 9189  \tTraining Loss: 0.000125145330607709\tValidation Loss: 0.0001285489671076834\n",
      "Epoch 9190  \tTraining Loss: 0.00012514377305669733\tValidation Loss: 0.00012854775148967885\n",
      "Epoch 9191  \tTraining Loss: 0.00012514221754057547\tValidation Loss: 0.00012854659354505506\n",
      "Epoch 9192  \tTraining Loss: 0.00012514066041577245\tValidation Loss: 0.00012854542563322356\n",
      "Epoch 9193  \tTraining Loss: 0.00012513910490346815\tValidation Loss: 0.0001285442031265441\n",
      "Epoch 9194  \tTraining Loss: 0.00012513754951909285\tValidation Loss: 0.00012854304453091683\n",
      "Epoch 9195  \tTraining Loss: 0.00012513599344646709\tValidation Loss: 0.00012854183046996843\n",
      "Epoch 9196  \tTraining Loss: 0.000125134439625303\tValidation Loss: 0.00012854067420600492\n",
      "Epoch 9197  \tTraining Loss: 0.0001251328840861444\tValidation Loss: 0.0001285395079510628\n",
      "Epoch 9198  \tTraining Loss: 0.00012513133005160495\tValidation Loss: 0.0001285382870351365\n",
      "Epoch 9199  \tTraining Loss: 0.00012512977636032732\tValidation Loss: 0.00012853713014165995\n",
      "Epoch 9200  \tTraining Loss: 0.00012512822180031348\tValidation Loss: 0.00012853596421580384\n",
      "Epoch 9201  \tTraining Loss: 0.00012512666939239737\tValidation Loss: 0.0001285347439270703\n",
      "Epoch 9202  \tTraining Loss: 0.0001251251159793202\tValidation Loss: 0.0001285335878885431\n",
      "Epoch 9203  \tTraining Loss: 0.0001251235629972886\tValidation Loss: 0.0001285323762843676\n",
      "Epoch 9204  \tTraining Loss: 0.00012512201115753515\tValidation Loss: 0.00012853122270109184\n",
      "Epoch 9205  \tTraining Loss: 0.00012512045815349167\tValidation Loss: 0.00012853005908579782\n",
      "Epoch 9206  \tTraining Loss: 0.00012511890719984461\tValidation Loss: 0.00012852884069358323\n",
      "Epoch 9207  \tTraining Loss: 0.00012511735549433043\tValidation Loss: 0.0001285276865188268\n",
      "Epoch 9208  \tTraining Loss: 0.00012511580397557115\tValidation Loss: 0.0001285264766064639\n",
      "Epoch 9209  \tTraining Loss: 0.000125114253833901\tValidation Loss: 0.0001285253247683077\n",
      "Epoch 9210  \tTraining Loss: 0.0001251127024094313\tValidation Loss: 0.0001285241628316499\n",
      "Epoch 9211  \tTraining Loss: 0.00012511115292473837\tValidation Loss: 0.00012852294602581685\n",
      "Epoch 9212  \tTraining Loss: 0.00012510960291098542\tValidation Loss: 0.00012852179353174144\n",
      "Epoch 9213  \tTraining Loss: 0.00012510805286241843\tValidation Loss: 0.0001285205851929788\n",
      "Epoch 9214  \tTraining Loss: 0.0001251065044097635\tValidation Loss: 0.00012851943502332\n",
      "Epoch 9215  \tTraining Loss: 0.00012510495459802144\tValidation Loss: 0.0001285181897479556\n",
      "Epoch 9216  \tTraining Loss: 0.00012510340686351364\tValidation Loss: 0.0001285170535692873\n",
      "Epoch 9217  \tTraining Loss: 0.00012510185828668263\tValidation Loss: 0.00012851591814260608\n",
      "Epoch 9218  \tTraining Loss: 0.00012510030975268957\tValidation Loss: 0.0001285147153589997\n",
      "Epoch 9219  \tTraining Loss: 0.00012509876292484122\tValidation Loss: 0.0001285135688894329\n",
      "Epoch 9220  \tTraining Loss: 0.00012509721464562843\tValidation Loss: 0.00012851241130721374\n",
      "Epoch 9221  \tTraining Loss: 0.00012509566818869882\tValidation Loss: 0.0001285111981517153\n",
      "Epoch 9222  \tTraining Loss: 0.00012509412145310023\tValidation Loss: 0.00012851004915591304\n",
      "Epoch 9223  \tTraining Loss: 0.00012509257444395542\tValidation Loss: 0.00012850884388769808\n",
      "Epoch 9224  \tTraining Loss: 0.00012509102925573894\tValidation Loss: 0.00012850769684501738\n",
      "Epoch 9225  \tTraining Loss: 0.0001250894825584743\tValidation Loss: 0.00012850653950094947\n",
      "Epoch 9226  \tTraining Loss: 0.00012508793759769454\tValidation Loss: 0.00012850532701413547\n",
      "Epoch 9227  \tTraining Loss: 0.00012508639251883102\tValidation Loss: 0.00012850417913223678\n",
      "Epoch 9228  \tTraining Loss: 0.0001250848469990463\tValidation Loss: 0.00012850297509370937\n",
      "Epoch 9229  \tTraining Loss: 0.00012508330347197035\tValidation Loss: 0.000128501829523367\n",
      "Epoch 9230  \tTraining Loss: 0.0001250817583490667\tValidation Loss: 0.00012850067370323706\n",
      "Epoch 9231  \tTraining Loss: 0.00012508021487333065\tValidation Loss: 0.00012849946272261662\n",
      "Epoch 9232  \tTraining Loss: 0.00012507867145645905\tValidation Loss: 0.00012849831649119182\n",
      "Epoch 9233  \tTraining Loss: 0.00012507712742033727\tValidation Loss: 0.00012849711171113824\n",
      "Epoch 9234  \tTraining Loss: 0.00012507558555497803\tValidation Loss: 0.00012849596204637579\n",
      "Epoch 9235  \tTraining Loss: 0.00012507404200376372\tValidation Loss: 0.00012849480202570601\n",
      "Epoch 9236  \tTraining Loss: 0.00012507250001122968\tValidation Loss: 0.0001284935867566029\n",
      "Epoch 9237  \tTraining Loss: 0.00012507095825415947\tValidation Loss: 0.00012849243642580228\n",
      "Epoch 9238  \tTraining Loss: 0.0001250694157008651\tValidation Loss: 0.0001284912296891297\n",
      "Epoch 9239  \tTraining Loss: 0.0001250678754941075\tValidation Loss: 0.00012849008171228606\n",
      "Epoch 9240  \tTraining Loss: 0.00012506633351254482\tValidation Loss: 0.00012848892334025257\n",
      "Epoch 9241  \tTraining Loss: 0.00012506479300326392\tValidation Loss: 0.0001284877096440434\n",
      "Epoch 9242  \tTraining Loss: 0.000125063252902181\tValidation Loss: 0.00012848656099005799\n",
      "Epoch 9243  \tTraining Loss: 0.00012506171189034468\tValidation Loss: 0.0001284854029158521\n",
      "Epoch 9244  \tTraining Loss: 0.00012506017301280485\tValidation Loss: 0.00012848418982087684\n",
      "Epoch 9245  \tTraining Loss: 0.00012505863316591832\tValidation Loss: 0.00012848304199726402\n",
      "Epoch 9246  \tTraining Loss: 0.00012505709371417824\tValidation Loss: 0.00012848183766591223\n",
      "Epoch 9247  \tTraining Loss: 0.0001250555554274395\tValidation Loss: 0.00012848069231520727\n",
      "Epoch 9248  \tTraining Loss: 0.00012505401595579342\tValidation Loss: 0.00012847953652800876\n",
      "Epoch 9249  \tTraining Loss: 0.00012505247853864787\tValidation Loss: 0.00012847832530164978\n",
      "Epoch 9250  \tTraining Loss: 0.0001250509403617848\tValidation Loss: 0.00012847717931133048\n",
      "Epoch 9251  \tTraining Loss: 0.00012504940237951124\tValidation Loss: 0.00012847597664137754\n",
      "Epoch 9252  \tTraining Loss: 0.00012504786575328158\tValidation Loss: 0.0001284748330045907\n",
      "Epoch 9253  \tTraining Loss: 0.0001250463279082324\tValidation Loss: 0.00012847359395699114\n",
      "Epoch 9254  \tTraining Loss: 0.00012504479221513205\tValidation Loss: 0.0001284724630992146\n",
      "Epoch 9255  \tTraining Loss: 0.00012504325544388122\tValidation Loss: 0.0001284713342093546\n",
      "Epoch 9256  \tTraining Loss: 0.0001250417189798554\tValidation Loss: 0.000128470137093987\n",
      "Epoch 9257  \tTraining Loss: 0.00012504018394694823\tValidation Loss: 0.0001284689971457765\n",
      "Epoch 9258  \tTraining Loss: 0.0001250386475921769\tValidation Loss: 0.00012846784571791787\n",
      "Epoch 9259  \tTraining Loss: 0.0001250371132099475\tValidation Loss: 0.00012846663812904593\n",
      "Epoch 9260  \tTraining Loss: 0.0001250355782485759\tValidation Loss: 0.00012846549560966447\n",
      "Epoch 9261  \tTraining Loss: 0.0001250340433136661\tValidation Loss: 0.0001284642959770281\n",
      "Epoch 9262  \tTraining Loss: 0.00012503250988852682\tValidation Loss: 0.0001284631554273541\n",
      "Epoch 9263  \tTraining Loss: 0.00012503097510193755\tValidation Loss: 0.00012846200421093933\n",
      "Epoch 9264  \tTraining Loss: 0.00012502944222014836\tValidation Loss: 0.0001284607972659656\n",
      "Epoch 9265  \tTraining Loss: 0.00012502790888368814\tValidation Loss: 0.0001284596558358904\n",
      "Epoch 9266  \tTraining Loss: 0.00012502637544243252\tValidation Loss: 0.00012845845740996773\n",
      "Epoch 9267  \tTraining Loss: 0.00012502484364663375\tValidation Loss: 0.00012845731830947343\n",
      "Epoch 9268  \tTraining Loss: 0.00012502331042067697\tValidation Loss: 0.00012845616859459775\n",
      "Epoch 9269  \tTraining Loss: 0.0001250217790281977\tValidation Loss: 0.00012845496313437037\n",
      "Epoch 9270  \tTraining Loss: 0.00012502024732199252\tValidation Loss: 0.00012845382333272106\n",
      "Epoch 9271  \tTraining Loss: 0.00012501871536865512\tValidation Loss: 0.0001284526264580064\n",
      "Epoch 9272  \tTraining Loss: 0.00012501718520287265\tValidation Loss: 0.00012845148902278672\n",
      "Epoch 9273  \tTraining Loss: 0.00012501565353490826\tValidation Loss: 0.00012845034094316701\n",
      "Epoch 9274  \tTraining Loss: 0.00012501412362984243\tValidation Loss: 0.00012844913704813612\n",
      "Epoch 9275  \tTraining Loss: 0.0001250125935518139\tValidation Loss: 0.0001284479989199906\n",
      "Epoch 9276  \tTraining Loss: 0.00012501106308549778\tValidation Loss: 0.00012844680361898697\n",
      "Epoch 9277  \tTraining Loss: 0.00012500953454673468\tValidation Loss: 0.00012844566785653576\n",
      "Epoch 9278  \tTraining Loss: 0.00012500800443475407\tValidation Loss: 0.0001284445214101256\n",
      "Epoch 9279  \tTraining Loss: 0.00012500647601708435\tValidation Loss: 0.0001284433190722919\n",
      "Epoch 9280  \tTraining Loss: 0.00012500494756354218\tValidation Loss: 0.00012844218260585433\n",
      "Epoch 9281  \tTraining Loss: 0.00012500341858453798\tValidation Loss: 0.000128440988864604\n",
      "Epoch 9282  \tTraining Loss: 0.00012500189166881817\tValidation Loss: 0.00012843985475979232\n",
      "Epoch 9283  \tTraining Loss: 0.00012500036311094886\tValidation Loss: 0.0001284387099309556\n",
      "Epoch 9284  \tTraining Loss: 0.00012499883618128266\tValidation Loss: 0.0001284375091346681\n",
      "Epoch 9285  \tTraining Loss: 0.0001249973093479872\tValidation Loss: 0.00012843637431455155\n",
      "Epoch 9286  \tTraining Loss: 0.0001249957818570394\tValidation Loss: 0.0001284351821180515\n",
      "Epoch 9287  \tTraining Loss: 0.0001249942565599966\tValidation Loss: 0.00012843404965643868\n",
      "Epoch 9288  \tTraining Loss: 0.00012499272955441198\tValidation Loss: 0.00012843290643127666\n",
      "Epoch 9289  \tTraining Loss: 0.00012499120411364756\tValidation Loss: 0.00012843170716323122\n",
      "Epoch 9290  \tTraining Loss: 0.00012498967889610193\tValidation Loss: 0.0001284305739767866\n",
      "Epoch 9291  \tTraining Loss: 0.0001249881529434901\tValidation Loss: 0.00012842929851039072\n",
      "Epoch 9292  \tTraining Loss: 0.00012498662956888522\tValidation Loss: 0.00012842824645072415\n",
      "Epoch 9293  \tTraining Loss: 0.0001249851038212159\tValidation Loss: 0.00012842712023140333\n",
      "Epoch 9294  \tTraining Loss: 0.00012498357990580817\tValidation Loss: 0.00012842592648044783\n",
      "Epoch 9295  \tTraining Loss: 0.00012498205624250617\tValidation Loss: 0.00012842479696303357\n",
      "Epoch 9296  \tTraining Loss: 0.0001249805318017375\tValidation Loss: 0.00012842360893708694\n",
      "Epoch 9297  \tTraining Loss: 0.00012497900964815537\tValidation Loss: 0.00012842248028190496\n",
      "Epoch 9298  \tTraining Loss: 0.00012497748573643474\tValidation Loss: 0.00012842134042483102\n",
      "Epoch 9299  \tTraining Loss: 0.00012497596336942329\tValidation Loss: 0.00012842014414746466\n",
      "Epoch 9300  \tTraining Loss: 0.0001249744412735174\tValidation Loss: 0.00012841901402018636\n",
      "Epoch 9301  \tTraining Loss: 0.00012497291835196096\tValidation Loss: 0.00012841782612919085\n",
      "Epoch 9302  \tTraining Loss: 0.0001249713977815272\tValidation Loss: 0.00012841669822197016\n",
      "Epoch 9303  \tTraining Loss: 0.00012496987542010937\tValidation Loss: 0.00012841555940898235\n",
      "Epoch 9304  \tTraining Loss: 0.0001249683545577133\tValidation Loss: 0.00012841436431811174\n",
      "Epoch 9305  \tTraining Loss: 0.00012496683405266947\tValidation Loss: 0.00012841323562688453\n",
      "Epoch 9306  \tTraining Loss: 0.00012496531264813868\tValidation Loss: 0.00012841209705572663\n",
      "Epoch 9307  \tTraining Loss: 0.00012496379342886394\tValidation Loss: 0.0001284109025359164\n",
      "Epoch 9308  \tTraining Loss: 0.00012496227313996288\tValidation Loss: 0.00012840977466096668\n",
      "Epoch 9309  \tTraining Loss: 0.00012496075334834328\tValidation Loss: 0.00012840858901899587\n",
      "Epoch 9310  \tTraining Loss: 0.00012495923459878957\tValidation Loss: 0.00012840746364933041\n",
      "Epoch 9311  \tTraining Loss: 0.00012495771471226556\tValidation Loss: 0.00012840632737299594\n",
      "Epoch 9312  \tTraining Loss: 0.00012495619696642636\tValidation Loss: 0.00012840513472848442\n",
      "Epoch 9313  \tTraining Loss: 0.00012495467828973054\tValidation Loss: 0.0001284040086955612\n",
      "Epoch 9314  \tTraining Loss: 0.00012495315997998745\tValidation Loss: 0.00012840282472272992\n",
      "Epoch 9315  \tTraining Loss: 0.0001249516428337824\tValidation Loss: 0.0001284017010757105\n",
      "Epoch 9316  \tTraining Loss: 0.00012495012448892476\tValidation Loss: 0.00012840056645457493\n",
      "Epoch 9317  \tTraining Loss: 0.00012494860822987025\tValidation Loss: 0.00012839937537199026\n",
      "Epoch 9318  \tTraining Loss: 0.00012494709115100353\tValidation Loss: 0.00012839825099469175\n",
      "Epoch 9319  \tTraining Loss: 0.00012494557432890434\tValidation Loss: 0.00012839706856876334\n",
      "Epoch 9320  \tTraining Loss: 0.00012494405877822116\tValidation Loss: 0.0001283959465628915\n",
      "Epoch 9321  \tTraining Loss: 0.00012494254197383938\tValidation Loss: 0.0001283948135404274\n",
      "Epoch 9322  \tTraining Loss: 0.00012494102720390967\tValidation Loss: 0.00012839362397923756\n",
      "Epoch 9323  \tTraining Loss: 0.0001249395117171663\tValidation Loss: 0.00012839250122793786\n",
      "Epoch 9324  \tTraining Loss: 0.00012493799638452118\tValidation Loss: 0.00012839132032589002\n",
      "Epoch 9325  \tTraining Loss: 0.00012493648242414937\tValidation Loss: 0.00012839019994285503\n",
      "Epoch 9326  \tTraining Loss: 0.00012493496715855742\tValidation Loss: 0.00012838906850367235\n",
      "Epoch 9327  \tTraining Loss: 0.00012493345387919175\tValidation Loss: 0.00012838788045044827\n",
      "Epoch 9328  \tTraining Loss: 0.00012493193997963382\tValidation Loss: 0.00012838675931328052\n",
      "Epoch 9329  \tTraining Loss: 0.00012493042613780336\tValidation Loss: 0.00012838557992417177\n",
      "Epoch 9330  \tTraining Loss: 0.00012492891389301113\tValidation Loss: 0.00012838436604446213\n",
      "Epoch 9331  \tTraining Loss: 0.00012492740053447393\tValidation Loss: 0.00012838330556342453\n",
      "Epoch 9332  \tTraining Loss: 0.00012492588869349066\tValidation Loss: 0.0001283821299457118\n",
      "Epoch 9333  \tTraining Loss: 0.00012492437630941635\tValidation Loss: 0.00012838100726052186\n",
      "Epoch 9334  \tTraining Loss: 0.0001249228642203793\tValidation Loss: 0.00012837982813603867\n",
      "Epoch 9335  \tTraining Loss: 0.00012492135332277304\tValidation Loss: 0.0001283786951862449\n",
      "Epoch 9336  \tTraining Loss: 0.00012491984121309495\tValidation Loss: 0.00012837756547433944\n",
      "Epoch 9337  \tTraining Loss: 0.00012491833119129798\tValidation Loss: 0.00012837637927103028\n",
      "Epoch 9338  \tTraining Loss: 0.000124916820345718\tValidation Loss: 0.00012837525587905237\n",
      "Epoch 9339  \tTraining Loss: 0.00012491530981264564\tValidation Loss: 0.00012837407701890254\n",
      "Epoch 9340  \tTraining Loss: 0.00012491380041435913\tValidation Loss: 0.00012837294494678485\n",
      "Epoch 9341  \tTraining Loss: 0.00012491228987360757\tValidation Loss: 0.00012837181642711602\n",
      "Epoch 9342  \tTraining Loss: 0.00012491078139880468\tValidation Loss: 0.00012837063158346727\n",
      "Epoch 9343  \tTraining Loss: 0.00012490927207786686\tValidation Loss: 0.0001283695098293055\n",
      "Epoch 9344  \tTraining Loss: 0.00012490776309441736\tValidation Loss: 0.0001283683326143904\n",
      "Epoch 9345  \tTraining Loss: 0.00012490625521469545\tValidation Loss: 0.0001283672127428682\n",
      "Epoch 9346  \tTraining Loss: 0.00012490474626129802\tValidation Loss: 0.00012836608505573888\n",
      "Epoch 9347  \tTraining Loss: 0.00012490323924080248\tValidation Loss: 0.00012836488668913274\n",
      "Epoch 9348  \tTraining Loss: 0.00012490173148029966\tValidation Loss: 0.00012836377045146843\n",
      "Epoch 9349  \tTraining Loss: 0.0001249002240580763\tValidation Loss: 0.00012836259614166145\n",
      "Epoch 9350  \tTraining Loss: 0.0001248987176920385\tValidation Loss: 0.00012836146833863687\n",
      "Epoch 9351  \tTraining Loss: 0.00012489721021339764\tValidation Loss: 0.00012836034376661245\n",
      "Epoch 9352  \tTraining Loss: 0.00012489570486371926\tValidation Loss: 0.0001283591625992515\n",
      "Epoch 9353  \tTraining Loss: 0.00012489419854539482\tValidation Loss: 0.00012835804468663159\n",
      "Epoch 9354  \tTraining Loss: 0.0001248926926979663\tValidation Loss: 0.00012835687107409118\n",
      "Epoch 9355  \tTraining Loss: 0.00012489118784667986\tValidation Loss: 0.0001283557445838259\n",
      "Epoch 9356  \tTraining Loss: 0.0001248896818674167\tValidation Loss: 0.00012835462159413018\n",
      "Epoch 9357  \tTraining Loss: 0.00012488817810215534\tValidation Loss: 0.00012835344204722743\n",
      "Epoch 9358  \tTraining Loss: 0.00012488667326551915\tValidation Loss: 0.00012835231550437906\n",
      "Epoch 9359  \tTraining Loss: 0.00012488516893781782\tValidation Loss: 0.0001283511445709534\n",
      "Epoch 9360  \tTraining Loss: 0.00012488366566251264\tValidation Loss: 0.00012835003522663476\n",
      "Epoch 9361  \tTraining Loss: 0.00012488216117051014\tValidation Loss: 0.00012834889996673303\n",
      "Epoch 9362  \tTraining Loss: 0.00012488065896626885\tValidation Loss: 0.00012834772198333294\n",
      "Epoch 9363  \tTraining Loss: 0.00012487915567406164\tValidation Loss: 0.00012834661201375098\n",
      "Epoch 9364  \tTraining Loss: 0.00012487765283172855\tValidation Loss: 0.0001283454388853481\n",
      "Epoch 9365  \tTraining Loss: 0.0001248761511312339\tValidation Loss: 0.0001283442454089683\n",
      "Epoch 9366  \tTraining Loss: 0.00012487464849788443\tValidation Loss: 0.0001283431899919436\n",
      "Epoch 9367  \tTraining Loss: 0.00012487314753356158\tValidation Loss: 0.00012834202876282904\n",
      "Epoch 9368  \tTraining Loss: 0.00012487164578840228\tValidation Loss: 0.000128340924377296\n",
      "Epoch 9369  \tTraining Loss: 0.00012487014453948852\tValidation Loss: 0.00012833974447136795\n",
      "Epoch 9370  \tTraining Loss: 0.00012486864420348514\tValidation Loss: 0.00012833863949086115\n",
      "Epoch 9371  \tTraining Loss: 0.00012486714294352802\tValidation Loss: 0.00012833747419395257\n",
      "Epoch 9372  \tTraining Loss: 0.0001248656438738559\tValidation Loss: 0.00012833635494828303\n",
      "Epoch 9373  \tTraining Loss: 0.00012486414304723766\tValidation Loss: 0.00012833523838103682\n",
      "Epoch 9374  \tTraining Loss: 0.00012486264396184953\tValidation Loss: 0.00012833406460542414\n",
      "Epoch 9375  \tTraining Loss: 0.0001248611448636996\tValidation Loss: 0.0001283329439396888\n",
      "Epoch 9376  \tTraining Loss: 0.0001248596451342139\tValidation Loss: 0.000128331778464349\n",
      "Epoch 9377  \tTraining Loss: 0.00012485814763787953\tValidation Loss: 0.00012833067481868315\n",
      "Epoch 9378  \tTraining Loss: 0.00012485664833369748\tValidation Loss: 0.00012832954505611032\n",
      "Epoch 9379  \tTraining Loss: 0.00012485515071314934\tValidation Loss: 0.00012832837233747109\n",
      "Epoch 9380  \tTraining Loss: 0.00012485365320784192\tValidation Loss: 0.00012832726795737804\n",
      "Epoch 9381  \tTraining Loss: 0.0001248521550317001\tValidation Loss: 0.00012832608965413257\n",
      "Epoch 9382  \tTraining Loss: 0.0001248506589309431\tValidation Loss: 0.00012832498744700621\n",
      "Epoch 9383  \tTraining Loss: 0.00012484916127639633\tValidation Loss: 0.00012832387406798112\n",
      "Epoch 9384  \tTraining Loss: 0.00012484766517667357\tValidation Loss: 0.00012832268859778147\n",
      "Epoch 9385  \tTraining Loss: 0.0001248461690674492\tValidation Loss: 0.0001283215752515749\n",
      "Epoch 9386  \tTraining Loss: 0.0001248446725375439\tValidation Loss: 0.00012832041431205777\n",
      "Epoch 9387  \tTraining Loss: 0.0001248431779329611\tValidation Loss: 0.00012831931483561833\n",
      "Epoch 9388  \tTraining Loss: 0.00012484168167459166\tValidation Loss: 0.000128318188824672\n",
      "Epoch 9389  \tTraining Loss: 0.00012484018722706808\tValidation Loss: 0.00012831701959176633\n",
      "Epoch 9390  \tTraining Loss: 0.0001248386926317869\tValidation Loss: 0.0001283159188437046\n",
      "Epoch 9391  \tTraining Loss: 0.00012483719759439103\tValidation Loss: 0.0001283147439112339\n",
      "Epoch 9392  \tTraining Loss: 0.00012483570445506376\tValidation Loss: 0.0001283136452856232\n",
      "Epoch 9393  \tTraining Loss: 0.00012483420982049907\tValidation Loss: 0.00012831253538179323\n",
      "Epoch 9394  \tTraining Loss: 0.0001248327168568551\tValidation Loss: 0.00012831135320312914\n",
      "Epoch 9395  \tTraining Loss: 0.00012483122369059973\tValidation Loss: 0.00012831024336562618\n",
      "Epoch 9396  \tTraining Loss: 0.00012482973029247465\tValidation Loss: 0.0001283090857354608\n",
      "Epoch 9397  \tTraining Loss: 0.00012482823862759338\tValidation Loss: 0.0001283079897533062\n",
      "Epoch 9398  \tTraining Loss: 0.00012482674540520425\tValidation Loss: 0.0001283068671312723\n",
      "Epoch 9399  \tTraining Loss: 0.00012482525407518956\tValidation Loss: 0.0001283057011492928\n",
      "Epoch 9400  \tTraining Loss: 0.00012482376242936425\tValidation Loss: 0.00012830460385281135\n",
      "Epoch 9401  \tTraining Loss: 0.00012482227056777784\tValidation Loss: 0.00012830334756200166\n",
      "Epoch 9402  \tTraining Loss: 0.00012482078067359612\tValidation Loss: 0.00012830232059430895\n",
      "Epoch 9403  \tTraining Loss: 0.00012481928869651548\tValidation Loss: 0.00012830123044124447\n",
      "Epoch 9404  \tTraining Loss: 0.0001248177990456049\tValidation Loss: 0.00012830007079855943\n",
      "Epoch 9405  \tTraining Loss: 0.00012481630870432058\tValidation Loss: 0.00012829896272102966\n",
      "Epoch 9406  \tTraining Loss: 0.00012481481844931958\tValidation Loss: 0.00012829779781169435\n",
      "Epoch 9407  \tTraining Loss: 0.00012481332967729427\tValidation Loss: 0.00012829670623904367\n",
      "Epoch 9408  \tTraining Loss: 0.00012481183951619922\tValidation Loss: 0.0001282956022382164\n",
      "Epoch 9409  \tTraining Loss: 0.00012481035135329877\tValidation Loss: 0.00012829442510824295\n",
      "Epoch 9410  \tTraining Loss: 0.0001248088625054807\tValidation Loss: 0.00012829333085125503\n",
      "Epoch 9411  \tTraining Loss: 0.0001248073739368839\tValidation Loss: 0.00012829217680370672\n",
      "Epoch 9412  \tTraining Loss: 0.00012480588648623034\tValidation Loss: 0.00012829107021389237\n",
      "Epoch 9413  \tTraining Loss: 0.00012480439780159584\tValidation Loss: 0.00012828995589556257\n",
      "Epoch 9414  \tTraining Loss: 0.00012480291130317734\tValidation Loss: 0.00012828879544924293\n",
      "Epoch 9415  \tTraining Loss: 0.000124801423897535\tValidation Loss: 0.0001282877034391879\n",
      "Epoch 9416  \tTraining Loss: 0.00012479993680531364\tValidation Loss: 0.00012828653635735138\n",
      "Epoch 9417  \tTraining Loss: 0.00012479845084432137\tValidation Loss: 0.00012828543551218253\n",
      "Epoch 9418  \tTraining Loss: 0.00012479696371854674\tValidation Loss: 0.00012828433469309525\n",
      "Epoch 9419  \tTraining Loss: 0.00012479547881906976\tValidation Loss: 0.00012828317548342665\n",
      "Epoch 9420  \tTraining Loss: 0.00012479399278626357\tValidation Loss: 0.0001282820700742291\n",
      "Epoch 9421  \tTraining Loss: 0.00012479250727651296\tValidation Loss: 0.00012828090845375638\n",
      "Epoch 9422  \tTraining Loss: 0.0001247910228195643\tValidation Loss: 0.00012827982107521855\n",
      "Epoch 9423  \tTraining Loss: 0.00012478953721025362\tValidation Loss: 0.00012827867211040139\n",
      "Epoch 9424  \tTraining Loss: 0.0001247880539791498\tValidation Loss: 0.00012827757015705882\n",
      "Epoch 9425  \tTraining Loss: 0.00012478656890968625\tValidation Loss: 0.00012827646013583365\n",
      "Epoch 9426  \tTraining Loss: 0.00012478508547101257\tValidation Loss: 0.00012827530370697072\n",
      "Epoch 9427  \tTraining Loss: 0.00012478360223972491\tValidation Loss: 0.0001282742158758607\n",
      "Epoch 9428  \tTraining Loss: 0.00012478211819582898\tValidation Loss: 0.0001282730526777427\n",
      "Epoch 9429  \tTraining Loss: 0.00012478063642125487\tValidation Loss: 0.00012827195593047373\n",
      "Epoch 9430  \tTraining Loss: 0.00012477915290824351\tValidation Loss: 0.00012827085912259621\n",
      "Epoch 9431  \tTraining Loss: 0.0001247776710609511\tValidation Loss: 0.00012826970371938368\n",
      "Epoch 9432  \tTraining Loss: 0.0001247761892035513\tValidation Loss: 0.00012826860233850374\n",
      "Epoch 9433  \tTraining Loss: 0.00012477470675076134\tValidation Loss: 0.00012826744451098203\n",
      "Epoch 9434  \tTraining Loss: 0.00012477322644541445\tValidation Loss: 0.00012826636117451636\n",
      "Epoch 9435  \tTraining Loss: 0.00012477174444749624\tValidation Loss: 0.00012826526551951953\n",
      "Epoch 9436  \tTraining Loss: 0.00012477026407188213\tValidation Loss: 0.00012826409651941123\n",
      "Epoch 9437  \tTraining Loss: 0.00012476878373645408\tValidation Loss: 0.00012826291609513118\n",
      "Epoch 9438  \tTraining Loss: 0.00012476730316225853\tValidation Loss: 0.00012826184842468935\n",
      "Epoch 9439  \tTraining Loss: 0.00012476582404489608\tValidation Loss: 0.00012826077997104196\n",
      "Epoch 9440  \tTraining Loss: 0.00012476434346445485\tValidation Loss: 0.00012825967794537644\n",
      "Epoch 9441  \tTraining Loss: 0.0001247628647646596\tValidation Loss: 0.00012825851954350519\n",
      "Epoch 9442  \tTraining Loss: 0.0001247613857277368\tValidation Loss: 0.00012825744130161537\n",
      "Epoch 9443  \tTraining Loss: 0.00012475990662657533\tValidation Loss: 0.00012825630162296035\n",
      "Epoch 9444  \tTraining Loss: 0.000124758428994225\tValidation Loss: 0.00012825520928308595\n",
      "Epoch 9445  \tTraining Loss: 0.00012475694993591932\tValidation Loss: 0.00012825410874715674\n",
      "Epoch 9446  \tTraining Loss: 0.00012475547285836433\tValidation Loss: 0.00012825295087582746\n",
      "Epoch 9447  \tTraining Loss: 0.00012475399524650792\tValidation Loss: 0.0001282518735373395\n",
      "Epoch 9448  \tTraining Loss: 0.00012475251770971982\tValidation Loss: 0.00012825073481925845\n",
      "Epoch 9449  \tTraining Loss: 0.0001247510414760008\tValidation Loss: 0.0001282496437270507\n",
      "Epoch 9450  \tTraining Loss: 0.0001247495639187874\tValidation Loss: 0.00012824854455219915\n",
      "Epoch 9451  \tTraining Loss: 0.00012474808843651863\tValidation Loss: 0.00012824739861040012\n",
      "Epoch 9452  \tTraining Loss: 0.00012474661225768407\tValidation Loss: 0.000128246321732597\n",
      "Epoch 9453  \tTraining Loss: 0.00012474513622405623\tValidation Loss: 0.0001282451690927638\n",
      "Epoch 9454  \tTraining Loss: 0.00012474366144747613\tValidation Loss: 0.0001282440833506891\n",
      "Epoch 9455  \tTraining Loss: 0.00012474218539027673\tValidation Loss: 0.00012824298693486812\n",
      "Epoch 9456  \tTraining Loss: 0.0001247407115203487\tValidation Loss: 0.00012824184314051378\n",
      "Epoch 9457  \tTraining Loss: 0.00012473923672126673\tValidation Loss: 0.00012824076820291372\n",
      "Epoch 9458  \tTraining Loss: 0.00012473776227424208\tValidation Loss: 0.00012823961726144206\n",
      "Epoch 9459  \tTraining Loss: 0.0001247362889032763\tValidation Loss: 0.00012823853326277733\n",
      "Epoch 9460  \tTraining Loss: 0.00012473481434131635\tValidation Loss: 0.0001282374385216383\n",
      "Epoch 9461  \tTraining Loss: 0.00012473334207207512\tValidation Loss: 0.00012823629631724166\n",
      "Epoch 9462  \tTraining Loss: 0.00012473186866058733\tValidation Loss: 0.0001282352230618122\n",
      "Epoch 9463  \tTraining Loss: 0.0001247303957980358\tValidation Loss: 0.0001282340736855682\n",
      "Epoch 9464  \tTraining Loss: 0.0001247289238308607\tValidation Loss: 0.0001282329913526812\n",
      "Epoch 9465  \tTraining Loss: 0.0001247274508320552\tValidation Loss: 0.00012823184831159778\n",
      "Epoch 9466  \tTraining Loss: 0.00012472598022029568\tValidation Loss: 0.00012823077935182815\n",
      "Epoch 9467  \tTraining Loss: 0.0001247245077885221\tValidation Loss: 0.00012822969737926655\n",
      "Epoch 9468  \tTraining Loss: 0.00012472303691436382\tValidation Loss: 0.0001282285412872366\n",
      "Epoch 9469  \tTraining Loss: 0.00012472156618404726\tValidation Loss: 0.0001282274588397984\n",
      "Epoch 9470  \tTraining Loss: 0.00012472009482563255\tValidation Loss: 0.00012822631662838364\n",
      "Epoch 9471  \tTraining Loss: 0.00012471862558426666\tValidation Loss: 0.00012822524898344875\n",
      "Epoch 9472  \tTraining Loss: 0.00012471715467236413\tValidation Loss: 0.00012822408396366927\n",
      "Epoch 9473  \tTraining Loss: 0.00012471568569951669\tValidation Loss: 0.00012822300798278567\n",
      "Epoch 9474  \tTraining Loss: 0.00012471421609320512\tValidation Loss: 0.0001282219422930484\n",
      "Epoch 9475  \tTraining Loss: 0.00012471274637083433\tValidation Loss: 0.00012822080547093104\n",
      "Epoch 9476  \tTraining Loss: 0.00012471127843296088\tValidation Loss: 0.00012821974144161194\n",
      "Epoch 9477  \tTraining Loss: 0.00012470980899582874\tValidation Loss: 0.00012821866353859\n",
      "Epoch 9478  \tTraining Loss: 0.00012470834139374396\tValidation Loss: 0.0001282175108835998\n",
      "Epoch 9479  \tTraining Loss: 0.0001247068733458207\tValidation Loss: 0.00012821643178325692\n",
      "Epoch 9480  \tTraining Loss: 0.0001247054052660495\tValidation Loss: 0.00012821529251533112\n",
      "Epoch 9481  \tTraining Loss: 0.0001247039386652379\tValidation Loss: 0.0001282142279084711\n",
      "Epoch 9482  \tTraining Loss: 0.0001247024707312536\tValidation Loss: 0.0001282131502114725\n",
      "Epoch 9483  \tTraining Loss: 0.0001247010047403772\tValidation Loss: 0.00012821199817922375\n",
      "Epoch 9484  \tTraining Loss: 0.00012469953805874364\tValidation Loss: 0.00012821092014908789\n",
      "Epoch 9485  \tTraining Loss: 0.0001246980715821783\tValidation Loss: 0.00012820978206333874\n",
      "Epoch 9486  \tTraining Loss: 0.00012469660633642469\tValidation Loss: 0.0001282087082735731\n",
      "Epoch 9487  \tTraining Loss: 0.00012469513984877552\tValidation Loss: 0.00012820763324709785\n",
      "Epoch 9488  \tTraining Loss: 0.00012469367556421112\tValidation Loss: 0.00012820649815071874\n",
      "Epoch 9489  \tTraining Loss: 0.00012469221020755837\tValidation Loss: 0.00012820541799315506\n",
      "Epoch 9490  \tTraining Loss: 0.0001246907453228246\tValidation Loss: 0.0001282042802867576\n",
      "Epoch 9491  \tTraining Loss: 0.00012468928145256648\tValidation Loss: 0.0001282032075522752\n",
      "Epoch 9492  \tTraining Loss: 0.0001246878164389425\tValidation Loss: 0.0001282020835847129\n",
      "Epoch 9493  \tTraining Loss: 0.00012468635392105893\tValidation Loss: 0.00012820102217510538\n",
      "Epoch 9494  \tTraining Loss: 0.00012468488946681974\tValidation Loss: 0.0001281999330600385\n",
      "Epoch 9495  \tTraining Loss: 0.00012468342660850655\tValidation Loss: 0.00012819878855244632\n",
      "Epoch 9496  \tTraining Loss: 0.00012468196394827742\tValidation Loss: 0.00012819771566590212\n",
      "Epoch 9497  \tTraining Loss: 0.00012468050057588058\tValidation Loss: 0.00012819658190823632\n",
      "Epoch 9498  \tTraining Loss: 0.00012467903938126639\tValidation Loss: 0.00012819552297643142\n",
      "Epoch 9499  \tTraining Loss: 0.00012467757646952455\tValidation Loss: 0.0001281944507441533\n",
      "Epoch 9500  \tTraining Loss: 0.00012467611518438194\tValidation Loss: 0.00012819330389425416\n",
      "Epoch 9501  \tTraining Loss: 0.00012467465388459859\tValidation Loss: 0.00012819223141241655\n",
      "Epoch 9502  \tTraining Loss: 0.0001246731921132489\tValidation Loss: 0.00012819109854275744\n",
      "Epoch 9503  \tTraining Loss: 0.00012467173224471555\tValidation Loss: 0.00012819003030413578\n",
      "Epoch 9504  \tTraining Loss: 0.00012467027078620396\tValidation Loss: 0.00012818896072110876\n",
      "Epoch 9505  \tTraining Loss: 0.00012466881121375658\tValidation Loss: 0.00012818783079087613\n",
      "Epoch 9506  \tTraining Loss: 0.0001246673512248736\tValidation Loss: 0.00012818675613200247\n",
      "Epoch 9507  \tTraining Loss: 0.0001246658910481916\tValidation Loss: 0.00012818562359224653\n",
      "Epoch 9508  \tTraining Loss: 0.00012466443254021428\tValidation Loss: 0.0001281845563560484\n",
      "Epoch 9509  \tTraining Loss: 0.00012466297261469762\tValidation Loss: 0.0001281833930364464\n",
      "Epoch 9510  \tTraining Loss: 0.00012466151485543\tValidation Loss: 0.00012818234435501427\n",
      "Epoch 9511  \tTraining Loss: 0.00012466005603221415\tValidation Loss: 0.00012818130190922586\n",
      "Epoch 9512  \tTraining Loss: 0.0001246585974823143\tValidation Loss: 0.00012818017096805846\n",
      "Epoch 9513  \tTraining Loss: 0.00012465714027535708\tValidation Loss: 0.00012817910615043165\n",
      "Epoch 9514  \tTraining Loss: 0.00012465568173562096\tValidation Loss: 0.00012817802924883342\n",
      "Epoch 9515  \tTraining Loss: 0.00012465422537999424\tValidation Loss: 0.00012817689297728855\n",
      "Epoch 9516  \tTraining Loss: 0.00012465276806206918\tValidation Loss: 0.0001281758381981333\n",
      "Epoch 9517  \tTraining Loss: 0.0001246513112625151\tValidation Loss: 0.00012817472022371207\n",
      "Epoch 9518  \tTraining Loss: 0.0001246498553215554\tValidation Loss: 0.00012817365099945452\n",
      "Epoch 9519  \tTraining Loss: 0.00012464839829652024\tValidation Loss: 0.0001281725224889673\n",
      "Epoch 9520  \tTraining Loss: 0.00012464694369064336\tValidation Loss: 0.00012817145906397043\n",
      "Epoch 9521  \tTraining Loss: 0.00012464548721710772\tValidation Loss: 0.00012817038366819955\n",
      "Epoch 9522  \tTraining Loss: 0.00012464403239150724\tValidation Loss: 0.00012816924893546518\n",
      "Epoch 9523  \tTraining Loss: 0.00012464257769364737\tValidation Loss: 0.00012816819601380912\n",
      "Epoch 9524  \tTraining Loss: 0.00012464112239965903\tValidation Loss: 0.00012816707989556924\n",
      "Epoch 9525  \tTraining Loss: 0.00012463966905790205\tValidation Loss: 0.00012816601275095386\n",
      "Epoch 9526  \tTraining Loss: 0.00012463821405916565\tValidation Loss: 0.00012816493694923708\n",
      "Epoch 9527  \tTraining Loss: 0.00012463676084669656\tValidation Loss: 0.00012816380272239733\n",
      "Epoch 9528  \tTraining Loss: 0.00012463530744435914\tValidation Loss: 0.00012816274026857867\n",
      "Epoch 9529  \tTraining Loss: 0.00012463385371511555\tValidation Loss: 0.00012816161599377368\n",
      "Epoch 9530  \tTraining Loss: 0.00012463240178072954\tValidation Loss: 0.00012816056711389796\n",
      "Epoch 9531  \tTraining Loss: 0.0001246309482933577\tValidation Loss: 0.0001281595045876069\n",
      "Epoch 9532  \tTraining Loss: 0.00012462949665347365\tValidation Loss: 0.00012815836691324668\n",
      "Epoch 9533  \tTraining Loss: 0.0001246280445891098\tValidation Loss: 0.00012815730430566858\n",
      "Epoch 9534  \tTraining Loss: 0.00012462659246313806\tValidation Loss: 0.0001281561806666555\n",
      "Epoch 9535  \tTraining Loss: 0.00012462514182084883\tValidation Loss: 0.0001281551223088141\n",
      "Epoch 9536  \tTraining Loss: 0.0001246236897666964\tValidation Loss: 0.00012815405172075206\n",
      "Epoch 9537  \tTraining Loss: 0.00012462223982637072\tValidation Loss: 0.00012815293218561676\n",
      "Epoch 9538  \tTraining Loss: 0.00012462078908430875\tValidation Loss: 0.00012815188290684053\n",
      "Epoch 9539  \tTraining Loss: 0.000124619338554089\tValidation Loss: 0.00012815075580348964\n",
      "Epoch 9540  \tTraining Loss: 0.00012461788924899022\tValidation Loss: 0.00012814969726665358\n",
      "Epoch 9541  \tTraining Loss: 0.00012461643866188535\tValidation Loss: 0.00012814862735187118\n",
      "Epoch 9542  \tTraining Loss: 0.00012461499028156356\tValidation Loss: 0.00012814749821535401\n",
      "Epoch 9543  \tTraining Loss: 0.0001246135408649641\tValidation Loss: 0.0001281464515501289\n",
      "Epoch 9544  \tTraining Loss: 0.0001246120920386697\tValidation Loss: 0.0001281453413336294\n",
      "Epoch 9545  \tTraining Loss: 0.00012461064401402623\tValidation Loss: 0.00012814428053344666\n",
      "Epoch 9546  \tTraining Loss: 0.00012460919504969888\tValidation Loss: 0.00012814307574171961\n",
      "Epoch 9547  \tTraining Loss: 0.00012460774862638855\tValidation Loss: 0.00012814209952246778\n",
      "Epoch 9548  \tTraining Loss: 0.00012460629978678565\tValidation Loss: 0.00012814104765626228\n",
      "Epoch 9549  \tTraining Loss: 0.00012460485295373943\tValidation Loss: 0.00012813992481964953\n",
      "Epoch 9550  \tTraining Loss: 0.00012460340606523844\tValidation Loss: 0.0001281388718230042\n",
      "Epoch 9551  \tTraining Loss: 0.00012460195875410164\tValidation Loss: 0.00012813775542136618\n",
      "Epoch 9552  \tTraining Loss: 0.00012460051328967283\tValidation Loss: 0.0001281367142099413\n",
      "Epoch 9553  \tTraining Loss: 0.00012459906625547385\tValidation Loss: 0.0001281356587303611\n",
      "Epoch 9554  \tTraining Loss: 0.000124597621060044\tValidation Loss: 0.0001281345274964974\n",
      "Epoch 9555  \tTraining Loss: 0.00012459617544928995\tValidation Loss: 0.00012813347164413846\n",
      "Epoch 9556  \tTraining Loss: 0.0001245947297751298\tValidation Loss: 0.0001281323542239297\n",
      "Epoch 9557  \tTraining Loss: 0.0001245932855735401\tValidation Loss: 0.0001281313024666767\n",
      "Epoch 9558  \tTraining Loss: 0.0001245918399607398\tValidation Loss: 0.00012813023827576352\n",
      "Epoch 9559  \tTraining Loss: 0.0001245903964416159\tValidation Loss: 0.0001281291141386949\n",
      "Epoch 9560  \tTraining Loss: 0.0001245889521524041\tValidation Loss: 0.00012812807264760525\n",
      "Epoch 9561  \tTraining Loss: 0.00012458750814970003\tValidation Loss: 0.0001281269671533401\n",
      "Epoch 9562  \tTraining Loss: 0.00012458606520664468\tValidation Loss: 0.00012812591135792027\n",
      "Epoch 9563  \tTraining Loss: 0.00012458462105176702\tValidation Loss: 0.0001281248466410921\n",
      "Epoch 9564  \tTraining Loss: 0.00012458317914193007\tValidation Loss: 0.0001281237229577707\n",
      "Epoch 9565  \tTraining Loss: 0.0001245817361228845\tValidation Loss: 0.00012812267185367493\n",
      "Epoch 9566  \tTraining Loss: 0.00012458029369487496\tValidation Loss: 0.0001281215581775105\n",
      "Epoch 9567  \tTraining Loss: 0.000124578852090568\tValidation Loss: 0.00012812050997495922\n",
      "Epoch 9568  \tTraining Loss: 0.00012457740953614586\tValidation Loss: 0.00012811940856237019\n",
      "Epoch 9569  \tTraining Loss: 0.00012457596924236644\tValidation Loss: 0.00012811837089381903\n",
      "Epoch 9570  \tTraining Loss: 0.00012457452712302081\tValidation Loss: 0.00012811730474923514\n",
      "Epoch 9571  \tTraining Loss: 0.00012457308673275192\tValidation Loss: 0.00012811618207983927\n",
      "Epoch 9572  \tTraining Loss: 0.00012457164627420388\tValidation Loss: 0.00012811513264818943\n",
      "Epoch 9573  \tTraining Loss: 0.00012457020537173793\tValidation Loss: 0.00012811402080721476\n",
      "Epoch 9574  \tTraining Loss: 0.00012456876632049128\tValidation Loss: 0.00012811297468519532\n",
      "Epoch 9575  \tTraining Loss: 0.00012456732566569769\tValidation Loss: 0.0001281119158849619\n",
      "Epoch 9576  \tTraining Loss: 0.00012456588691827608\tValidation Loss: 0.00012811079683823506\n",
      "Epoch 9577  \tTraining Loss: 0.00012456444776645707\tValidation Loss: 0.00012810976079285114\n",
      "Epoch 9578  \tTraining Loss: 0.0001245630085376836\tValidation Loss: 0.00012810866036525024\n",
      "Epoch 9579  \tTraining Loss: 0.00012456157073351142\tValidation Loss: 0.00012810760995135014\n",
      "Epoch 9580  \tTraining Loss: 0.00012456013153050062\tValidation Loss: 0.00012810655048555106\n",
      "Epoch 9581  \tTraining Loss: 0.00012455869439370227\tValidation Loss: 0.0001281054317995678\n",
      "Epoch 9582  \tTraining Loss: 0.00012455725654297146\tValidation Loss: 0.0001281043018339653\n",
      "Epoch 9583  \tTraining Loss: 0.0001245558191663241\tValidation Loss: 0.0001281032718020803\n",
      "Epoch 9584  \tTraining Loss: 0.00012455438244119793\tValidation Loss: 0.00012810224404663146\n",
      "Epoch 9585  \tTraining Loss: 0.00012455294469203908\tValidation Loss: 0.00012810114079567963\n",
      "Epoch 9586  \tTraining Loss: 0.00012455150942261675\tValidation Loss: 0.00012810010099659994\n",
      "Epoch 9587  \tTraining Loss: 0.0001245500722534829\tValidation Loss: 0.0001280990580076691\n",
      "Epoch 9588  \tTraining Loss: 0.0001245486368002795\tValidation Loss: 0.00012809795258869676\n",
      "Epoch 9589  \tTraining Loss: 0.0001245472013164612\tValidation Loss: 0.00012809690366667647\n",
      "Epoch 9590  \tTraining Loss: 0.00012454576528097557\tValidation Loss: 0.00012809579496413594\n",
      "Epoch 9591  \tTraining Loss: 0.00012454433123855414\tValidation Loss: 0.0001280947530690815\n",
      "Epoch 9592  \tTraining Loss: 0.000124542895518633\tValidation Loss: 0.00012809369873738322\n",
      "Epoch 9593  \tTraining Loss: 0.00012454146165406265\tValidation Loss: 0.00012809258405832623\n",
      "Epoch 9594  \tTraining Loss: 0.0001245400274590332\tValidation Loss: 0.00012809154206842415\n",
      "Epoch 9595  \tTraining Loss: 0.000124538593081314\tValidation Loss: 0.00012809043671012474\n",
      "Epoch 9596  \tTraining Loss: 0.00012453716029363135\tValidation Loss: 0.0001280894080527396\n",
      "Epoch 9597  \tTraining Loss: 0.00012453572608764056\tValidation Loss: 0.00012808836509494952\n",
      "Epoch 9598  \tTraining Loss: 0.0001245342938114418\tValidation Loss: 0.00012808724594213735\n",
      "Epoch 9599  \tTraining Loss: 0.00012453286089481177\tValidation Loss: 0.00012808620329631622\n",
      "Epoch 9600  \tTraining Loss: 0.0001245314281358229\tValidation Loss: 0.00012808509830293912\n",
      "Epoch 9601  \tTraining Loss: 0.0001245299966198903\tValidation Loss: 0.00012808405992797286\n",
      "Epoch 9602  \tTraining Loss: 0.0001245285637994852\tValidation Loss: 0.0001280830088185031\n",
      "Epoch 9603  \tTraining Loss: 0.0001245271331970961\tValidation Loss: 0.00012808189713708724\n",
      "Epoch 9604  \tTraining Loss: 0.0001245257015339463\tValidation Loss: 0.0001280808583448291\n",
      "Epoch 9605  \tTraining Loss: 0.00012452427041352183\tValidation Loss: 0.0001280797559669652\n",
      "Epoch 9606  \tTraining Loss: 0.00012452284015387948\tValidation Loss: 0.00012807871973883082\n",
      "Epoch 9607  \tTraining Loss: 0.00012452140890041222\tValidation Loss: 0.00012807762949732697\n",
      "Epoch 9608  \tTraining Loss: 0.0001245199799402821\tValidation Loss: 0.00012807660369862389\n",
      "Epoch 9609  \tTraining Loss: 0.00012451854913443297\tValidation Loss: 0.00012807554907956213\n",
      "Epoch 9610  \tTraining Loss: 0.00012451712005903624\tValidation Loss: 0.00012807443737414175\n",
      "Epoch 9611  \tTraining Loss: 0.00012451569091261516\tValidation Loss: 0.00012807339974369776\n",
      "Epoch 9612  \tTraining Loss: 0.0001245142613261483\tValidation Loss: 0.0001280722989206299\n",
      "Epoch 9613  \tTraining Loss: 0.00012451283357510138\tValidation Loss: 0.00012807126460144506\n",
      "Epoch 9614  \tTraining Loss: 0.00012451140422094808\tValidation Loss: 0.00012807021730218827\n",
      "Epoch 9615  \tTraining Loss: 0.00012450997679551334\tValidation Loss: 0.0001280691091825142\n",
      "Epoch 9616  \tTraining Loss: 0.00012450854888872354\tValidation Loss: 0.00012806807416291686\n",
      "Epoch 9617  \tTraining Loss: 0.00012450712134410156\tValidation Loss: 0.00012806695065768083\n",
      "Epoch 9618  \tTraining Loss: 0.00012450569579290725\tValidation Loss: 0.00012806582063226162\n",
      "Epoch 9619  \tTraining Loss: 0.00012450426802819935\tValidation Loss: 0.0001280648626950341\n",
      "Epoch 9620  \tTraining Loss: 0.00012450284251048347\tValidation Loss: 0.00012806376649444646\n",
      "Epoch 9621  \tTraining Loss: 0.00012450141664807296\tValidation Loss: 0.00012806279626579228\n",
      "Epoch 9622  \tTraining Loss: 0.00012449999131723745\tValidation Loss: 0.0001280616313875015\n",
      "Epoch 9623  \tTraining Loss: 0.00012449856640864258\tValidation Loss: 0.00012806059763574497\n",
      "Epoch 9624  \tTraining Loss: 0.0001244971412840289\tValidation Loss: 0.00012805956044258988\n",
      "Epoch 9625  \tTraining Loss: 0.00012449571768407094\tValidation Loss: 0.000128058460366944\n",
      "Epoch 9626  \tTraining Loss: 0.00012449429220045898\tValidation Loss: 0.00012805746866817287\n",
      "Epoch 9627  \tTraining Loss: 0.0001244928698219243\tValidation Loss: 0.00012805632800723572\n",
      "Epoch 9628  \tTraining Loss: 0.0001244914453609038\tValidation Loss: 0.0001280552569719966\n",
      "Epoch 9629  \tTraining Loss: 0.00012449002192875186\tValidation Loss: 0.00012805421647970426\n",
      "Epoch 9630  \tTraining Loss: 0.00012448860002193014\tValidation Loss: 0.0001280531526693414\n",
      "Epoch 9631  \tTraining Loss: 0.00012448717556342567\tValidation Loss: 0.00012805207162169601\n",
      "Epoch 9632  \tTraining Loss: 0.00012448575477446005\tValidation Loss: 0.00012805102386624455\n",
      "Epoch 9633  \tTraining Loss: 0.0001244843322657828\tValidation Loss: 0.0001280499594939659\n",
      "Epoch 9634  \tTraining Loss: 0.00012448290990852898\tValidation Loss: 0.0001280488830757532\n",
      "Epoch 9635  \tTraining Loss: 0.00012448148923579493\tValidation Loss: 0.00012804781752903102\n",
      "Epoch 9636  \tTraining Loss: 0.00012448006698932284\tValidation Loss: 0.0001280467214194245\n",
      "Epoch 9637  \tTraining Loss: 0.00012447864727901936\tValidation Loss: 0.0001280457164508617\n",
      "Epoch 9638  \tTraining Loss: 0.00012447722539094287\tValidation Loss: 0.00012804463994538955\n",
      "Epoch 9639  \tTraining Loss: 0.0001244758060122194\tValidation Loss: 0.000128043592585698\n",
      "Epoch 9640  \tTraining Loss: 0.00012447438585109363\tValidation Loss: 0.00012804249294166297\n",
      "Epoch 9641  \tTraining Loss: 0.0001244729649672961\tValidation Loss: 0.00012804142716404807\n",
      "Epoch 9642  \tTraining Loss: 0.00012447154716736485\tValidation Loss: 0.0001280403665383719\n",
      "Epoch 9643  \tTraining Loss: 0.00012447012650864786\tValidation Loss: 0.0001280393261588776\n",
      "Epoch 9644  \tTraining Loss: 0.00012446870847914525\tValidation Loss: 0.0001280382487327293\n",
      "Epoch 9645  \tTraining Loss: 0.00012446728969331639\tValidation Loss: 0.00012803718655783725\n",
      "Epoch 9646  \tTraining Loss: 0.00012446587121202012\tValidation Loss: 0.0001280361503092206\n",
      "Epoch 9647  \tTraining Loss: 0.00012446445379134697\tValidation Loss: 0.00012803505512071748\n",
      "Epoch 9648  \tTraining Loss: 0.00012446303457510334\tValidation Loss: 0.00012803393631923148\n",
      "Epoch 9649  \tTraining Loss: 0.00012446161886422576\tValidation Loss: 0.00012803297278554681\n",
      "Epoch 9650  \tTraining Loss: 0.00012446020067027305\tValidation Loss: 0.00012803190413291501\n",
      "Epoch 9651  \tTraining Loss: 0.0001244587840224553\tValidation Loss: 0.0001280308257146925\n",
      "Epoch 9652  \tTraining Loss: 0.00012445736771780279\tValidation Loss: 0.0001280297648456858\n",
      "Epoch 9653  \tTraining Loss: 0.0001244559508932947\tValidation Loss: 0.00012802873031266646\n",
      "Epoch 9654  \tTraining Loss: 0.00012445453590203897\tValidation Loss: 0.00012802763722250972\n",
      "Epoch 9655  \tTraining Loss: 0.00012445311845801142\tValidation Loss: 0.00012802657287404944\n",
      "Epoch 9656  \tTraining Loss: 0.00012445170462286123\tValidation Loss: 0.00012802553583578745\n",
      "Epoch 9657  \tTraining Loss: 0.00012445028907596013\tValidation Loss: 0.0001280244812508593\n",
      "Epoch 9658  \tTraining Loss: 0.00012444887380134526\tValidation Loss: 0.00012802332976716958\n",
      "Epoch 9659  \tTraining Loss: 0.00012444746015564029\tValidation Loss: 0.00012802232689491843\n",
      "Epoch 9660  \tTraining Loss: 0.00012444604465492164\tValidation Loss: 0.00012802131146791914\n",
      "Epoch 9661  \tTraining Loss: 0.00012444463233153864\tValidation Loss: 0.00012802026260187388\n",
      "Epoch 9662  \tTraining Loss: 0.00012444321682454856\tValidation Loss: 0.00012801919445118437\n",
      "Epoch 9663  \tTraining Loss: 0.00012444180451381232\tValidation Loss: 0.0001280181336726585\n",
      "Epoch 9664  \tTraining Loss: 0.00012444039136823106\tValidation Loss: 0.00012801708178252025\n",
      "Epoch 9665  \tTraining Loss: 0.00012443897831148897\tValidation Loss: 0.0001280160537611526\n",
      "Epoch 9666  \tTraining Loss: 0.00012443756624321768\tValidation Loss: 0.00012801496608045042\n",
      "Epoch 9667  \tTraining Loss: 0.00012443615262779323\tValidation Loss: 0.00012801391032304135\n",
      "Epoch 9668  \tTraining Loss: 0.0001244347425762579\tValidation Loss: 0.00012801285937041735\n",
      "Epoch 9669  \tTraining Loss: 0.000124433329365599\tValidation Loss: 0.0001280118281282525\n",
      "Epoch 9670  \tTraining Loss: 0.00012443191886533654\tValidation Loss: 0.00012801075952951105\n",
      "Epoch 9671  \tTraining Loss: 0.00012443050737449672\tValidation Loss: 0.0001280096816125\n",
      "Epoch 9672  \tTraining Loss: 0.00012442909646380424\tValidation Loss: 0.00012800865495157478\n",
      "Epoch 9673  \tTraining Loss: 0.0001244276867447566\tValidation Loss: 0.0001280076062712446\n",
      "Epoch 9674  \tTraining Loss: 0.00012442627520325574\tValidation Loss: 0.00012800659651232915\n",
      "Epoch 9675  \tTraining Loss: 0.0001244248663547487\tValidation Loss: 0.00012800544408215925\n",
      "Epoch 9676  \tTraining Loss: 0.0001244234561473102\tValidation Loss: 0.00012800448308913307\n",
      "Epoch 9677  \tTraining Loss: 0.00012442204733983502\tValidation Loss: 0.0001280033678265318\n",
      "Epoch 9678  \tTraining Loss: 0.00012442063782476632\tValidation Loss: 0.00012800237297653225\n",
      "Epoch 9679  \tTraining Loss: 0.0001244192288490778\tValidation Loss: 0.00012800123110187837\n",
      "Epoch 9680  \tTraining Loss: 0.00012441782070556084\tValidation Loss: 0.00012800021721450606\n",
      "Epoch 9681  \tTraining Loss: 0.00012441641232032556\tValidation Loss: 0.0001279992512101132\n",
      "Epoch 9682  \tTraining Loss: 0.00012441500459142176\tValidation Loss: 0.00012799809422866468\n",
      "Epoch 9683  \tTraining Loss: 0.0001244135968468991\tValidation Loss: 0.00012799710931165573\n",
      "Epoch 9684  \tTraining Loss: 0.00012441218906489616\tValidation Loss: 0.0001279959718891755\n",
      "Epoch 9685  \tTraining Loss: 0.0001244107827622003\tValidation Loss: 0.00012799501780232442\n",
      "Epoch 9686  \tTraining Loss: 0.00012440937529767756\tValidation Loss: 0.0001279939071400589\n",
      "Epoch 9687  \tTraining Loss: 0.00012440796954585207\tValidation Loss: 0.0001279929163407939\n",
      "Epoch 9688  \tTraining Loss: 0.00012440656255134635\tValidation Loss: 0.00012799183063701123\n",
      "Epoch 9689  \tTraining Loss: 0.00012440515673702626\tValidation Loss: 0.00012799080002183123\n",
      "Epoch 9690  \tTraining Loss: 0.00012440375188464705\tValidation Loss: 0.0001279897547663833\n",
      "Epoch 9691  \tTraining Loss: 0.0001244023452852384\tValidation Loss: 0.0001279887000349909\n",
      "Epoch 9692  \tTraining Loss: 0.00012440094170449586\tValidation Loss: 0.0001279876257220707\n",
      "Epoch 9693  \tTraining Loss: 0.00012439953519747878\tValidation Loss: 0.00012798663524163286\n",
      "Epoch 9694  \tTraining Loss: 0.00012439813241350663\tValidation Loss: 0.00012798552124059135\n",
      "Epoch 9695  \tTraining Loss: 0.0001243967275035429\tValidation Loss: 0.00012798450819076018\n",
      "Epoch 9696  \tTraining Loss: 0.00012439532366477499\tValidation Loss: 0.00012798345893610448\n",
      "Epoch 9697  \tTraining Loss: 0.00012439392037175452\tValidation Loss: 0.0001279823118547298\n",
      "Epoch 9698  \tTraining Loss: 0.00012439251658105264\tValidation Loss: 0.0001279813454494695\n",
      "Epoch 9699  \tTraining Loss: 0.00012439111432458057\tValidation Loss: 0.00012798032704230215\n",
      "Epoch 9700  \tTraining Loss: 0.00012438971097022963\tValidation Loss: 0.00012797936487031207\n",
      "Epoch 9701  \tTraining Loss: 0.00012438830883266714\tValidation Loss: 0.00012797821654616612\n",
      "Epoch 9702  \tTraining Loss: 0.00012438690652213818\tValidation Loss: 0.0001279772423542148\n",
      "Epoch 9703  \tTraining Loss: 0.00012438550431576574\tValidation Loss: 0.00012797610806933497\n",
      "Epoch 9704  \tTraining Loss: 0.00012438410327453354\tValidation Loss: 0.00012797516530641266\n",
      "Epoch 9705  \tTraining Loss: 0.00012438270146770283\tValidation Loss: 0.000127974057588275\n",
      "Epoch 9706  \tTraining Loss: 0.0001243813011991804\tValidation Loss: 0.00012797307505759935\n",
      "Epoch 9707  \tTraining Loss: 0.00012437989962064025\tValidation Loss: 0.00012797199911914598\n",
      "Epoch 9708  \tTraining Loss: 0.0001243784993372239\tValidation Loss: 0.0001279709465626406\n",
      "Epoch 9709  \tTraining Loss: 0.00012437709950717058\tValidation Loss: 0.0001279699132253136\n",
      "Epoch 9710  \tTraining Loss: 0.00012437569918486997\tValidation Loss: 0.00012796889575558085\n",
      "Epoch 9711  \tTraining Loss: 0.0001243743004370229\tValidation Loss: 0.00012796782452059773\n",
      "Epoch 9712  \tTraining Loss: 0.00012437289973848435\tValidation Loss: 0.000127966839782727\n",
      "Epoch 9713  \tTraining Loss: 0.00012437150185062146\tValidation Loss: 0.0001279656987810072\n",
      "Epoch 9714  \tTraining Loss: 0.00012437010237930973\tValidation Loss: 0.00012796473210115332\n",
      "Epoch 9715  \tTraining Loss: 0.00012436870436013697\tValidation Loss: 0.000127963627462966\n",
      "Epoch 9716  \tTraining Loss: 0.0001243673063268249\tValidation Loss: 0.00012796268511087788\n",
      "Epoch 9717  \tTraining Loss: 0.00012436590778709428\tValidation Loss: 0.00012796155058131755\n",
      "Epoch 9718  \tTraining Loss: 0.00012436451088510968\tValidation Loss: 0.0001279605757110726\n",
      "Epoch 9719  \tTraining Loss: 0.00012436311278937916\tValidation Loss: 0.00012795950439198244\n",
      "Epoch 9720  \tTraining Loss: 0.00012436171610728436\tValidation Loss: 0.00012795848850057762\n",
      "Epoch 9721  \tTraining Loss: 0.0001243603198447136\tValidation Loss: 0.0001279574507683136\n",
      "Epoch 9722  \tTraining Loss: 0.00012435892243849902\tValidation Loss: 0.0001279564078280079\n",
      "Epoch 9723  \tTraining Loss: 0.0001243575272862475\tValidation Loss: 0.00012795534444499397\n",
      "Epoch 9724  \tTraining Loss: 0.00012435612994484088\tValidation Loss: 0.00012795436756947063\n",
      "Epoch 9725  \tTraining Loss: 0.0001243547356658869\tValidation Loss: 0.00012795323198536873\n",
      "Epoch 9726  \tTraining Loss: 0.00012435333962691812\tValidation Loss: 0.0001279522920318455\n",
      "Epoch 9727  \tTraining Loss: 0.0001243519450727392\tValidation Loss: 0.00012795119696682787\n",
      "Epoch 9728  \tTraining Loss: 0.00012435055017498604\tValidation Loss: 0.0001279502166304333\n",
      "Epoch 9729  \tTraining Loss: 0.0001243491551141794\tValidation Loss: 0.00012794909540331\n",
      "Epoch 9730  \tTraining Loss: 0.00012434776169037494\tValidation Loss: 0.00012794812745399242\n",
      "Epoch 9731  \tTraining Loss: 0.00012434636680633103\tValidation Loss: 0.00012794706150607413\n",
      "Epoch 9732  \tTraining Loss: 0.00012434497390597136\tValidation Loss: 0.0001279459662548291\n",
      "Epoch 9733  \tTraining Loss: 0.00012434358090900665\tValidation Loss: 0.0001279450113092398\n",
      "Epoch 9734  \tTraining Loss: 0.0001243421871240693\tValidation Loss: 0.00012794398735321055\n",
      "Epoch 9735  \tTraining Loss: 0.0001243407949493638\tValidation Loss: 0.00012794293182767216\n",
      "Epoch 9736  \tTraining Loss: 0.00012433940144332347\tValidation Loss: 0.00012794190455697586\n",
      "Epoch 9737  \tTraining Loss: 0.00012433801028560092\tValidation Loss: 0.00012794085796789356\n",
      "Epoch 9738  \tTraining Loss: 0.00012433661733817026\tValidation Loss: 0.0001279398804787174\n",
      "Epoch 9739  \tTraining Loss: 0.00012433522638803746\tValidation Loss: 0.0001279387827038713\n",
      "Epoch 9740  \tTraining Loss: 0.0001243338352070819\tValidation Loss: 0.0001279378418065898\n",
      "Epoch 9741  \tTraining Loss: 0.00012433244333736368\tValidation Loss: 0.00012793671197021742\n",
      "Epoch 9742  \tTraining Loss: 0.00012433105341610036\tValidation Loss: 0.00012793575141747124\n",
      "Epoch 9743  \tTraining Loss: 0.000124329661568901\tValidation Loss: 0.00012793462896840938\n",
      "Epoch 9744  \tTraining Loss: 0.00012432827283209556\tValidation Loss: 0.00012793367434283553\n",
      "Epoch 9745  \tTraining Loss: 0.00012432688148345612\tValidation Loss: 0.00012793263305286578\n",
      "Epoch 9746  \tTraining Loss: 0.0001243254930300984\tValidation Loss: 0.00012793162278742386\n",
      "Epoch 9747  \tTraining Loss: 0.00012432410301193412\tValidation Loss: 0.00012793056318191972\n",
      "Epoch 9748  \tTraining Loss: 0.00012432271385955305\tValidation Loss: 0.0001279295288721043\n",
      "Epoch 9749  \tTraining Loss: 0.00012432132512821262\tValidation Loss: 0.00012792854081739087\n",
      "Epoch 9750  \tTraining Loss: 0.00012431993615723774\tValidation Loss: 0.00012792747063030438\n",
      "Epoch 9751  \tTraining Loss: 0.00012431854834176915\tValidation Loss: 0.00012792643897176057\n",
      "Epoch 9752  \tTraining Loss: 0.00012431716035450117\tValidation Loss: 0.00012792541145276005\n",
      "Epoch 9753  \tTraining Loss: 0.00012431577261150126\tValidation Loss: 0.00012792441152996624\n",
      "Epoch 9754  \tTraining Loss: 0.0001243143854543474\tValidation Loss: 0.00012792335573652685\n",
      "Epoch 9755  \tTraining Loss: 0.00012431299752789095\tValidation Loss: 0.00012792232414593848\n",
      "Epoch 9756  \tTraining Loss: 0.00012431161146720217\tValidation Loss: 0.00012792128177493522\n",
      "Epoch 9757  \tTraining Loss: 0.00012431022412801943\tValidation Loss: 0.00012792030898691793\n",
      "Epoch 9758  \tTraining Loss: 0.00012430883811312301\tValidation Loss: 0.0001279191884349921\n",
      "Epoch 9759  \tTraining Loss: 0.00012430745237555413\tValidation Loss: 0.00012791826120760432\n",
      "Epoch 9760  \tTraining Loss: 0.00012430606617892334\tValidation Loss: 0.00012791716896278472\n",
      "Epoch 9761  \tTraining Loss: 0.00012430468164079713\tValidation Loss: 0.00012791620710780435\n",
      "Epoch 9762  \tTraining Loss: 0.00012430329488498197\tValidation Loss: 0.00012791514472824584\n",
      "Epoch 9763  \tTraining Loss: 0.00012430191162462014\tValidation Loss: 0.00012791409701639947\n",
      "Epoch 9764  \tTraining Loss: 0.0001243005257421482\tValidation Loss: 0.0001279131311881989\n",
      "Epoch 9765  \tTraining Loss: 0.000124299142028\tValidation Loss: 0.00012791200654959804\n",
      "Epoch 9766  \tTraining Loss: 0.00012429775793970963\tValidation Loss: 0.00012791105712657024\n",
      "Epoch 9767  \tTraining Loss: 0.0001242963736585697\tValidation Loss: 0.00012790988427186032\n",
      "Epoch 9768  \tTraining Loss: 0.00012429499162010902\tValidation Loss: 0.00012790904005830164\n",
      "Epoch 9769  \tTraining Loss: 0.00012429360679552494\tValidation Loss: 0.0001279079831123812\n",
      "Epoch 9770  \tTraining Loss: 0.00012429222507275158\tValidation Loss: 0.0001279069554988016\n",
      "Epoch 9771  \tTraining Loss: 0.00012429084147596678\tValidation Loss: 0.00012790591676236073\n",
      "Epoch 9772  \tTraining Loss: 0.00012428945969267434\tValidation Loss: 0.00012790489492294475\n",
      "Epoch 9773  \tTraining Loss: 0.00012428807719109595\tValidation Loss: 0.0001279039172138825\n",
      "Epoch 9774  \tTraining Loss: 0.0001242866952553829\tValidation Loss: 0.00012790280178859428\n",
      "Epoch 9775  \tTraining Loss: 0.00012428531434545867\tValidation Loss: 0.00012790185341645036\n",
      "Epoch 9776  \tTraining Loss: 0.00012428393219227304\tValidation Loss: 0.00012790082761218125\n",
      "Epoch 9777  \tTraining Loss: 0.00012428255247176696\tValidation Loss: 0.00012789982097047268\n",
      "Epoch 9778  \tTraining Loss: 0.00012428117091893107\tValidation Loss: 0.0001278987716707393\n",
      "Epoch 9779  \tTraining Loss: 0.00012427979077412552\tValidation Loss: 0.00012789774647098916\n",
      "Epoch 9780  \tTraining Loss: 0.0001242784103990572\tValidation Loss: 0.0001278967111585045\n",
      "Epoch 9781  \tTraining Loss: 0.00012427703054328053\tValidation Loss: 0.0001278956915464976\n",
      "Epoch 9782  \tTraining Loss: 0.00012427565129587744\tValidation Loss: 0.00012789471327930176\n",
      "Epoch 9783  \tTraining Loss: 0.00012427427130159914\tValidation Loss: 0.0001278936608122641\n",
      "Epoch 9784  \tTraining Loss: 0.00012427289302154786\tValidation Loss: 0.00012789265608003318\n",
      "Epoch 9785  \tTraining Loss: 0.0001242715140912118\tValidation Loss: 0.00012789164516738226\n",
      "Epoch 9786  \tTraining Loss: 0.00012427013560185505\tValidation Loss: 0.00012789061310850577\n",
      "Epoch 9787  \tTraining Loss: 0.00012426875742874662\tValidation Loss: 0.00012788957404853856\n",
      "Epoch 9788  \tTraining Loss: 0.00012426737925886637\tValidation Loss: 0.00012788856161268923\n",
      "Epoch 9789  \tTraining Loss: 0.00012426600221507412\tValidation Loss: 0.00012788757979875223\n",
      "Epoch 9790  \tTraining Loss: 0.00012426462414453365\tValidation Loss: 0.00012788653006658935\n",
      "Epoch 9791  \tTraining Loss: 0.0001242632476784876\tValidation Loss: 0.00012788550270020072\n",
      "Epoch 9792  \tTraining Loss: 0.00012426187056168111\tValidation Loss: 0.00012788446759917077\n",
      "Epoch 9793  \tTraining Loss: 0.00012426049403955558\tValidation Loss: 0.00012788345796255692\n",
      "Epoch 9794  \tTraining Loss: 0.0001242591180548308\tValidation Loss: 0.00012788244609634883\n",
      "Epoch 9795  \tTraining Loss: 0.0001242577419852835\tValidation Loss: 0.00012788151677617382\n",
      "Epoch 9796  \tTraining Loss: 0.0001242563665664842\tValidation Loss: 0.00012788044594025768\n",
      "Epoch 9797  \tTraining Loss: 0.0001242549909485823\tValidation Loss: 0.0001278794066291509\n",
      "Epoch 9798  \tTraining Loss: 0.0001242536159153391\tValidation Loss: 0.00012787838801463426\n",
      "Epoch 9799  \tTraining Loss: 0.0001242522409889445\tValidation Loss: 0.00012787735599609776\n",
      "Epoch 9800  \tTraining Loss: 0.00012425086629059434\tValidation Loss: 0.00012787634870064477\n",
      "Epoch 9801  \tTraining Loss: 0.00012424949233228358\tValidation Loss: 0.00012787537136093948\n",
      "Epoch 9802  \tTraining Loss: 0.00012424811771894652\tValidation Loss: 0.00012787432560625817\n",
      "Epoch 9803  \tTraining Loss: 0.00012424674483969887\tValidation Loss: 0.00012787321838307657\n",
      "Epoch 9804  \tTraining Loss: 0.00012424537125111661\tValidation Loss: 0.00012787229022096194\n",
      "Epoch 9805  \tTraining Loss: 0.0001242439983183978\tValidation Loss: 0.00012787132234589208\n",
      "Epoch 9806  \tTraining Loss: 0.00012424262510023088\tValidation Loss: 0.00012787027957342147\n",
      "Epoch 9807  \tTraining Loss: 0.00012424125235045198\tValidation Loss: 0.00012786927316137722\n",
      "Epoch 9808  \tTraining Loss: 0.0001242398804673398\tValidation Loss: 0.00012786829725056016\n",
      "Epoch 9809  \tTraining Loss: 0.00012423850756254954\tValidation Loss: 0.00012786725331943204\n",
      "Epoch 9810  \tTraining Loss: 0.0001242371367518836\tValidation Loss: 0.0001278662315159906\n",
      "Epoch 9811  \tTraining Loss: 0.0001242357645270345\tValidation Loss: 0.00012786520213774907\n",
      "Epoch 9812  \tTraining Loss: 0.00012423439371109596\tValidation Loss: 0.00012786419786254286\n",
      "Epoch 9813  \tTraining Loss: 0.00012423302268786422\tValidation Loss: 0.00012786322386616332\n",
      "Epoch 9814  \tTraining Loss: 0.00012423165147366696\tValidation Loss: 0.00012786212693767523\n",
      "Epoch 9815  \tTraining Loss: 0.00012423028224943676\tValidation Loss: 0.00012786118584468358\n",
      "Epoch 9816  \tTraining Loss: 0.00012422891078176312\tValidation Loss: 0.00012786016972312443\n",
      "Epoch 9817  \tTraining Loss: 0.00012422754246249886\tValidation Loss: 0.00012785918117379021\n",
      "Epoch 9818  \tTraining Loss: 0.00012422617201522974\tValidation Loss: 0.00012785819312470469\n",
      "Epoch 9819  \tTraining Loss: 0.00012422480269004722\tValidation Loss: 0.0001278570916320912\n",
      "Epoch 9820  \tTraining Loss: 0.0001242234343743847\tValidation Loss: 0.0001278561491606664\n",
      "Epoch 9821  \tTraining Loss: 0.00012422206428463022\tValidation Loss: 0.0001278550569105484\n",
      "Epoch 9822  \tTraining Loss: 0.00012422069764623307\tValidation Loss: 0.0001278541188685222\n",
      "Epoch 9823  \tTraining Loss: 0.00012421932785814342\tValidation Loss: 0.00012785313777037544\n",
      "Epoch 9824  \tTraining Loss: 0.00012421796079304512\tValidation Loss: 0.00012785203292542573\n",
      "Epoch 9825  \tTraining Loss: 0.00012421659328728613\tValidation Loss: 0.00012785109247469523\n",
      "Epoch 9826  \tTraining Loss: 0.00012421522505758265\tValidation Loss: 0.00012785000202246256\n",
      "Epoch 9827  \tTraining Loss: 0.00012421385936296875\tValidation Loss: 0.00012784906578731166\n",
      "Epoch 9828  \tTraining Loss: 0.00012421249099474178\tValidation Loss: 0.00012784811407246535\n",
      "Epoch 9829  \tTraining Loss: 0.0001242111259944146\tValidation Loss: 0.00012784702725756594\n",
      "Epoch 9830  \tTraining Loss: 0.00012420975929564105\tValidation Loss: 0.0001278460798068003\n",
      "Epoch 9831  \tTraining Loss: 0.00012420839283964628\tValidation Loss: 0.00012784504177846908\n",
      "Epoch 9832  \tTraining Loss: 0.0001242070279548252\tValidation Loss: 0.00012784400876404123\n",
      "Epoch 9833  \tTraining Loss: 0.0001242056617523117\tValidation Loss: 0.00012784300938988236\n",
      "Epoch 9834  \tTraining Loss: 0.00012420429732659964\tValidation Loss: 0.0001278419835734775\n",
      "Epoch 9835  \tTraining Loss: 0.00012420293181251247\tValidation Loss: 0.00012784103921316962\n",
      "Epoch 9836  \tTraining Loss: 0.0001242015676766438\tValidation Loss: 0.00012783999948636247\n",
      "Epoch 9837  \tTraining Loss: 0.00012420020332640726\tValidation Loss: 0.00012783896900858037\n",
      "Epoch 9838  \tTraining Loss: 0.00012419883930683105\tValidation Loss: 0.0001278378883140753\n",
      "Epoch 9839  \tTraining Loss: 0.00012419747584562242\tValidation Loss: 0.00012783694277469898\n",
      "Epoch 9840  \tTraining Loss: 0.0001241961119725568\tValidation Loss: 0.00012783596342945885\n",
      "Epoch 9841  \tTraining Loss: 0.00012419474927836873\tValidation Loss: 0.00012783500250027887\n",
      "Epoch 9842  \tTraining Loss: 0.0001241933853792227\tValidation Loss: 0.00012783396763528992\n",
      "Epoch 9843  \tTraining Loss: 0.00012419202391860383\tValidation Loss: 0.0001278329878291922\n",
      "Epoch 9844  \tTraining Loss: 0.00012419066091314793\tValidation Loss: 0.00012783204441256274\n",
      "Epoch 9845  \tTraining Loss: 0.00012418929895065804\tValidation Loss: 0.00012783094159505047\n",
      "Epoch 9846  \tTraining Loss: 0.00012418793759475313\tValidation Loss: 0.00012783000426829497\n",
      "Epoch 9847  \tTraining Loss: 0.00012418657484312891\tValidation Loss: 0.0001278289146733338\n",
      "Epoch 9848  \tTraining Loss: 0.0001241852152577843\tValidation Loss: 0.00012782799034204492\n",
      "Epoch 9849  \tTraining Loss: 0.0001241838527305344\tValidation Loss: 0.00012782701126957315\n",
      "Epoch 9850  \tTraining Loss: 0.00012418249280509908\tValidation Loss: 0.0001278259141162252\n",
      "Epoch 9851  \tTraining Loss: 0.00012418113251969553\tValidation Loss: 0.00012782498134512472\n",
      "Epoch 9852  \tTraining Loss: 0.00012417977160076161\tValidation Loss: 0.00012782395510176967\n",
      "Epoch 9853  \tTraining Loss: 0.00012417841257503847\tValidation Loss: 0.00012782293078654106\n",
      "Epoch 9854  \tTraining Loss: 0.0001241770521454224\tValidation Loss: 0.0001278219354328282\n",
      "Epoch 9855  \tTraining Loss: 0.00012417569348754324\tValidation Loss: 0.00012782098022124923\n",
      "Epoch 9856  \tTraining Loss: 0.0001241743338790057\tValidation Loss: 0.0001278199414123645\n",
      "Epoch 9857  \tTraining Loss: 0.00012417297566603004\tValidation Loss: 0.00012781894008583923\n",
      "Epoch 9858  \tTraining Loss: 0.0001241716166478396\tValidation Loss: 0.00012781796216179074\n",
      "Epoch 9859  \tTraining Loss: 0.00012417025810444496\tValidation Loss: 0.00012781689639680967\n",
      "Epoch 9860  \tTraining Loss: 0.0001241689006219497\tValidation Loss: 0.00012781604130578545\n",
      "Epoch 9861  \tTraining Loss: 0.00012416754113991823\tValidation Loss: 0.00012781499188342643\n",
      "Epoch 9862  \tTraining Loss: 0.00012416618527454252\tValidation Loss: 0.00012781397680470446\n",
      "Epoch 9863  \tTraining Loss: 0.000124164826269033\tValidation Loss: 0.00012781307522997294\n",
      "Epoch 9864  \tTraining Loss: 0.00012416346947142048\tValidation Loss: 0.00012781190941208391\n",
      "Epoch 9865  \tTraining Loss: 0.00012416211337838137\tValidation Loss: 0.00012781106014386794\n",
      "Epoch 9866  \tTraining Loss: 0.00012416075495232757\tValidation Loss: 0.00012781004882119387\n",
      "Epoch 9867  \tTraining Loss: 0.00012415940034711084\tValidation Loss: 0.0001278089536952099\n",
      "Epoch 9868  \tTraining Loss: 0.00012415804309572537\tValidation Loss: 0.00012780809074983086\n",
      "Epoch 9869  \tTraining Loss: 0.0001241566876605809\tValidation Loss: 0.00012780698995884523\n",
      "Epoch 9870  \tTraining Loss: 0.00012415533246631815\tValidation Loss: 0.0001278060489497732\n",
      "Epoch 9871  \tTraining Loss: 0.00012415397640962766\tValidation Loss: 0.0001278049914590718\n",
      "Epoch 9872  \tTraining Loss: 0.00012415262223180624\tValidation Loss: 0.00012780411516211834\n",
      "Epoch 9873  \tTraining Loss: 0.0001241512665751767\tValidation Loss: 0.00012780304573612372\n",
      "Epoch 9874  \tTraining Loss: 0.00012414991317658076\tValidation Loss: 0.00012780204763303274\n",
      "Epoch 9875  \tTraining Loss: 0.00012414855775022397\tValidation Loss: 0.00012780109701596889\n",
      "Epoch 9876  \tTraining Loss: 0.00012414720481355714\tValidation Loss: 0.0001278000325873743\n",
      "Epoch 9877  \tTraining Loss: 0.00012414585119719477\tValidation Loss: 0.00012779915613411145\n",
      "Epoch 9878  \tTraining Loss: 0.00012414449681511514\tValidation Loss: 0.00012779808740295882\n",
      "Epoch 9879  \tTraining Loss: 0.00012414314508326377\tValidation Loss: 0.0001277970904824002\n",
      "Epoch 9880  \tTraining Loss: 0.00012414179111051667\tValidation Loss: 0.00012779614319067184\n",
      "Epoch 9881  \tTraining Loss: 0.00012414043910496424\tValidation Loss: 0.00012779506501899184\n",
      "Epoch 9882  \tTraining Loss: 0.0001241390869316851\tValidation Loss: 0.00012779420820953634\n",
      "Epoch 9883  \tTraining Loss: 0.0001241377345194141\tValidation Loss: 0.00012779322042928758\n",
      "Epoch 9884  \tTraining Loss: 0.0001241363833201585\tValidation Loss: 0.00012779214463356367\n",
      "Epoch 9885  \tTraining Loss: 0.00012413503178990696\tValidation Loss: 0.00012779122160851906\n",
      "Epoch 9886  \tTraining Loss: 0.00012413368045057738\tValidation Loss: 0.00012779019968405637\n",
      "Epoch 9887  \tTraining Loss: 0.0001241323291577246\tValidation Loss: 0.00012778919148543255\n",
      "Epoch 9888  \tTraining Loss: 0.00012413097935351955\tValidation Loss: 0.0001277882925240604\n",
      "Epoch 9889  \tTraining Loss: 0.0001241296282826436\tValidation Loss: 0.00012778724236588753\n",
      "Epoch 9890  \tTraining Loss: 0.00012412827845866968\tValidation Loss: 0.00012778623349088904\n",
      "Epoch 9891  \tTraining Loss: 0.00012412692893495574\tValidation Loss: 0.000127785238537196\n",
      "Epoch 9892  \tTraining Loss: 0.00012412557896600664\tValidation Loss: 0.00012778430156583836\n",
      "Epoch 9893  \tTraining Loss: 0.000124124229605024\tValidation Loss: 0.00012778329955757858\n",
      "Epoch 9894  \tTraining Loss: 0.00012412288085475941\tValidation Loss: 0.0001277822300905207\n",
      "Epoch 9895  \tTraining Loss: 0.00012412153266506235\tValidation Loss: 0.00012778135547165714\n",
      "Epoch 9896  \tTraining Loss: 0.00012412018308038798\tValidation Loss: 0.00012778024925116235\n",
      "Epoch 9897  \tTraining Loss: 0.000124118836170666\tValidation Loss: 0.00012777933748941089\n",
      "Epoch 9898  \tTraining Loss: 0.0001241174872801878\tValidation Loss: 0.0001277783131647902\n",
      "Epoch 9899  \tTraining Loss: 0.000124116139827025\tValidation Loss: 0.00012777731178534614\n",
      "Epoch 9900  \tTraining Loss: 0.0001241147930459881\tValidation Loss: 0.00012777640738391904\n",
      "Epoch 9901  \tTraining Loss: 0.00012411344497579172\tValidation Loss: 0.00012777535284458453\n",
      "Epoch 9902  \tTraining Loss: 0.0001241120989455643\tValidation Loss: 0.00012777440659598094\n",
      "Epoch 9903  \tTraining Loss: 0.00012411075200608013\tValidation Loss: 0.00012777331270481405\n",
      "Epoch 9904  \tTraining Loss: 0.00012410940637992641\tValidation Loss: 0.00012777241043328725\n",
      "Epoch 9905  \tTraining Loss: 0.0001241080595202881\tValidation Loss: 0.00012777147281180343\n",
      "Epoch 9906  \tTraining Loss: 0.00012410671404138862\tValidation Loss: 0.0001277703116560261\n",
      "Epoch 9907  \tTraining Loss: 0.0001241053695504032\tValidation Loss: 0.0001277694726645421\n",
      "Epoch 9908  \tTraining Loss: 0.00012410402260518413\tValidation Loss: 0.00012776843115208975\n",
      "Epoch 9909  \tTraining Loss: 0.00012410267925886014\tValidation Loss: 0.00012776742784982587\n",
      "Epoch 9910  \tTraining Loss: 0.00012410133383723457\tValidation Loss: 0.00012776652293942998\n",
      "Epoch 9911  \tTraining Loss: 0.0001240999895915367\tValidation Loss: 0.0001277654243456561\n",
      "Epoch 9912  \tTraining Loss: 0.00012409864578957484\tValidation Loss: 0.00012776449214545752\n",
      "Epoch 9913  \tTraining Loss: 0.00012409730139631328\tValidation Loss: 0.0001277634390548658\n",
      "Epoch 9914  \tTraining Loss: 0.00012409595884159158\tValidation Loss: 0.00012776257148157903\n",
      "Epoch 9915  \tTraining Loss: 0.0001240946137938583\tValidation Loss: 0.00012776155468608022\n",
      "Epoch 9916  \tTraining Loss: 0.0001240932726765833\tValidation Loss: 0.00012776048644092854\n",
      "Epoch 9917  \tTraining Loss: 0.00012409192916995314\tValidation Loss: 0.0001277596145934994\n",
      "Epoch 9918  \tTraining Loss: 0.00012409058617787752\tValidation Loss: 0.00012775852490762383\n",
      "Epoch 9919  \tTraining Loss: 0.0001240892451713044\tValidation Loss: 0.00012775759476502682\n",
      "Epoch 9920  \tTraining Loss: 0.00012408790184456503\tValidation Loss: 0.00012775654342578427\n",
      "Epoch 9921  \tTraining Loss: 0.00012408656169370452\tValidation Loss: 0.0001277556745684868\n",
      "Epoch 9922  \tTraining Loss: 0.0001240852187669026\tValidation Loss: 0.000127754666146323\n",
      "Epoch 9923  \tTraining Loss: 0.00012408387889129737\tValidation Loss: 0.00012775359460258525\n",
      "Epoch 9924  \tTraining Loss: 0.00012408253775797465\tValidation Loss: 0.00012775275013914722\n",
      "Epoch 9925  \tTraining Loss: 0.00012408119656434597\tValidation Loss: 0.0001277516276723129\n",
      "Epoch 9926  \tTraining Loss: 0.00012407985785571645\tValidation Loss: 0.00012775078188995904\n",
      "Epoch 9927  \tTraining Loss: 0.0001240785158630688\tValidation Loss: 0.00012774977866132457\n",
      "Epoch 9928  \tTraining Loss: 0.0001240771772355498\tValidation Loss: 0.00012774874359229463\n",
      "Epoch 9929  \tTraining Loss: 0.00012407583725692752\tValidation Loss: 0.0001277477977262997\n",
      "Epoch 9930  \tTraining Loss: 0.00012407449828154288\tValidation Loss: 0.00012774673752413257\n",
      "Epoch 9931  \tTraining Loss: 0.00012407315959546766\tValidation Loss: 0.00012774587102276126\n",
      "Epoch 9932  \tTraining Loss: 0.00012407182000578632\tValidation Loss: 0.00012774481091949784\n",
      "Epoch 9933  \tTraining Loss: 0.00012407048282046294\tValidation Loss: 0.00012774381602527897\n",
      "Epoch 9934  \tTraining Loss: 0.0001240691435009076\tValidation Loss: 0.0001277428898348898\n",
      "Epoch 9935  \tTraining Loss: 0.00012406780623122865\tValidation Loss: 0.00012774186532915052\n",
      "Epoch 9936  \tTraining Loss: 0.0001240664686462768\tValidation Loss: 0.00012774092516523738\n",
      "Epoch 9937  \tTraining Loss: 0.00012406513126193668\tValidation Loss: 0.00012773995168223796\n",
      "Epoch 9938  \tTraining Loss: 0.0001240637942158019\tValidation Loss: 0.00012773892628225778\n",
      "Epoch 9939  \tTraining Loss: 0.00012406245715683\tValidation Loss: 0.00012773793191252195\n",
      "Epoch 9940  \tTraining Loss: 0.00012406112125277363\tValidation Loss: 0.00012773695043250406\n",
      "Epoch 9941  \tTraining Loss: 0.00012405978423697745\tValidation Loss: 0.00012773607391222555\n",
      "Epoch 9942  \tTraining Loss: 0.00012405844846668547\tValidation Loss: 0.00012773506533392425\n",
      "Epoch 9943  \tTraining Loss: 0.0001240571131667131\tValidation Loss: 0.0001277339781798358\n",
      "Epoch 9944  \tTraining Loss: 0.0001240557777802236\tValidation Loss: 0.0001277330789565994\n",
      "Epoch 9945  \tTraining Loss: 0.0001240544421254542\tValidation Loss: 0.00012773206563120997\n",
      "Epoch 9946  \tTraining Loss: 0.0001240531072644969\tValidation Loss: 0.00012773108220137216\n",
      "Epoch 9947  \tTraining Loss: 0.00012405177308099177\tValidation Loss: 0.00012773018222350852\n",
      "Epoch 9948  \tTraining Loss: 0.000124050438040124\tValidation Loss: 0.0001277291475457823\n",
      "Epoch 9949  \tTraining Loss: 0.00012404910455180896\tValidation Loss: 0.00012772820471908337\n",
      "Epoch 9950  \tTraining Loss: 0.00012404777036157725\tValidation Loss: 0.00012772713095567692\n",
      "Epoch 9951  \tTraining Loss: 0.00012404643744281827\tValidation Loss: 0.00012772623545199939\n",
      "Epoch 9952  \tTraining Loss: 0.0001240451032996348\tValidation Loss: 0.00012772522503493304\n",
      "Epoch 9953  \tTraining Loss: 0.000124043770518713\tValidation Loss: 0.00012772424385907357\n",
      "Epoch 9954  \tTraining Loss: 0.00012404243845074586\tValidation Loss: 0.00012772334610450578\n",
      "Epoch 9955  \tTraining Loss: 0.00012404110484836022\tValidation Loss: 0.00012772231348892387\n",
      "Epoch 9956  \tTraining Loss: 0.00012403977364074941\tValidation Loss: 0.00012772137244287701\n",
      "Epoch 9957  \tTraining Loss: 0.00012403844077055835\tValidation Loss: 0.00012772029753173916\n",
      "Epoch 9958  \tTraining Loss: 0.00012403711025535323\tValidation Loss: 0.0001277194103694439\n",
      "Epoch 9959  \tTraining Loss: 0.00012403577810301874\tValidation Loss: 0.0001277184800565907\n",
      "Epoch 9960  \tTraining Loss: 0.00012403444649100113\tValidation Loss: 0.00012771739763433343\n",
      "Epoch 9961  \tTraining Loss: 0.00012403311694924202\tValidation Loss: 0.00012771646875872711\n",
      "Epoch 9962  \tTraining Loss: 0.0001240317849378641\tValidation Loss: 0.0001277154779657276\n",
      "Epoch 9963  \tTraining Loss: 0.00012403045593332868\tValidation Loss: 0.00012771455070765408\n",
      "Epoch 9964  \tTraining Loss: 0.00012402912465537546\tValidation Loss: 0.00012771361574026905\n",
      "Epoch 9965  \tTraining Loss: 0.00012402779552016984\tValidation Loss: 0.00012771247459479994\n",
      "Epoch 9966  \tTraining Loss: 0.00012402646660695512\tValidation Loss: 0.00012771164227444204\n",
      "Epoch 9967  \tTraining Loss: 0.00012402513580532783\tValidation Loss: 0.00012771057697987705\n",
      "Epoch 9968  \tTraining Loss: 0.00012402380835758317\tValidation Loss: 0.00012770971105424608\n",
      "Epoch 9969  \tTraining Loss: 0.00012402247821445046\tValidation Loss: 0.00012770871069120148\n",
      "Epoch 9970  \tTraining Loss: 0.00012402115110883805\tValidation Loss: 0.0001277076566316637\n",
      "Epoch 9971  \tTraining Loss: 0.0001240198223439457\tValidation Loss: 0.00012770679125540774\n",
      "Epoch 9972  \tTraining Loss: 0.00012401849379315593\tValidation Loss: 0.00012770566467365068\n",
      "Epoch 9973  \tTraining Loss: 0.00012401716777734424\tValidation Loss: 0.00012770483694691322\n",
      "Epoch 9974  \tTraining Loss: 0.0001240158384557315\tValidation Loss: 0.000127703855024594\n",
      "Epoch 9975  \tTraining Loss: 0.00012401451285845195\tValidation Loss: 0.00012770283699821826\n",
      "Epoch 9976  \tTraining Loss: 0.00012401318477728665\tValidation Loss: 0.000127701897305525\n",
      "Epoch 9977  \tTraining Loss: 0.00012401185919672436\tValidation Loss: 0.00012770085516075377\n",
      "Epoch 9978  \tTraining Loss: 0.00012401053292734844\tValidation Loss: 0.00012770000378310096\n",
      "Epoch 9979  \tTraining Loss: 0.00012400920568536713\tValidation Loss: 0.0001276990149679481\n",
      "Epoch 9980  \tTraining Loss: 0.00012400788110027763\tValidation Loss: 0.00012769793737954727\n",
      "Epoch 9981  \tTraining Loss: 0.00012400655501782756\tValidation Loss: 0.00012769704729627868\n",
      "Epoch 9982  \tTraining Loss: 0.0001240052303511643\tValidation Loss: 0.00012769604689059477\n",
      "Epoch 9983  \tTraining Loss: 0.0001240039046501158\tValidation Loss: 0.00012769516985691997\n",
      "Epoch 9984  \tTraining Loss: 0.00012400257981318155\tValidation Loss: 0.00012769403455282176\n",
      "Epoch 9985  \tTraining Loss: 0.00012400125680619745\tValidation Loss: 0.00012769321788895274\n",
      "Epoch 9986  \tTraining Loss: 0.00012399993119337733\tValidation Loss: 0.00012769223503634488\n",
      "Epoch 9987  \tTraining Loss: 0.0001239986079652048\tValidation Loss: 0.00012769121872129697\n",
      "Epoch 9988  \tTraining Loss: 0.0001239972835422657\tValidation Loss: 0.00012769029243585303\n",
      "Epoch 9989  \tTraining Loss: 0.00012399596100112916\tValidation Loss: 0.00012768922973609677\n",
      "Epoch 9990  \tTraining Loss: 0.00012399463786467494\tValidation Loss: 0.000127688390332838\n",
      "Epoch 9991  \tTraining Loss: 0.00012399331462434107\tValidation Loss: 0.0001276873996565057\n",
      "Epoch 9992  \tTraining Loss: 0.00012399199215433877\tValidation Loss: 0.00012768639345578936\n",
      "Epoch 9993  \tTraining Loss: 0.00012399066988151228\tValidation Loss: 0.00012768540709935926\n",
      "Epoch 9994  \tTraining Loss: 0.00012398934807203907\tValidation Loss: 0.00012768444106582496\n",
      "Epoch 9995  \tTraining Loss: 0.0001239880259189858\tValidation Loss: 0.0001276835237298464\n",
      "Epoch 9996  \tTraining Loss: 0.00012398670464205182\tValidation Loss: 0.00012768260708813947\n",
      "Epoch 9997  \tTraining Loss: 0.00012398538298887763\tValidation Loss: 0.00012768151344139358\n",
      "Epoch 9998  \tTraining Loss: 0.00012398406299137565\tValidation Loss: 0.00012768062090991975\n",
      "Epoch 9999  \tTraining Loss: 0.0001239827414554491\tValidation Loss: 0.00012767963214255668\n",
      "Epoch 10000  \tTraining Loss: 0.0001239814212183288\tValidation Loss: 0.00012767870873896676\n"
     ]
    }
   ],
   "source": [
    "# training time!\n",
    "\n",
    "import numpy as np\n",
    "from nn.nn import NeuralNetwork\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# transpose to match nn (feature, batch) code\n",
    "X_train, X_val, y_train, y_val = X_train.T, X_val.T, y_train.T, y_val.T\n",
    "\n",
    "\n",
    "nn_arch = [\n",
    "    {'input_dim': 64, 'output_dim': 16, 'activation': 'relu'},\n",
    "    {'input_dim': 16, 'output_dim': 64, 'activation': 'sigmoid'}\n",
    "]\n",
    "\n",
    "\n",
    "autoencoder = NeuralNetwork(nn_arch=nn_arch, lr=0.1, seed=42, batch_size=32, epochs=10000, loss_function='mean_squared_error')\n",
    "\n",
    "train_loss, val_loss = autoencoder.fit(X_train, y_train, X_val, y_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(train_loss, label='train loss')\n",
    "plt.plot(val_loss, label='val loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reconstruction MSE error: 0.008171437359293873\n"
     ]
    }
   ],
   "source": [
    "y_val_pred = autoencoder.predict(X_val)\n",
    "\n",
    "# MSE as reconstruction metric (lower is better)\n",
    "reconstruction_error = np.mean(np.square(y_val - y_val_pred))\n",
    "print(f'Average reconstruction MSE error: {reconstruction_error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    example_truth = (X_val.T)[i].reshape(8, 8)\n",
    "    example_pred =  (y_val_pred.T)[i].reshape(8, 8)\n",
    "    # plotting the images side by side\n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(example_truth, cmap='gray')\n",
    "    plt.title('Ground Truth')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(example_pred, cmap='gray')\n",
    "    plt.title('Predicted Output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nExplain why you chose the hyperparameter values you did.\\n\\n\\n\\nLearning rate: I played around with a bunch of values, and found that 0.1 did well\\n\\nEpochs: I found the larger the value, the better the model performed. I\\'m sure maybe\\na touch higher could be better, but I didn\\'t want to wait for longer epochs.\\n\\nRelu is more aggressive activation, so I used it in the inner layer. Last layer had to\\nbe sigmoid since relu could be too \"binary\" to yeild a good image.\\n\\nMSE loss since this isn\\'t a binary task\\n\\n32 mini batches was chosen without any real reason. I think this is near lower\\nboundbefore things get too bumpy in the loss\\n'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "*Explain why you chose the hyperparameter values you did.*\n",
    "\n",
    "\n",
    "\n",
    "Learning rate: I played around with a bunch of values, and found that 0.1 did well\n",
    "\n",
    "Epochs: I found the larger the value, the better the model performed. I'm sure maybe\n",
    "a touch higher could be better, but I didn't want to wait for longer epochs.\n",
    "\n",
    "Relu is more aggressive activation, so I used it in the inner layer. Last layer had to\n",
    "be sigmoid since relu could be too \"binary\" to yeild a good image.\n",
    "\n",
    "MSE loss since this isn't a binary task\n",
    "\n",
    "32 mini batches was chosen without any real reason. I think this is near lower\n",
    "boundbefore things get too bumpy in the loss\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
